I1123 11:43:11.967068 19447 caffe.cpp:218] Using GPUs 2
I1123 11:43:12.019883 19447 caffe.cpp:223] GPU 2: GeForce GTX TITAN X
I1123 11:43:14.223839 19447 solver.cpp:44] Initializing solver from parameters: 
train_net: "/home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/train_densenet.prototxt"
test_net: "/home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/test_densenet.prototxt"
test_iter: 359
test_interval: 3629
base_lr: 0.0005
display: 3629
max_iter: 362900
lr_policy: "multistep"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
snapshot: 10000
snapshot_prefix: "/home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG"
solver_mode: GPU
device_id: 2
random_seed: 831486
train_state {
  level: 0
  stage: ""
}
stepvalue: 108870
stepvalue: 217740
type: "Nesterov"
I1123 11:43:14.279443 19447 solver.cpp:77] Creating training net from train_net file: /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/train_densenet.prototxt
I1123 11:43:14.306401 19447 net.cpp:294] The NetState phase (0) differed from the phase (1) specified by a rule in layer Accuracy1
I1123 11:43:14.306882 19447 net.cpp:51] Initializing net from parameters: 
state {
  phase: TRAIN
  level: 0
  stage: ""
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "label"
  include {
    phase: TRAIN
  }
  transform_param {
    mirror: false
    crop_size: 224
    mean_file: "/home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/mean.binaryproto"
  }
  data_param {
    source: "/home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/img_train_lmdb"
    batch_size: 3
    backend: LMDB
  }
}
layer {
  name: "Convolution1_3x3"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1_3x3"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Convolution1_3x3"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "BatchNorm1"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "BatchNorm1"
  top: "Convolution2"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "Convolution2"
  top: "Dropout1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Convolution1"
  bottom: "Dropout1"
  top: "Concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Concat1"
  top: "BatchNorm2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "BatchNorm2"
  top: "Convolution3"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout2"
  type: "Dropout"
  bottom: "Convolution3"
  top: "Dropout2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat2"
  type: "Concat"
  bottom: "Concat1"
  bottom: "Dropout2"
  top: "Concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Concat2"
  top: "BatchNorm3"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "BatchNorm3"
  top: "Convolution4"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout3"
  type: "Dropout"
  bottom: "Convolution4"
  top: "Dropout3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat3"
  type: "Concat"
  bottom: "Concat2"
  bottom: "Dropout3"
  top: "Concat3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Concat3"
  top: "BatchNorm4"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "BatchNorm4"
  top: "Convolution5"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout4"
  type: "Dropout"
  bottom: "Convolution5"
  top: "Dropout4"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat4"
  type: "Concat"
  bottom: "Concat3"
  bottom: "Dropout4"
  top: "Concat4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Concat4"
  top: "BatchNorm5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "BatchNorm5"
  top: "Convolution6"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout5"
  type: "Dropout"
  bottom: "Convolution6"
  top: "Dropout5"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat5"
  type: "Concat"
  bottom: "Concat4"
  bottom: "Dropout5"
  top: "Concat5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Concat5"
  top: "BatchNorm6"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "BatchNorm6"
  top: "Convolution7"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout6"
  type: "Dropout"
  bottom: "Convolution7"
  top: "Dropout6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat6"
  type: "Concat"
  bottom: "Concat5"
  bottom: "Dropout6"
  top: "Concat6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Concat6"
  top: "BatchNorm7"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "BatchNorm7"
  top: "Convolution8"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout7"
  type: "Dropout"
  bottom: "Convolution8"
  top: "Dropout7"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat7"
  type: "Concat"
  bottom: "Concat6"
  bottom: "Dropout7"
  top: "Concat7"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Concat7"
  top: "BatchNorm8"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "BatchNorm8"
  top: "Convolution9"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout8"
  type: "Dropout"
  bottom: "Convolution9"
  top: "Dropout8"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat8"
  type: "Concat"
  bottom: "Concat7"
  bottom: "Dropout8"
  top: "Concat8"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Concat8"
  top: "BatchNorm9"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "BatchNorm9"
  top: "Convolution10"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout9"
  type: "Dropout"
  bottom: "Convolution10"
  top: "Dropout9"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat9"
  type: "Concat"
  bottom: "Concat8"
  bottom: "Dropout9"
  top: "Concat9"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Concat9"
  top: "BatchNorm10"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "BatchNorm10"
  top: "Convolution11"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout10"
  type: "Dropout"
  bottom: "Convolution11"
  top: "Dropout10"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat10"
  type: "Concat"
  bottom: "Concat9"
  bottom: "Dropout10"
  top: "Concat10"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Concat10"
  top: "BatchNorm11"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "BatchNorm11"
  top: "Convolution12"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout11"
  type: "Dropout"
  bottom: "Convolution12"
  top: "Dropout11"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat11"
  type: "Concat"
  bottom: "Concat10"
  bottom: "Dropout11"
  top: "Concat11"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Concat11"
  top: "BatchNorm12"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "BatchNorm12"
  top: "Convolution13"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout12"
  type: "Dropout"
  bottom: "Convolution13"
  top: "Dropout12"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat12"
  type: "Concat"
  bottom: "Concat11"
  bottom: "Dropout12"
  top: "Concat12"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Concat12"
  top: "BatchNorm13"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "BatchNorm13"
  top: "Pooling3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 16
    bias_term: true
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "label"
  top: "SoftmaxWithLoss1"
}
I1123 11:43:14.307543 19447 layer_factory.hpp:77] Creating layer Data1
I1123 11:43:14.389889 19447 db_lmdb.cpp:35] Opened lmdb /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/img_train_lmdb
I1123 11:43:14.406414 19447 net.cpp:84] Creating Layer Data1
I1123 11:43:14.406455 19447 net.cpp:380] Data1 -> Data1
I1123 11:43:14.406493 19447 net.cpp:380] Data1 -> label
I1123 11:43:14.406554 19447 data_transformer.cpp:25] Loading mean file from: /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/mean.binaryproto
I1123 11:43:14.616520 19447 data_layer.cpp:45] output data size: 3,3,224,224
I1123 11:43:14.627811 19447 net.cpp:122] Setting up Data1
I1123 11:43:14.627863 19447 net.cpp:129] Top shape: 3 3 224 224 (451584)
I1123 11:43:14.627878 19447 net.cpp:129] Top shape: 3 (3)
I1123 11:43:14.628264 19447 net.cpp:137] Memory required for data: 1806348
I1123 11:43:14.628288 19447 layer_factory.hpp:77] Creating layer Convolution1_3x3
I1123 11:43:14.628378 19447 net.cpp:84] Creating Layer Convolution1_3x3
I1123 11:43:14.628433 19447 net.cpp:406] Convolution1_3x3 <- Data1
I1123 11:43:14.628484 19447 net.cpp:380] Convolution1_3x3 -> Convolution1_3x3
I1123 11:43:16.308202 19447 net.cpp:122] Setting up Convolution1_3x3
I1123 11:43:16.309773 19447 net.cpp:129] Top shape: 3 16 112 112 (602112)
I1123 11:43:16.309787 19447 net.cpp:137] Memory required for data: 4214796
I1123 11:43:16.345072 19447 layer_factory.hpp:77] Creating layer Convolution1
I1123 11:43:16.346773 19447 net.cpp:84] Creating Layer Convolution1
I1123 11:43:16.346786 19447 net.cpp:406] Convolution1 <- Convolution1_3x3
I1123 11:43:16.346794 19447 net.cpp:380] Convolution1 -> Convolution1
I1123 11:43:16.371063 19447 net.cpp:122] Setting up Convolution1
I1123 11:43:16.371084 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.371088 19447 net.cpp:137] Memory required for data: 4816908
I1123 11:43:16.371099 19447 layer_factory.hpp:77] Creating layer Convolution1_Convolution1_0_split
I1123 11:43:16.371107 19447 net.cpp:84] Creating Layer Convolution1_Convolution1_0_split
I1123 11:43:16.371111 19447 net.cpp:406] Convolution1_Convolution1_0_split <- Convolution1
I1123 11:43:16.371117 19447 net.cpp:380] Convolution1_Convolution1_0_split -> Convolution1_Convolution1_0_split_0
I1123 11:43:16.371126 19447 net.cpp:380] Convolution1_Convolution1_0_split -> Convolution1_Convolution1_0_split_1
I1123 11:43:16.371534 19447 net.cpp:122] Setting up Convolution1_Convolution1_0_split
I1123 11:43:16.371548 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.371552 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.371556 19447 net.cpp:137] Memory required for data: 6021132
I1123 11:43:16.371558 19447 layer_factory.hpp:77] Creating layer BatchNorm1
I1123 11:43:16.371793 19447 net.cpp:84] Creating Layer BatchNorm1
I1123 11:43:16.371803 19447 net.cpp:406] BatchNorm1 <- Convolution1_Convolution1_0_split_0
I1123 11:43:16.371809 19447 net.cpp:380] BatchNorm1 -> BatchNorm1
I1123 11:43:16.386485 19447 net.cpp:122] Setting up BatchNorm1
I1123 11:43:16.386500 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.386504 19447 net.cpp:137] Memory required for data: 6623244
I1123 11:43:16.386517 19447 layer_factory.hpp:77] Creating layer Scale1
I1123 11:43:16.387053 19447 net.cpp:84] Creating Layer Scale1
I1123 11:43:16.387063 19447 net.cpp:406] Scale1 <- BatchNorm1
I1123 11:43:16.387068 19447 net.cpp:367] Scale1 -> BatchNorm1 (in-place)
I1123 11:43:16.395592 19447 layer_factory.hpp:77] Creating layer Scale1
I1123 11:43:16.396263 19447 net.cpp:122] Setting up Scale1
I1123 11:43:16.396275 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.396277 19447 net.cpp:137] Memory required for data: 7225356
I1123 11:43:16.396283 19447 layer_factory.hpp:77] Creating layer ReLU1
I1123 11:43:16.396289 19447 net.cpp:84] Creating Layer ReLU1
I1123 11:43:16.396292 19447 net.cpp:406] ReLU1 <- BatchNorm1
I1123 11:43:16.396297 19447 net.cpp:367] ReLU1 -> BatchNorm1 (in-place)
I1123 11:43:16.397315 19447 net.cpp:122] Setting up ReLU1
I1123 11:43:16.397330 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.397333 19447 net.cpp:137] Memory required for data: 7827468
I1123 11:43:16.397336 19447 layer_factory.hpp:77] Creating layer Convolution2
I1123 11:43:16.397346 19447 net.cpp:84] Creating Layer Convolution2
I1123 11:43:16.397349 19447 net.cpp:406] Convolution2 <- BatchNorm1
I1123 11:43:16.397356 19447 net.cpp:380] Convolution2 -> Convolution2
I1123 11:43:16.399176 19447 net.cpp:122] Setting up Convolution2
I1123 11:43:16.399190 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.399194 19447 net.cpp:137] Memory required for data: 8279052
I1123 11:43:16.399199 19447 layer_factory.hpp:77] Creating layer Dropout1
I1123 11:43:16.399209 19447 net.cpp:84] Creating Layer Dropout1
I1123 11:43:16.399215 19447 net.cpp:406] Dropout1 <- Convolution2
I1123 11:43:16.399220 19447 net.cpp:380] Dropout1 -> Dropout1
I1123 11:43:16.399266 19447 net.cpp:122] Setting up Dropout1
I1123 11:43:16.399279 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.399305 19447 net.cpp:137] Memory required for data: 8730636
I1123 11:43:16.399313 19447 layer_factory.hpp:77] Creating layer Concat1
I1123 11:43:16.399346 19447 net.cpp:84] Creating Layer Concat1
I1123 11:43:16.399355 19447 net.cpp:406] Concat1 <- Convolution1_Convolution1_0_split_1
I1123 11:43:16.399359 19447 net.cpp:406] Concat1 <- Dropout1
I1123 11:43:16.399364 19447 net.cpp:380] Concat1 -> Concat1
I1123 11:43:16.399395 19447 net.cpp:122] Setting up Concat1
I1123 11:43:16.399404 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.399406 19447 net.cpp:137] Memory required for data: 9784332
I1123 11:43:16.399410 19447 layer_factory.hpp:77] Creating layer Concat1_Concat1_0_split
I1123 11:43:16.399415 19447 net.cpp:84] Creating Layer Concat1_Concat1_0_split
I1123 11:43:16.399423 19447 net.cpp:406] Concat1_Concat1_0_split <- Concat1
I1123 11:43:16.399431 19447 net.cpp:380] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_0
I1123 11:43:16.399443 19447 net.cpp:380] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_1
I1123 11:43:16.399505 19447 net.cpp:122] Setting up Concat1_Concat1_0_split
I1123 11:43:16.399515 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.399519 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.399523 19447 net.cpp:137] Memory required for data: 11891724
I1123 11:43:16.399528 19447 layer_factory.hpp:77] Creating layer BatchNorm2
I1123 11:43:16.399535 19447 net.cpp:84] Creating Layer BatchNorm2
I1123 11:43:16.399543 19447 net.cpp:406] BatchNorm2 <- Concat1_Concat1_0_split_0
I1123 11:43:16.399552 19447 net.cpp:380] BatchNorm2 -> BatchNorm2
I1123 11:43:16.399763 19447 net.cpp:122] Setting up BatchNorm2
I1123 11:43:16.399773 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.399776 19447 net.cpp:137] Memory required for data: 12945420
I1123 11:43:16.399788 19447 layer_factory.hpp:77] Creating layer Scale2
I1123 11:43:16.399801 19447 net.cpp:84] Creating Layer Scale2
I1123 11:43:16.399808 19447 net.cpp:406] Scale2 <- BatchNorm2
I1123 11:43:16.399816 19447 net.cpp:367] Scale2 -> BatchNorm2 (in-place)
I1123 11:43:16.399874 19447 layer_factory.hpp:77] Creating layer Scale2
I1123 11:43:16.400005 19447 net.cpp:122] Setting up Scale2
I1123 11:43:16.400017 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.400019 19447 net.cpp:137] Memory required for data: 13999116
I1123 11:43:16.400027 19447 layer_factory.hpp:77] Creating layer ReLU2
I1123 11:43:16.400038 19447 net.cpp:84] Creating Layer ReLU2
I1123 11:43:16.400044 19447 net.cpp:406] ReLU2 <- BatchNorm2
I1123 11:43:16.400051 19447 net.cpp:367] ReLU2 -> BatchNorm2 (in-place)
I1123 11:43:16.400238 19447 net.cpp:122] Setting up ReLU2
I1123 11:43:16.400249 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.400251 19447 net.cpp:137] Memory required for data: 15052812
I1123 11:43:16.400256 19447 layer_factory.hpp:77] Creating layer Convolution3
I1123 11:43:16.400269 19447 net.cpp:84] Creating Layer Convolution3
I1123 11:43:16.400277 19447 net.cpp:406] Convolution3 <- BatchNorm2
I1123 11:43:16.400286 19447 net.cpp:380] Convolution3 -> Convolution3
I1123 11:43:16.402150 19447 net.cpp:122] Setting up Convolution3
I1123 11:43:16.402165 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.402168 19447 net.cpp:137] Memory required for data: 15504396
I1123 11:43:16.402173 19447 layer_factory.hpp:77] Creating layer Dropout2
I1123 11:43:16.402180 19447 net.cpp:84] Creating Layer Dropout2
I1123 11:43:16.402185 19447 net.cpp:406] Dropout2 <- Convolution3
I1123 11:43:16.402194 19447 net.cpp:380] Dropout2 -> Dropout2
I1123 11:43:16.402247 19447 net.cpp:122] Setting up Dropout2
I1123 11:43:16.402258 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.402261 19447 net.cpp:137] Memory required for data: 15955980
I1123 11:43:16.402264 19447 layer_factory.hpp:77] Creating layer Concat2
I1123 11:43:16.402276 19447 net.cpp:84] Creating Layer Concat2
I1123 11:43:16.402293 19447 net.cpp:406] Concat2 <- Concat1_Concat1_0_split_1
I1123 11:43:16.402303 19447 net.cpp:406] Concat2 <- Dropout2
I1123 11:43:16.402329 19447 net.cpp:380] Concat2 -> Concat2
I1123 11:43:16.402371 19447 net.cpp:122] Setting up Concat2
I1123 11:43:16.402382 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.402385 19447 net.cpp:137] Memory required for data: 17461260
I1123 11:43:16.402390 19447 layer_factory.hpp:77] Creating layer Concat2_Concat2_0_split
I1123 11:43:16.402397 19447 net.cpp:84] Creating Layer Concat2_Concat2_0_split
I1123 11:43:16.402402 19447 net.cpp:406] Concat2_Concat2_0_split <- Concat2
I1123 11:43:16.402410 19447 net.cpp:380] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_0
I1123 11:43:16.402423 19447 net.cpp:380] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_1
I1123 11:43:16.402479 19447 net.cpp:122] Setting up Concat2_Concat2_0_split
I1123 11:43:16.402490 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.402493 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.402496 19447 net.cpp:137] Memory required for data: 20471820
I1123 11:43:16.402501 19447 layer_factory.hpp:77] Creating layer BatchNorm3
I1123 11:43:16.402509 19447 net.cpp:84] Creating Layer BatchNorm3
I1123 11:43:16.402514 19447 net.cpp:406] BatchNorm3 <- Concat2_Concat2_0_split_0
I1123 11:43:16.402525 19447 net.cpp:380] BatchNorm3 -> BatchNorm3
I1123 11:43:16.402737 19447 net.cpp:122] Setting up BatchNorm3
I1123 11:43:16.402747 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.402750 19447 net.cpp:137] Memory required for data: 21977100
I1123 11:43:16.402762 19447 layer_factory.hpp:77] Creating layer Scale3
I1123 11:43:16.402776 19447 net.cpp:84] Creating Layer Scale3
I1123 11:43:16.402784 19447 net.cpp:406] Scale3 <- BatchNorm3
I1123 11:43:16.402793 19447 net.cpp:367] Scale3 -> BatchNorm3 (in-place)
I1123 11:43:16.402848 19447 layer_factory.hpp:77] Creating layer Scale3
I1123 11:43:16.402978 19447 net.cpp:122] Setting up Scale3
I1123 11:43:16.402988 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.402992 19447 net.cpp:137] Memory required for data: 23482380
I1123 11:43:16.402997 19447 layer_factory.hpp:77] Creating layer ReLU3
I1123 11:43:16.403005 19447 net.cpp:84] Creating Layer ReLU3
I1123 11:43:16.403010 19447 net.cpp:406] ReLU3 <- BatchNorm3
I1123 11:43:16.403018 19447 net.cpp:367] ReLU3 -> BatchNorm3 (in-place)
I1123 11:43:16.403208 19447 net.cpp:122] Setting up ReLU3
I1123 11:43:16.403219 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.403223 19447 net.cpp:137] Memory required for data: 24987660
I1123 11:43:16.403224 19447 layer_factory.hpp:77] Creating layer Convolution4
I1123 11:43:16.403237 19447 net.cpp:84] Creating Layer Convolution4
I1123 11:43:16.403244 19447 net.cpp:406] Convolution4 <- BatchNorm3
I1123 11:43:16.403251 19447 net.cpp:380] Convolution4 -> Convolution4
I1123 11:43:16.405570 19447 net.cpp:122] Setting up Convolution4
I1123 11:43:16.405583 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.405587 19447 net.cpp:137] Memory required for data: 25439244
I1123 11:43:16.405592 19447 layer_factory.hpp:77] Creating layer Dropout3
I1123 11:43:16.405599 19447 net.cpp:84] Creating Layer Dropout3
I1123 11:43:16.405602 19447 net.cpp:406] Dropout3 <- Convolution4
I1123 11:43:16.405607 19447 net.cpp:380] Dropout3 -> Dropout3
I1123 11:43:16.405652 19447 net.cpp:122] Setting up Dropout3
I1123 11:43:16.405665 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.405670 19447 net.cpp:137] Memory required for data: 25890828
I1123 11:43:16.405675 19447 layer_factory.hpp:77] Creating layer Concat3
I1123 11:43:16.405683 19447 net.cpp:84] Creating Layer Concat3
I1123 11:43:16.405689 19447 net.cpp:406] Concat3 <- Concat2_Concat2_0_split_1
I1123 11:43:16.405694 19447 net.cpp:406] Concat3 <- Dropout3
I1123 11:43:16.405704 19447 net.cpp:380] Concat3 -> Concat3
I1123 11:43:16.405737 19447 net.cpp:122] Setting up Concat3
I1123 11:43:16.405750 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.405753 19447 net.cpp:137] Memory required for data: 27847692
I1123 11:43:16.405756 19447 layer_factory.hpp:77] Creating layer Concat3_Concat3_0_split
I1123 11:43:16.405778 19447 net.cpp:84] Creating Layer Concat3_Concat3_0_split
I1123 11:43:16.405786 19447 net.cpp:406] Concat3_Concat3_0_split <- Concat3
I1123 11:43:16.405792 19447 net.cpp:380] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_0
I1123 11:43:16.405810 19447 net.cpp:380] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_1
I1123 11:43:16.405864 19447 net.cpp:122] Setting up Concat3_Concat3_0_split
I1123 11:43:16.405874 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.405876 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.405879 19447 net.cpp:137] Memory required for data: 31761420
I1123 11:43:16.405882 19447 layer_factory.hpp:77] Creating layer BatchNorm4
I1123 11:43:16.405894 19447 net.cpp:84] Creating Layer BatchNorm4
I1123 11:43:16.405900 19447 net.cpp:406] BatchNorm4 <- Concat3_Concat3_0_split_0
I1123 11:43:16.405918 19447 net.cpp:380] BatchNorm4 -> BatchNorm4
I1123 11:43:16.406137 19447 net.cpp:122] Setting up BatchNorm4
I1123 11:43:16.406147 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.406150 19447 net.cpp:137] Memory required for data: 33718284
I1123 11:43:16.406159 19447 layer_factory.hpp:77] Creating layer Scale4
I1123 11:43:16.406167 19447 net.cpp:84] Creating Layer Scale4
I1123 11:43:16.406177 19447 net.cpp:406] Scale4 <- BatchNorm4
I1123 11:43:16.406183 19447 net.cpp:367] Scale4 -> BatchNorm4 (in-place)
I1123 11:43:16.406241 19447 layer_factory.hpp:77] Creating layer Scale4
I1123 11:43:16.406373 19447 net.cpp:122] Setting up Scale4
I1123 11:43:16.406383 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.406386 19447 net.cpp:137] Memory required for data: 35675148
I1123 11:43:16.406392 19447 layer_factory.hpp:77] Creating layer ReLU4
I1123 11:43:16.406399 19447 net.cpp:84] Creating Layer ReLU4
I1123 11:43:16.406404 19447 net.cpp:406] ReLU4 <- BatchNorm4
I1123 11:43:16.406411 19447 net.cpp:367] ReLU4 -> BatchNorm4 (in-place)
I1123 11:43:16.407160 19447 net.cpp:122] Setting up ReLU4
I1123 11:43:16.407172 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.407176 19447 net.cpp:137] Memory required for data: 37632012
I1123 11:43:16.407178 19447 layer_factory.hpp:77] Creating layer Convolution5
I1123 11:43:16.407192 19447 net.cpp:84] Creating Layer Convolution5
I1123 11:43:16.407200 19447 net.cpp:406] Convolution5 <- BatchNorm4
I1123 11:43:16.407210 19447 net.cpp:380] Convolution5 -> Convolution5
I1123 11:43:16.409524 19447 net.cpp:122] Setting up Convolution5
I1123 11:43:16.409536 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.409539 19447 net.cpp:137] Memory required for data: 38083596
I1123 11:43:16.409543 19447 layer_factory.hpp:77] Creating layer Dropout4
I1123 11:43:16.409550 19447 net.cpp:84] Creating Layer Dropout4
I1123 11:43:16.409554 19447 net.cpp:406] Dropout4 <- Convolution5
I1123 11:43:16.409564 19447 net.cpp:380] Dropout4 -> Dropout4
I1123 11:43:16.409615 19447 net.cpp:122] Setting up Dropout4
I1123 11:43:16.409624 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.409627 19447 net.cpp:137] Memory required for data: 38535180
I1123 11:43:16.409629 19447 layer_factory.hpp:77] Creating layer Concat4
I1123 11:43:16.409634 19447 net.cpp:84] Creating Layer Concat4
I1123 11:43:16.409639 19447 net.cpp:406] Concat4 <- Concat3_Concat3_0_split_1
I1123 11:43:16.409644 19447 net.cpp:406] Concat4 <- Dropout4
I1123 11:43:16.409651 19447 net.cpp:380] Concat4 -> Concat4
I1123 11:43:16.409682 19447 net.cpp:122] Setting up Concat4
I1123 11:43:16.409690 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.409694 19447 net.cpp:137] Memory required for data: 40943628
I1123 11:43:16.409698 19447 layer_factory.hpp:77] Creating layer Concat4_Concat4_0_split
I1123 11:43:16.409720 19447 net.cpp:84] Creating Layer Concat4_Concat4_0_split
I1123 11:43:16.409726 19447 net.cpp:406] Concat4_Concat4_0_split <- Concat4
I1123 11:43:16.409734 19447 net.cpp:380] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_0
I1123 11:43:16.409741 19447 net.cpp:380] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_1
I1123 11:43:16.409798 19447 net.cpp:122] Setting up Concat4_Concat4_0_split
I1123 11:43:16.409808 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.409813 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.409817 19447 net.cpp:137] Memory required for data: 45760524
I1123 11:43:16.409821 19447 layer_factory.hpp:77] Creating layer BatchNorm5
I1123 11:43:16.409834 19447 net.cpp:84] Creating Layer BatchNorm5
I1123 11:43:16.409840 19447 net.cpp:406] BatchNorm5 <- Concat4_Concat4_0_split_0
I1123 11:43:16.409847 19447 net.cpp:380] BatchNorm5 -> BatchNorm5
I1123 11:43:16.410082 19447 net.cpp:122] Setting up BatchNorm5
I1123 11:43:16.410092 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.410096 19447 net.cpp:137] Memory required for data: 48168972
I1123 11:43:16.410105 19447 layer_factory.hpp:77] Creating layer Scale5
I1123 11:43:16.410115 19447 net.cpp:84] Creating Layer Scale5
I1123 11:43:16.410118 19447 net.cpp:406] Scale5 <- BatchNorm5
I1123 11:43:16.410126 19447 net.cpp:367] Scale5 -> BatchNorm5 (in-place)
I1123 11:43:16.410174 19447 layer_factory.hpp:77] Creating layer Scale5
I1123 11:43:16.410320 19447 net.cpp:122] Setting up Scale5
I1123 11:43:16.410331 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.410334 19447 net.cpp:137] Memory required for data: 50577420
I1123 11:43:16.410341 19447 layer_factory.hpp:77] Creating layer ReLU5
I1123 11:43:16.410347 19447 net.cpp:84] Creating Layer ReLU5
I1123 11:43:16.410353 19447 net.cpp:406] ReLU5 <- BatchNorm5
I1123 11:43:16.410360 19447 net.cpp:367] ReLU5 -> BatchNorm5 (in-place)
I1123 11:43:16.410523 19447 net.cpp:122] Setting up ReLU5
I1123 11:43:16.410533 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.410537 19447 net.cpp:137] Memory required for data: 52985868
I1123 11:43:16.410538 19447 layer_factory.hpp:77] Creating layer Convolution6
I1123 11:43:16.410547 19447 net.cpp:84] Creating Layer Convolution6
I1123 11:43:16.410549 19447 net.cpp:406] Convolution6 <- BatchNorm5
I1123 11:43:16.410558 19447 net.cpp:380] Convolution6 -> Convolution6
I1123 11:43:16.412703 19447 net.cpp:122] Setting up Convolution6
I1123 11:43:16.412720 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.412725 19447 net.cpp:137] Memory required for data: 53437452
I1123 11:43:16.412732 19447 layer_factory.hpp:77] Creating layer Dropout5
I1123 11:43:16.412739 19447 net.cpp:84] Creating Layer Dropout5
I1123 11:43:16.412744 19447 net.cpp:406] Dropout5 <- Convolution6
I1123 11:43:16.412752 19447 net.cpp:380] Dropout5 -> Dropout5
I1123 11:43:16.412807 19447 net.cpp:122] Setting up Dropout5
I1123 11:43:16.412819 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.412823 19447 net.cpp:137] Memory required for data: 53889036
I1123 11:43:16.412827 19447 layer_factory.hpp:77] Creating layer Concat5
I1123 11:43:16.412835 19447 net.cpp:84] Creating Layer Concat5
I1123 11:43:16.412842 19447 net.cpp:406] Concat5 <- Concat4_Concat4_0_split_1
I1123 11:43:16.412847 19447 net.cpp:406] Concat5 <- Dropout5
I1123 11:43:16.412853 19447 net.cpp:380] Concat5 -> Concat5
I1123 11:43:16.412883 19447 net.cpp:122] Setting up Concat5
I1123 11:43:16.412894 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.412897 19447 net.cpp:137] Memory required for data: 56749068
I1123 11:43:16.412900 19447 layer_factory.hpp:77] Creating layer Concat5_Concat5_0_split
I1123 11:43:16.412907 19447 net.cpp:84] Creating Layer Concat5_Concat5_0_split
I1123 11:43:16.412911 19447 net.cpp:406] Concat5_Concat5_0_split <- Concat5
I1123 11:43:16.412920 19447 net.cpp:380] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_0
I1123 11:43:16.412930 19447 net.cpp:380] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_1
I1123 11:43:16.412981 19447 net.cpp:122] Setting up Concat5_Concat5_0_split
I1123 11:43:16.412992 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.412994 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.412997 19447 net.cpp:137] Memory required for data: 62469132
I1123 11:43:16.413007 19447 layer_factory.hpp:77] Creating layer BatchNorm6
I1123 11:43:16.413013 19447 net.cpp:84] Creating Layer BatchNorm6
I1123 11:43:16.413017 19447 net.cpp:406] BatchNorm6 <- Concat5_Concat5_0_split_0
I1123 11:43:16.413022 19447 net.cpp:380] BatchNorm6 -> BatchNorm6
I1123 11:43:16.413194 19447 net.cpp:122] Setting up BatchNorm6
I1123 11:43:16.413203 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.413204 19447 net.cpp:137] Memory required for data: 65329164
I1123 11:43:16.413213 19447 layer_factory.hpp:77] Creating layer Scale6
I1123 11:43:16.413220 19447 net.cpp:84] Creating Layer Scale6
I1123 11:43:16.413223 19447 net.cpp:406] Scale6 <- BatchNorm6
I1123 11:43:16.413228 19447 net.cpp:367] Scale6 -> BatchNorm6 (in-place)
I1123 11:43:16.413264 19447 layer_factory.hpp:77] Creating layer Scale6
I1123 11:43:16.413367 19447 net.cpp:122] Setting up Scale6
I1123 11:43:16.413375 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.413378 19447 net.cpp:137] Memory required for data: 68189196
I1123 11:43:16.413381 19447 layer_factory.hpp:77] Creating layer ReLU6
I1123 11:43:16.413385 19447 net.cpp:84] Creating Layer ReLU6
I1123 11:43:16.413388 19447 net.cpp:406] ReLU6 <- BatchNorm6
I1123 11:43:16.413393 19447 net.cpp:367] ReLU6 -> BatchNorm6 (in-place)
I1123 11:43:16.413542 19447 net.cpp:122] Setting up ReLU6
I1123 11:43:16.413550 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.413553 19447 net.cpp:137] Memory required for data: 71049228
I1123 11:43:16.413555 19447 layer_factory.hpp:77] Creating layer Convolution7
I1123 11:43:16.413563 19447 net.cpp:84] Creating Layer Convolution7
I1123 11:43:16.413568 19447 net.cpp:406] Convolution7 <- BatchNorm6
I1123 11:43:16.413571 19447 net.cpp:380] Convolution7 -> Convolution7
I1123 11:43:16.416213 19447 net.cpp:122] Setting up Convolution7
I1123 11:43:16.416226 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.416229 19447 net.cpp:137] Memory required for data: 71500812
I1123 11:43:16.416234 19447 layer_factory.hpp:77] Creating layer Dropout6
I1123 11:43:16.416239 19447 net.cpp:84] Creating Layer Dropout6
I1123 11:43:16.416244 19447 net.cpp:406] Dropout6 <- Convolution7
I1123 11:43:16.416250 19447 net.cpp:380] Dropout6 -> Dropout6
I1123 11:43:16.416302 19447 net.cpp:122] Setting up Dropout6
I1123 11:43:16.416311 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.416313 19447 net.cpp:137] Memory required for data: 71952396
I1123 11:43:16.416316 19447 layer_factory.hpp:77] Creating layer Concat6
I1123 11:43:16.416322 19447 net.cpp:84] Creating Layer Concat6
I1123 11:43:16.416327 19447 net.cpp:406] Concat6 <- Concat5_Concat5_0_split_1
I1123 11:43:16.416334 19447 net.cpp:406] Concat6 <- Dropout6
I1123 11:43:16.416342 19447 net.cpp:380] Concat6 -> Concat6
I1123 11:43:16.416379 19447 net.cpp:122] Setting up Concat6
I1123 11:43:16.416388 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.416389 19447 net.cpp:137] Memory required for data: 75264012
I1123 11:43:16.416391 19447 layer_factory.hpp:77] Creating layer Concat6_Concat6_0_split
I1123 11:43:16.416398 19447 net.cpp:84] Creating Layer Concat6_Concat6_0_split
I1123 11:43:16.416402 19447 net.cpp:406] Concat6_Concat6_0_split <- Concat6
I1123 11:43:16.416410 19447 net.cpp:380] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_0
I1123 11:43:16.416424 19447 net.cpp:380] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_1
I1123 11:43:16.416471 19447 net.cpp:122] Setting up Concat6_Concat6_0_split
I1123 11:43:16.416478 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.416481 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.416484 19447 net.cpp:137] Memory required for data: 81887244
I1123 11:43:16.416487 19447 layer_factory.hpp:77] Creating layer BatchNorm7
I1123 11:43:16.416496 19447 net.cpp:84] Creating Layer BatchNorm7
I1123 11:43:16.416501 19447 net.cpp:406] BatchNorm7 <- Concat6_Concat6_0_split_0
I1123 11:43:16.416508 19447 net.cpp:380] BatchNorm7 -> BatchNorm7
I1123 11:43:16.416718 19447 net.cpp:122] Setting up BatchNorm7
I1123 11:43:16.416726 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.416729 19447 net.cpp:137] Memory required for data: 85198860
I1123 11:43:16.416736 19447 layer_factory.hpp:77] Creating layer Scale7
I1123 11:43:16.416743 19447 net.cpp:84] Creating Layer Scale7
I1123 11:43:16.416749 19447 net.cpp:406] Scale7 <- BatchNorm7
I1123 11:43:16.416756 19447 net.cpp:367] Scale7 -> BatchNorm7 (in-place)
I1123 11:43:16.416808 19447 layer_factory.hpp:77] Creating layer Scale7
I1123 11:43:16.416931 19447 net.cpp:122] Setting up Scale7
I1123 11:43:16.416940 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.416942 19447 net.cpp:137] Memory required for data: 88510476
I1123 11:43:16.416947 19447 layer_factory.hpp:77] Creating layer ReLU7
I1123 11:43:16.416951 19447 net.cpp:84] Creating Layer ReLU7
I1123 11:43:16.416955 19447 net.cpp:406] ReLU7 <- BatchNorm7
I1123 11:43:16.416963 19447 net.cpp:367] ReLU7 -> BatchNorm7 (in-place)
I1123 11:43:16.417630 19447 net.cpp:122] Setting up ReLU7
I1123 11:43:16.417641 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.417644 19447 net.cpp:137] Memory required for data: 91822092
I1123 11:43:16.417647 19447 layer_factory.hpp:77] Creating layer Convolution8
I1123 11:43:16.417659 19447 net.cpp:84] Creating Layer Convolution8
I1123 11:43:16.417665 19447 net.cpp:406] Convolution8 <- BatchNorm7
I1123 11:43:16.417675 19447 net.cpp:380] Convolution8 -> Convolution8
I1123 11:43:16.420243 19447 net.cpp:122] Setting up Convolution8
I1123 11:43:16.420253 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.420256 19447 net.cpp:137] Memory required for data: 92273676
I1123 11:43:16.420260 19447 layer_factory.hpp:77] Creating layer Dropout7
I1123 11:43:16.420266 19447 net.cpp:84] Creating Layer Dropout7
I1123 11:43:16.420269 19447 net.cpp:406] Dropout7 <- Convolution8
I1123 11:43:16.420274 19447 net.cpp:380] Dropout7 -> Dropout7
I1123 11:43:16.420313 19447 net.cpp:122] Setting up Dropout7
I1123 11:43:16.420320 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.420322 19447 net.cpp:137] Memory required for data: 92725260
I1123 11:43:16.420325 19447 layer_factory.hpp:77] Creating layer Concat7
I1123 11:43:16.420330 19447 net.cpp:84] Creating Layer Concat7
I1123 11:43:16.420332 19447 net.cpp:406] Concat7 <- Concat6_Concat6_0_split_1
I1123 11:43:16.420336 19447 net.cpp:406] Concat7 <- Dropout7
I1123 11:43:16.420339 19447 net.cpp:380] Concat7 -> Concat7
I1123 11:43:16.420359 19447 net.cpp:122] Setting up Concat7
I1123 11:43:16.420366 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.420367 19447 net.cpp:137] Memory required for data: 96488460
I1123 11:43:16.420369 19447 layer_factory.hpp:77] Creating layer Concat7_Concat7_0_split
I1123 11:43:16.420380 19447 net.cpp:84] Creating Layer Concat7_Concat7_0_split
I1123 11:43:16.420382 19447 net.cpp:406] Concat7_Concat7_0_split <- Concat7
I1123 11:43:16.420387 19447 net.cpp:380] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_0
I1123 11:43:16.420392 19447 net.cpp:380] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_1
I1123 11:43:16.420423 19447 net.cpp:122] Setting up Concat7_Concat7_0_split
I1123 11:43:16.420429 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.420433 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.420434 19447 net.cpp:137] Memory required for data: 104014860
I1123 11:43:16.420436 19447 layer_factory.hpp:77] Creating layer BatchNorm8
I1123 11:43:16.420442 19447 net.cpp:84] Creating Layer BatchNorm8
I1123 11:43:16.420444 19447 net.cpp:406] BatchNorm8 <- Concat7_Concat7_0_split_0
I1123 11:43:16.420449 19447 net.cpp:380] BatchNorm8 -> BatchNorm8
I1123 11:43:16.420621 19447 net.cpp:122] Setting up BatchNorm8
I1123 11:43:16.420627 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.420630 19447 net.cpp:137] Memory required for data: 107778060
I1123 11:43:16.420635 19447 layer_factory.hpp:77] Creating layer Scale8
I1123 11:43:16.420642 19447 net.cpp:84] Creating Layer Scale8
I1123 11:43:16.420655 19447 net.cpp:406] Scale8 <- BatchNorm8
I1123 11:43:16.420660 19447 net.cpp:367] Scale8 -> BatchNorm8 (in-place)
I1123 11:43:16.420708 19447 layer_factory.hpp:77] Creating layer Scale8
I1123 11:43:16.420814 19447 net.cpp:122] Setting up Scale8
I1123 11:43:16.420821 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.420825 19447 net.cpp:137] Memory required for data: 111541260
I1123 11:43:16.420828 19447 layer_factory.hpp:77] Creating layer ReLU8
I1123 11:43:16.420833 19447 net.cpp:84] Creating Layer ReLU8
I1123 11:43:16.420836 19447 net.cpp:406] ReLU8 <- BatchNorm8
I1123 11:43:16.420840 19447 net.cpp:367] ReLU8 -> BatchNorm8 (in-place)
I1123 11:43:16.420989 19447 net.cpp:122] Setting up ReLU8
I1123 11:43:16.420999 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.421000 19447 net.cpp:137] Memory required for data: 115304460
I1123 11:43:16.421003 19447 layer_factory.hpp:77] Creating layer Convolution9
I1123 11:43:16.421010 19447 net.cpp:84] Creating Layer Convolution9
I1123 11:43:16.421013 19447 net.cpp:406] Convolution9 <- BatchNorm8
I1123 11:43:16.421018 19447 net.cpp:380] Convolution9 -> Convolution9
I1123 11:43:16.423414 19447 net.cpp:122] Setting up Convolution9
I1123 11:43:16.423426 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.423429 19447 net.cpp:137] Memory required for data: 115756044
I1123 11:43:16.423434 19447 layer_factory.hpp:77] Creating layer Dropout8
I1123 11:43:16.423441 19447 net.cpp:84] Creating Layer Dropout8
I1123 11:43:16.423444 19447 net.cpp:406] Dropout8 <- Convolution9
I1123 11:43:16.423449 19447 net.cpp:380] Dropout8 -> Dropout8
I1123 11:43:16.423491 19447 net.cpp:122] Setting up Dropout8
I1123 11:43:16.423496 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.423498 19447 net.cpp:137] Memory required for data: 116207628
I1123 11:43:16.423501 19447 layer_factory.hpp:77] Creating layer Concat8
I1123 11:43:16.423506 19447 net.cpp:84] Creating Layer Concat8
I1123 11:43:16.423508 19447 net.cpp:406] Concat8 <- Concat7_Concat7_0_split_1
I1123 11:43:16.423511 19447 net.cpp:406] Concat8 <- Dropout8
I1123 11:43:16.423516 19447 net.cpp:380] Concat8 -> Concat8
I1123 11:43:16.423535 19447 net.cpp:122] Setting up Concat8
I1123 11:43:16.423547 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.423549 19447 net.cpp:137] Memory required for data: 120422412
I1123 11:43:16.423552 19447 layer_factory.hpp:77] Creating layer Concat8_Concat8_0_split
I1123 11:43:16.423555 19447 net.cpp:84] Creating Layer Concat8_Concat8_0_split
I1123 11:43:16.423558 19447 net.cpp:406] Concat8_Concat8_0_split <- Concat8
I1123 11:43:16.423562 19447 net.cpp:380] Concat8_Concat8_0_split -> Concat8_Concat8_0_split_0
I1123 11:43:16.423568 19447 net.cpp:380] Concat8_Concat8_0_split -> Concat8_Concat8_0_split_1
I1123 11:43:16.423597 19447 net.cpp:122] Setting up Concat8_Concat8_0_split
I1123 11:43:16.423604 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.423606 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.423609 19447 net.cpp:137] Memory required for data: 128851980
I1123 11:43:16.423610 19447 layer_factory.hpp:77] Creating layer BatchNorm9
I1123 11:43:16.423616 19447 net.cpp:84] Creating Layer BatchNorm9
I1123 11:43:16.423619 19447 net.cpp:406] BatchNorm9 <- Concat8_Concat8_0_split_0
I1123 11:43:16.423624 19447 net.cpp:380] BatchNorm9 -> BatchNorm9
I1123 11:43:16.423800 19447 net.cpp:122] Setting up BatchNorm9
I1123 11:43:16.423807 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.423810 19447 net.cpp:137] Memory required for data: 133066764
I1123 11:43:16.423815 19447 layer_factory.hpp:77] Creating layer Scale9
I1123 11:43:16.423820 19447 net.cpp:84] Creating Layer Scale9
I1123 11:43:16.423822 19447 net.cpp:406] Scale9 <- BatchNorm9
I1123 11:43:16.423826 19447 net.cpp:367] Scale9 -> BatchNorm9 (in-place)
I1123 11:43:16.423863 19447 layer_factory.hpp:77] Creating layer Scale9
I1123 11:43:16.423969 19447 net.cpp:122] Setting up Scale9
I1123 11:43:16.423976 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.423987 19447 net.cpp:137] Memory required for data: 137281548
I1123 11:43:16.423992 19447 layer_factory.hpp:77] Creating layer ReLU9
I1123 11:43:16.423996 19447 net.cpp:84] Creating Layer ReLU9
I1123 11:43:16.424000 19447 net.cpp:406] ReLU9 <- BatchNorm9
I1123 11:43:16.424002 19447 net.cpp:367] ReLU9 -> BatchNorm9 (in-place)
I1123 11:43:16.424161 19447 net.cpp:122] Setting up ReLU9
I1123 11:43:16.424170 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.424172 19447 net.cpp:137] Memory required for data: 141496332
I1123 11:43:16.424175 19447 layer_factory.hpp:77] Creating layer Convolution10
I1123 11:43:16.424182 19447 net.cpp:84] Creating Layer Convolution10
I1123 11:43:16.424185 19447 net.cpp:406] Convolution10 <- BatchNorm9
I1123 11:43:16.424191 19447 net.cpp:380] Convolution10 -> Convolution10
I1123 11:43:16.426360 19447 net.cpp:122] Setting up Convolution10
I1123 11:43:16.426373 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.426375 19447 net.cpp:137] Memory required for data: 141947916
I1123 11:43:16.426379 19447 layer_factory.hpp:77] Creating layer Dropout9
I1123 11:43:16.426391 19447 net.cpp:84] Creating Layer Dropout9
I1123 11:43:16.426394 19447 net.cpp:406] Dropout9 <- Convolution10
I1123 11:43:16.426399 19447 net.cpp:380] Dropout9 -> Dropout9
I1123 11:43:16.426439 19447 net.cpp:122] Setting up Dropout9
I1123 11:43:16.426445 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.426447 19447 net.cpp:137] Memory required for data: 142399500
I1123 11:43:16.426450 19447 layer_factory.hpp:77] Creating layer Concat9
I1123 11:43:16.426455 19447 net.cpp:84] Creating Layer Concat9
I1123 11:43:16.426457 19447 net.cpp:406] Concat9 <- Concat8_Concat8_0_split_1
I1123 11:43:16.426461 19447 net.cpp:406] Concat9 <- Dropout9
I1123 11:43:16.426465 19447 net.cpp:380] Concat9 -> Concat9
I1123 11:43:16.426486 19447 net.cpp:122] Setting up Concat9
I1123 11:43:16.426491 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.426492 19447 net.cpp:137] Memory required for data: 147065868
I1123 11:43:16.426494 19447 layer_factory.hpp:77] Creating layer Concat9_Concat9_0_split
I1123 11:43:16.426498 19447 net.cpp:84] Creating Layer Concat9_Concat9_0_split
I1123 11:43:16.426501 19447 net.cpp:406] Concat9_Concat9_0_split <- Concat9
I1123 11:43:16.426507 19447 net.cpp:380] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_0
I1123 11:43:16.426512 19447 net.cpp:380] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_1
I1123 11:43:16.426542 19447 net.cpp:122] Setting up Concat9_Concat9_0_split
I1123 11:43:16.426548 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.426551 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.426553 19447 net.cpp:137] Memory required for data: 156398604
I1123 11:43:16.426555 19447 layer_factory.hpp:77] Creating layer BatchNorm10
I1123 11:43:16.426561 19447 net.cpp:84] Creating Layer BatchNorm10
I1123 11:43:16.426564 19447 net.cpp:406] BatchNorm10 <- Concat9_Concat9_0_split_0
I1123 11:43:16.426568 19447 net.cpp:380] BatchNorm10 -> BatchNorm10
I1123 11:43:16.426756 19447 net.cpp:122] Setting up BatchNorm10
I1123 11:43:16.426764 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.426766 19447 net.cpp:137] Memory required for data: 161064972
I1123 11:43:16.426771 19447 layer_factory.hpp:77] Creating layer Scale10
I1123 11:43:16.426777 19447 net.cpp:84] Creating Layer Scale10
I1123 11:43:16.426780 19447 net.cpp:406] Scale10 <- BatchNorm10
I1123 11:43:16.426784 19447 net.cpp:367] Scale10 -> BatchNorm10 (in-place)
I1123 11:43:16.426826 19447 layer_factory.hpp:77] Creating layer Scale10
I1123 11:43:16.426933 19447 net.cpp:122] Setting up Scale10
I1123 11:43:16.426939 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.426941 19447 net.cpp:137] Memory required for data: 165731340
I1123 11:43:16.426946 19447 layer_factory.hpp:77] Creating layer ReLU10
I1123 11:43:16.426951 19447 net.cpp:84] Creating Layer ReLU10
I1123 11:43:16.426954 19447 net.cpp:406] ReLU10 <- BatchNorm10
I1123 11:43:16.426966 19447 net.cpp:367] ReLU10 -> BatchNorm10 (in-place)
I1123 11:43:16.427603 19447 net.cpp:122] Setting up ReLU10
I1123 11:43:16.427614 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.427616 19447 net.cpp:137] Memory required for data: 170397708
I1123 11:43:16.427619 19447 layer_factory.hpp:77] Creating layer Convolution11
I1123 11:43:16.427628 19447 net.cpp:84] Creating Layer Convolution11
I1123 11:43:16.427629 19447 net.cpp:406] Convolution11 <- BatchNorm10
I1123 11:43:16.427635 19447 net.cpp:380] Convolution11 -> Convolution11
I1123 11:43:16.429800 19447 net.cpp:122] Setting up Convolution11
I1123 11:43:16.429813 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.429816 19447 net.cpp:137] Memory required for data: 170849292
I1123 11:43:16.429821 19447 layer_factory.hpp:77] Creating layer Dropout10
I1123 11:43:16.429826 19447 net.cpp:84] Creating Layer Dropout10
I1123 11:43:16.429829 19447 net.cpp:406] Dropout10 <- Convolution11
I1123 11:43:16.429833 19447 net.cpp:380] Dropout10 -> Dropout10
I1123 11:43:16.429874 19447 net.cpp:122] Setting up Dropout10
I1123 11:43:16.429882 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.429883 19447 net.cpp:137] Memory required for data: 171300876
I1123 11:43:16.429885 19447 layer_factory.hpp:77] Creating layer Concat10
I1123 11:43:16.429889 19447 net.cpp:84] Creating Layer Concat10
I1123 11:43:16.429893 19447 net.cpp:406] Concat10 <- Concat9_Concat9_0_split_1
I1123 11:43:16.429895 19447 net.cpp:406] Concat10 <- Dropout10
I1123 11:43:16.429900 19447 net.cpp:380] Concat10 -> Concat10
I1123 11:43:16.429922 19447 net.cpp:122] Setting up Concat10
I1123 11:43:16.429929 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.429930 19447 net.cpp:137] Memory required for data: 176418828
I1123 11:43:16.429932 19447 layer_factory.hpp:77] Creating layer Concat10_Concat10_0_split
I1123 11:43:16.429937 19447 net.cpp:84] Creating Layer Concat10_Concat10_0_split
I1123 11:43:16.429939 19447 net.cpp:406] Concat10_Concat10_0_split <- Concat10
I1123 11:43:16.429944 19447 net.cpp:380] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_0
I1123 11:43:16.429957 19447 net.cpp:380] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_1
I1123 11:43:16.429989 19447 net.cpp:122] Setting up Concat10_Concat10_0_split
I1123 11:43:16.429996 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.429998 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.430001 19447 net.cpp:137] Memory required for data: 186654732
I1123 11:43:16.430002 19447 layer_factory.hpp:77] Creating layer BatchNorm11
I1123 11:43:16.430008 19447 net.cpp:84] Creating Layer BatchNorm11
I1123 11:43:16.430011 19447 net.cpp:406] BatchNorm11 <- Concat10_Concat10_0_split_0
I1123 11:43:16.430014 19447 net.cpp:380] BatchNorm11 -> BatchNorm11
I1123 11:43:16.430188 19447 net.cpp:122] Setting up BatchNorm11
I1123 11:43:16.430196 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.430197 19447 net.cpp:137] Memory required for data: 191772684
I1123 11:43:16.430209 19447 layer_factory.hpp:77] Creating layer Scale11
I1123 11:43:16.430217 19447 net.cpp:84] Creating Layer Scale11
I1123 11:43:16.430219 19447 net.cpp:406] Scale11 <- BatchNorm11
I1123 11:43:16.430223 19447 net.cpp:367] Scale11 -> BatchNorm11 (in-place)
I1123 11:43:16.430258 19447 layer_factory.hpp:77] Creating layer Scale11
I1123 11:43:16.430361 19447 net.cpp:122] Setting up Scale11
I1123 11:43:16.430368 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.430371 19447 net.cpp:137] Memory required for data: 196890636
I1123 11:43:16.430375 19447 layer_factory.hpp:77] Creating layer ReLU11
I1123 11:43:16.430379 19447 net.cpp:84] Creating Layer ReLU11
I1123 11:43:16.430382 19447 net.cpp:406] ReLU11 <- BatchNorm11
I1123 11:43:16.430388 19447 net.cpp:367] ReLU11 -> BatchNorm11 (in-place)
I1123 11:43:16.430550 19447 net.cpp:122] Setting up ReLU11
I1123 11:43:16.430559 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.430562 19447 net.cpp:137] Memory required for data: 202008588
I1123 11:43:16.430573 19447 layer_factory.hpp:77] Creating layer Convolution12
I1123 11:43:16.430583 19447 net.cpp:84] Creating Layer Convolution12
I1123 11:43:16.430585 19447 net.cpp:406] Convolution12 <- BatchNorm11
I1123 11:43:16.430591 19447 net.cpp:380] Convolution12 -> Convolution12
I1123 11:43:16.432819 19447 net.cpp:122] Setting up Convolution12
I1123 11:43:16.432832 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.432835 19447 net.cpp:137] Memory required for data: 202460172
I1123 11:43:16.432849 19447 layer_factory.hpp:77] Creating layer Dropout11
I1123 11:43:16.432857 19447 net.cpp:84] Creating Layer Dropout11
I1123 11:43:16.432860 19447 net.cpp:406] Dropout11 <- Convolution12
I1123 11:43:16.432867 19447 net.cpp:380] Dropout11 -> Dropout11
I1123 11:43:16.432909 19447 net.cpp:122] Setting up Dropout11
I1123 11:43:16.432917 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.432919 19447 net.cpp:137] Memory required for data: 202911756
I1123 11:43:16.432921 19447 layer_factory.hpp:77] Creating layer Concat11
I1123 11:43:16.432926 19447 net.cpp:84] Creating Layer Concat11
I1123 11:43:16.432929 19447 net.cpp:406] Concat11 <- Concat10_Concat10_0_split_1
I1123 11:43:16.432931 19447 net.cpp:406] Concat11 <- Dropout11
I1123 11:43:16.432935 19447 net.cpp:380] Concat11 -> Concat11
I1123 11:43:16.432957 19447 net.cpp:122] Setting up Concat11
I1123 11:43:16.432963 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.432965 19447 net.cpp:137] Memory required for data: 208481292
I1123 11:43:16.432967 19447 layer_factory.hpp:77] Creating layer Concat11_Concat11_0_split
I1123 11:43:16.432971 19447 net.cpp:84] Creating Layer Concat11_Concat11_0_split
I1123 11:43:16.432974 19447 net.cpp:406] Concat11_Concat11_0_split <- Concat11
I1123 11:43:16.432979 19447 net.cpp:380] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_0
I1123 11:43:16.432984 19447 net.cpp:380] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_1
I1123 11:43:16.433015 19447 net.cpp:122] Setting up Concat11_Concat11_0_split
I1123 11:43:16.433022 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.433024 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.433027 19447 net.cpp:137] Memory required for data: 219620364
I1123 11:43:16.433028 19447 layer_factory.hpp:77] Creating layer BatchNorm12
I1123 11:43:16.433032 19447 net.cpp:84] Creating Layer BatchNorm12
I1123 11:43:16.433034 19447 net.cpp:406] BatchNorm12 <- Concat11_Concat11_0_split_0
I1123 11:43:16.433039 19447 net.cpp:380] BatchNorm12 -> BatchNorm12
I1123 11:43:16.433215 19447 net.cpp:122] Setting up BatchNorm12
I1123 11:43:16.433223 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.433224 19447 net.cpp:137] Memory required for data: 225189900
I1123 11:43:16.433230 19447 layer_factory.hpp:77] Creating layer Scale12
I1123 11:43:16.433234 19447 net.cpp:84] Creating Layer Scale12
I1123 11:43:16.433238 19447 net.cpp:406] Scale12 <- BatchNorm12
I1123 11:43:16.433240 19447 net.cpp:367] Scale12 -> BatchNorm12 (in-place)
I1123 11:43:16.433277 19447 layer_factory.hpp:77] Creating layer Scale12
I1123 11:43:16.433382 19447 net.cpp:122] Setting up Scale12
I1123 11:43:16.433389 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.433392 19447 net.cpp:137] Memory required for data: 230759436
I1123 11:43:16.433396 19447 layer_factory.hpp:77] Creating layer ReLU12
I1123 11:43:16.433400 19447 net.cpp:84] Creating Layer ReLU12
I1123 11:43:16.433403 19447 net.cpp:406] ReLU12 <- BatchNorm12
I1123 11:43:16.433406 19447 net.cpp:367] ReLU12 -> BatchNorm12 (in-place)
I1123 11:43:16.433568 19447 net.cpp:122] Setting up ReLU12
I1123 11:43:16.433576 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.433578 19447 net.cpp:137] Memory required for data: 236328972
I1123 11:43:16.433583 19447 layer_factory.hpp:77] Creating layer Convolution13
I1123 11:43:16.433589 19447 net.cpp:84] Creating Layer Convolution13
I1123 11:43:16.433593 19447 net.cpp:406] Convolution13 <- BatchNorm12
I1123 11:43:16.433606 19447 net.cpp:380] Convolution13 -> Convolution13
I1123 11:43:16.435802 19447 net.cpp:122] Setting up Convolution13
I1123 11:43:16.435817 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.435820 19447 net.cpp:137] Memory required for data: 236780556
I1123 11:43:16.435825 19447 layer_factory.hpp:77] Creating layer Dropout12
I1123 11:43:16.435830 19447 net.cpp:84] Creating Layer Dropout12
I1123 11:43:16.435832 19447 net.cpp:406] Dropout12 <- Convolution13
I1123 11:43:16.435837 19447 net.cpp:380] Dropout12 -> Dropout12
I1123 11:43:16.435879 19447 net.cpp:122] Setting up Dropout12
I1123 11:43:16.435886 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.435889 19447 net.cpp:137] Memory required for data: 237232140
I1123 11:43:16.435890 19447 layer_factory.hpp:77] Creating layer Concat12
I1123 11:43:16.435894 19447 net.cpp:84] Creating Layer Concat12
I1123 11:43:16.435897 19447 net.cpp:406] Concat12 <- Concat11_Concat11_0_split_1
I1123 11:43:16.435901 19447 net.cpp:406] Concat12 <- Dropout12
I1123 11:43:16.435905 19447 net.cpp:380] Concat12 -> Concat12
I1123 11:43:16.435927 19447 net.cpp:122] Setting up Concat12
I1123 11:43:16.435935 19447 net.cpp:129] Top shape: 3 160 56 56 (1505280)
I1123 11:43:16.435936 19447 net.cpp:137] Memory required for data: 243253260
I1123 11:43:16.435938 19447 layer_factory.hpp:77] Creating layer BatchNorm13
I1123 11:43:16.435942 19447 net.cpp:84] Creating Layer BatchNorm13
I1123 11:43:16.435945 19447 net.cpp:406] BatchNorm13 <- Concat12
I1123 11:43:16.435950 19447 net.cpp:380] BatchNorm13 -> BatchNorm13
I1123 11:43:16.436132 19447 net.cpp:122] Setting up BatchNorm13
I1123 11:43:16.436139 19447 net.cpp:129] Top shape: 3 160 56 56 (1505280)
I1123 11:43:16.436141 19447 net.cpp:137] Memory required for data: 249274380
I1123 11:43:16.436146 19447 layer_factory.hpp:77] Creating layer Scale13
I1123 11:43:16.436151 19447 net.cpp:84] Creating Layer Scale13
I1123 11:43:16.436154 19447 net.cpp:406] Scale13 <- BatchNorm13
I1123 11:43:16.436158 19447 net.cpp:367] Scale13 -> BatchNorm13 (in-place)
I1123 11:43:16.436193 19447 layer_factory.hpp:77] Creating layer Scale13
I1123 11:43:16.436300 19447 net.cpp:122] Setting up Scale13
I1123 11:43:16.436307 19447 net.cpp:129] Top shape: 3 160 56 56 (1505280)
I1123 11:43:16.436309 19447 net.cpp:137] Memory required for data: 255295500
I1123 11:43:16.436313 19447 layer_factory.hpp:77] Creating layer ReLU13
I1123 11:43:16.436317 19447 net.cpp:84] Creating Layer ReLU13
I1123 11:43:16.436321 19447 net.cpp:406] ReLU13 <- BatchNorm13
I1123 11:43:16.436324 19447 net.cpp:367] ReLU13 -> BatchNorm13 (in-place)
I1123 11:43:16.436962 19447 net.cpp:122] Setting up ReLU13
I1123 11:43:16.436975 19447 net.cpp:129] Top shape: 3 160 56 56 (1505280)
I1123 11:43:16.436977 19447 net.cpp:137] Memory required for data: 261316620
I1123 11:43:16.436980 19447 layer_factory.hpp:77] Creating layer Pooling3
I1123 11:43:16.436985 19447 net.cpp:84] Creating Layer Pooling3
I1123 11:43:16.436988 19447 net.cpp:406] Pooling3 <- BatchNorm13
I1123 11:43:16.436995 19447 net.cpp:380] Pooling3 -> Pooling3
I1123 11:43:16.437314 19447 net.cpp:122] Setting up Pooling3
I1123 11:43:16.437326 19447 net.cpp:129] Top shape: 3 160 54 54 (1399680)
I1123 11:43:16.437330 19447 net.cpp:137] Memory required for data: 266915340
I1123 11:43:16.437331 19447 layer_factory.hpp:77] Creating layer InnerProduct1
I1123 11:43:16.437340 19447 net.cpp:84] Creating Layer InnerProduct1
I1123 11:43:16.437341 19447 net.cpp:406] InnerProduct1 <- Pooling3
I1123 11:43:16.437346 19447 net.cpp:380] InnerProduct1 -> InnerProduct1
I1123 11:43:16.523613 19447 net.cpp:122] Setting up InnerProduct1
I1123 11:43:16.523653 19447 net.cpp:129] Top shape: 3 16 (48)
I1123 11:43:16.523658 19447 net.cpp:137] Memory required for data: 266915532
I1123 11:43:16.523670 19447 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1123 11:43:16.523988 19447 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1123 11:43:16.524001 19447 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1
I1123 11:43:16.524008 19447 net.cpp:406] SoftmaxWithLoss1 <- label
I1123 11:43:16.524032 19447 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1123 11:43:16.524046 19447 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1123 11:43:16.525138 19447 net.cpp:122] Setting up SoftmaxWithLoss1
I1123 11:43:16.525153 19447 net.cpp:129] Top shape: (1)
I1123 11:43:16.525156 19447 net.cpp:132]     with loss weight 1
I1123 11:43:16.526476 19447 net.cpp:137] Memory required for data: 266915536
I1123 11:43:16.526484 19447 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1123 11:43:16.526496 19447 net.cpp:198] InnerProduct1 needs backward computation.
I1123 11:43:16.526500 19447 net.cpp:198] Pooling3 needs backward computation.
I1123 11:43:16.526504 19447 net.cpp:198] ReLU13 needs backward computation.
I1123 11:43:16.526506 19447 net.cpp:198] Scale13 needs backward computation.
I1123 11:43:16.526510 19447 net.cpp:198] BatchNorm13 needs backward computation.
I1123 11:43:16.526512 19447 net.cpp:198] Concat12 needs backward computation.
I1123 11:43:16.526516 19447 net.cpp:198] Dropout12 needs backward computation.
I1123 11:43:16.526520 19447 net.cpp:198] Convolution13 needs backward computation.
I1123 11:43:16.526522 19447 net.cpp:198] ReLU12 needs backward computation.
I1123 11:43:16.526525 19447 net.cpp:198] Scale12 needs backward computation.
I1123 11:43:16.526528 19447 net.cpp:198] BatchNorm12 needs backward computation.
I1123 11:43:16.526532 19447 net.cpp:198] Concat11_Concat11_0_split needs backward computation.
I1123 11:43:16.526535 19447 net.cpp:198] Concat11 needs backward computation.
I1123 11:43:16.526547 19447 net.cpp:198] Dropout11 needs backward computation.
I1123 11:43:16.526551 19447 net.cpp:198] Convolution12 needs backward computation.
I1123 11:43:16.526556 19447 net.cpp:198] ReLU11 needs backward computation.
I1123 11:43:16.526558 19447 net.cpp:198] Scale11 needs backward computation.
I1123 11:43:16.526561 19447 net.cpp:198] BatchNorm11 needs backward computation.
I1123 11:43:16.526564 19447 net.cpp:198] Concat10_Concat10_0_split needs backward computation.
I1123 11:43:16.526568 19447 net.cpp:198] Concat10 needs backward computation.
I1123 11:43:16.526572 19447 net.cpp:198] Dropout10 needs backward computation.
I1123 11:43:16.526576 19447 net.cpp:198] Convolution11 needs backward computation.
I1123 11:43:16.526578 19447 net.cpp:198] ReLU10 needs backward computation.
I1123 11:43:16.526582 19447 net.cpp:198] Scale10 needs backward computation.
I1123 11:43:16.526584 19447 net.cpp:198] BatchNorm10 needs backward computation.
I1123 11:43:16.526588 19447 net.cpp:198] Concat9_Concat9_0_split needs backward computation.
I1123 11:43:16.526592 19447 net.cpp:198] Concat9 needs backward computation.
I1123 11:43:16.526595 19447 net.cpp:198] Dropout9 needs backward computation.
I1123 11:43:16.526599 19447 net.cpp:198] Convolution10 needs backward computation.
I1123 11:43:16.526602 19447 net.cpp:198] ReLU9 needs backward computation.
I1123 11:43:16.526605 19447 net.cpp:198] Scale9 needs backward computation.
I1123 11:43:16.526608 19447 net.cpp:198] BatchNorm9 needs backward computation.
I1123 11:43:16.526612 19447 net.cpp:198] Concat8_Concat8_0_split needs backward computation.
I1123 11:43:16.526615 19447 net.cpp:198] Concat8 needs backward computation.
I1123 11:43:16.526619 19447 net.cpp:198] Dropout8 needs backward computation.
I1123 11:43:16.526623 19447 net.cpp:198] Convolution9 needs backward computation.
I1123 11:43:16.526628 19447 net.cpp:198] ReLU8 needs backward computation.
I1123 11:43:16.526630 19447 net.cpp:198] Scale8 needs backward computation.
I1123 11:43:16.526633 19447 net.cpp:198] BatchNorm8 needs backward computation.
I1123 11:43:16.526636 19447 net.cpp:198] Concat7_Concat7_0_split needs backward computation.
I1123 11:43:16.526640 19447 net.cpp:198] Concat7 needs backward computation.
I1123 11:43:16.526644 19447 net.cpp:198] Dropout7 needs backward computation.
I1123 11:43:16.526648 19447 net.cpp:198] Convolution8 needs backward computation.
I1123 11:43:16.526651 19447 net.cpp:198] ReLU7 needs backward computation.
I1123 11:43:16.526654 19447 net.cpp:198] Scale7 needs backward computation.
I1123 11:43:16.526669 19447 net.cpp:198] BatchNorm7 needs backward computation.
I1123 11:43:16.526672 19447 net.cpp:198] Concat6_Concat6_0_split needs backward computation.
I1123 11:43:16.526675 19447 net.cpp:198] Concat6 needs backward computation.
I1123 11:43:16.526679 19447 net.cpp:198] Dropout6 needs backward computation.
I1123 11:43:16.526682 19447 net.cpp:198] Convolution7 needs backward computation.
I1123 11:43:16.526685 19447 net.cpp:198] ReLU6 needs backward computation.
I1123 11:43:16.526688 19447 net.cpp:198] Scale6 needs backward computation.
I1123 11:43:16.526691 19447 net.cpp:198] BatchNorm6 needs backward computation.
I1123 11:43:16.526695 19447 net.cpp:198] Concat5_Concat5_0_split needs backward computation.
I1123 11:43:16.526698 19447 net.cpp:198] Concat5 needs backward computation.
I1123 11:43:16.526702 19447 net.cpp:198] Dropout5 needs backward computation.
I1123 11:43:16.526705 19447 net.cpp:198] Convolution6 needs backward computation.
I1123 11:43:16.526708 19447 net.cpp:198] ReLU5 needs backward computation.
I1123 11:43:16.526711 19447 net.cpp:198] Scale5 needs backward computation.
I1123 11:43:16.526715 19447 net.cpp:198] BatchNorm5 needs backward computation.
I1123 11:43:16.526718 19447 net.cpp:198] Concat4_Concat4_0_split needs backward computation.
I1123 11:43:16.526721 19447 net.cpp:198] Concat4 needs backward computation.
I1123 11:43:16.526726 19447 net.cpp:198] Dropout4 needs backward computation.
I1123 11:43:16.526729 19447 net.cpp:198] Convolution5 needs backward computation.
I1123 11:43:16.526732 19447 net.cpp:198] ReLU4 needs backward computation.
I1123 11:43:16.526736 19447 net.cpp:198] Scale4 needs backward computation.
I1123 11:43:16.526738 19447 net.cpp:198] BatchNorm4 needs backward computation.
I1123 11:43:16.526741 19447 net.cpp:198] Concat3_Concat3_0_split needs backward computation.
I1123 11:43:16.526744 19447 net.cpp:198] Concat3 needs backward computation.
I1123 11:43:16.526748 19447 net.cpp:198] Dropout3 needs backward computation.
I1123 11:43:16.526752 19447 net.cpp:198] Convolution4 needs backward computation.
I1123 11:43:16.526756 19447 net.cpp:198] ReLU3 needs backward computation.
I1123 11:43:16.526758 19447 net.cpp:198] Scale3 needs backward computation.
I1123 11:43:16.526762 19447 net.cpp:198] BatchNorm3 needs backward computation.
I1123 11:43:16.526764 19447 net.cpp:198] Concat2_Concat2_0_split needs backward computation.
I1123 11:43:16.526767 19447 net.cpp:198] Concat2 needs backward computation.
I1123 11:43:16.526772 19447 net.cpp:198] Dropout2 needs backward computation.
I1123 11:43:16.526775 19447 net.cpp:198] Convolution3 needs backward computation.
I1123 11:43:16.526778 19447 net.cpp:198] ReLU2 needs backward computation.
I1123 11:43:16.526782 19447 net.cpp:198] Scale2 needs backward computation.
I1123 11:43:16.526784 19447 net.cpp:198] BatchNorm2 needs backward computation.
I1123 11:43:16.526787 19447 net.cpp:198] Concat1_Concat1_0_split needs backward computation.
I1123 11:43:16.526790 19447 net.cpp:198] Concat1 needs backward computation.
I1123 11:43:16.526795 19447 net.cpp:198] Dropout1 needs backward computation.
I1123 11:43:16.526798 19447 net.cpp:198] Convolution2 needs backward computation.
I1123 11:43:16.526803 19447 net.cpp:198] ReLU1 needs backward computation.
I1123 11:43:16.526806 19447 net.cpp:198] Scale1 needs backward computation.
I1123 11:43:16.526809 19447 net.cpp:198] BatchNorm1 needs backward computation.
I1123 11:43:16.526813 19447 net.cpp:198] Convolution1_Convolution1_0_split needs backward computation.
I1123 11:43:16.526818 19447 net.cpp:198] Convolution1 needs backward computation.
I1123 11:43:16.526821 19447 net.cpp:198] Convolution1_3x3 needs backward computation.
I1123 11:43:16.526825 19447 net.cpp:200] Data1 does not need backward computation.
I1123 11:43:16.526829 19447 net.cpp:242] This network produces output SoftmaxWithLoss1
I1123 11:43:16.526893 19447 net.cpp:255] Network initialization done.
I1123 11:43:16.550007 19447 solver.cpp:172] Creating test net (#0) specified by test_net file: /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/test_densenet.prototxt
I1123 11:43:16.550622 19447 net.cpp:51] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "Data1"
  type: "Data"
  top: "Data1"
  top: "label"
  include {
    phase: TEST
  }
  transform_param {
    mirror: false
    crop_size: 224
    mean_file: "/home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/mean.binaryproto"
  }
  data_param {
    source: "/home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/img_val_lmdb"
    batch_size: 3
    backend: LMDB
  }
}
layer {
  name: "Convolution1_3x3"
  type: "Convolution"
  bottom: "Data1"
  top: "Convolution1_3x3"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Convolution1"
  type: "Convolution"
  bottom: "Convolution1_3x3"
  top: "Convolution1"
  convolution_param {
    num_output: 16
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 2
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "BatchNorm1"
  type: "BatchNorm"
  bottom: "Convolution1"
  top: "BatchNorm1"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale1"
  type: "Scale"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "ReLU1"
  type: "ReLU"
  bottom: "BatchNorm1"
  top: "BatchNorm1"
}
layer {
  name: "Convolution2"
  type: "Convolution"
  bottom: "BatchNorm1"
  top: "Convolution2"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout1"
  type: "Dropout"
  bottom: "Convolution2"
  top: "Dropout1"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat1"
  type: "Concat"
  bottom: "Convolution1"
  bottom: "Dropout1"
  top: "Concat1"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm2"
  type: "BatchNorm"
  bottom: "Concat1"
  top: "BatchNorm2"
  scale_param {
    bias_term: true
  }
}
layer {
  name: "Scale2"
  type: "Scale"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU2"
  type: "ReLU"
  bottom: "BatchNorm2"
  top: "BatchNorm2"
}
layer {
  name: "Convolution3"
  type: "Convolution"
  bottom: "BatchNorm2"
  top: "Convolution3"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout2"
  type: "Dropout"
  bottom: "Convolution3"
  top: "Dropout2"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat2"
  type: "Concat"
  bottom: "Concat1"
  bottom: "Dropout2"
  top: "Concat2"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm3"
  type: "BatchNorm"
  bottom: "Concat2"
  top: "BatchNorm3"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale3"
  type: "Scale"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU3"
  type: "ReLU"
  bottom: "BatchNorm3"
  top: "BatchNorm3"
}
layer {
  name: "Convolution4"
  type: "Convolution"
  bottom: "BatchNorm3"
  top: "Convolution4"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout3"
  type: "Dropout"
  bottom: "Convolution4"
  top: "Dropout3"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat3"
  type: "Concat"
  bottom: "Concat2"
  bottom: "Dropout3"
  top: "Concat3"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm4"
  type: "BatchNorm"
  bottom: "Concat3"
  top: "BatchNorm4"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale4"
  type: "Scale"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU4"
  type: "ReLU"
  bottom: "BatchNorm4"
  top: "BatchNorm4"
}
layer {
  name: "Convolution5"
  type: "Convolution"
  bottom: "BatchNorm4"
  top: "Convolution5"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout4"
  type: "Dropout"
  bottom: "Convolution5"
  top: "Dropout4"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat4"
  type: "Concat"
  bottom: "Concat3"
  bottom: "Dropout4"
  top: "Concat4"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm5"
  type: "BatchNorm"
  bottom: "Concat4"
  top: "BatchNorm5"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale5"
  type: "Scale"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU5"
  type: "ReLU"
  bottom: "BatchNorm5"
  top: "BatchNorm5"
}
layer {
  name: "Convolution6"
  type: "Convolution"
  bottom: "BatchNorm5"
  top: "Convolution6"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout5"
  type: "Dropout"
  bottom: "Convolution6"
  top: "Dropout5"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat5"
  type: "Concat"
  bottom: "Concat4"
  bottom: "Dropout5"
  top: "Concat5"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm6"
  type: "BatchNorm"
  bottom: "Concat5"
  top: "BatchNorm6"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale6"
  type: "Scale"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU6"
  type: "ReLU"
  bottom: "BatchNorm6"
  top: "BatchNorm6"
}
layer {
  name: "Convolution7"
  type: "Convolution"
  bottom: "BatchNorm6"
  top: "Convolution7"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout6"
  type: "Dropout"
  bottom: "Convolution7"
  top: "Dropout6"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat6"
  type: "Concat"
  bottom: "Concat5"
  bottom: "Dropout6"
  top: "Concat6"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm7"
  type: "BatchNorm"
  bottom: "Concat6"
  top: "BatchNorm7"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale7"
  type: "Scale"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU7"
  type: "ReLU"
  bottom: "BatchNorm7"
  top: "BatchNorm7"
}
layer {
  name: "Convolution8"
  type: "Convolution"
  bottom: "BatchNorm7"
  top: "Convolution8"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout7"
  type: "Dropout"
  bottom: "Convolution8"
  top: "Dropout7"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat7"
  type: "Concat"
  bottom: "Concat6"
  bottom: "Dropout7"
  top: "Concat7"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm8"
  type: "BatchNorm"
  bottom: "Concat7"
  top: "BatchNorm8"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale8"
  type: "Scale"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU8"
  type: "ReLU"
  bottom: "BatchNorm8"
  top: "BatchNorm8"
}
layer {
  name: "Convolution9"
  type: "Convolution"
  bottom: "BatchNorm8"
  top: "Convolution9"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout8"
  type: "Dropout"
  bottom: "Convolution9"
  top: "Dropout8"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat8"
  type: "Concat"
  bottom: "Concat7"
  bottom: "Dropout8"
  top: "Concat8"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm9"
  type: "BatchNorm"
  bottom: "Concat8"
  top: "BatchNorm9"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale9"
  type: "Scale"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU9"
  type: "ReLU"
  bottom: "BatchNorm9"
  top: "BatchNorm9"
}
layer {
  name: "Convolution10"
  type: "Convolution"
  bottom: "BatchNorm9"
  top: "Convolution10"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout9"
  type: "Dropout"
  bottom: "Convolution10"
  top: "Dropout9"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat9"
  type: "Concat"
  bottom: "Concat8"
  bottom: "Dropout9"
  top: "Concat9"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm10"
  type: "BatchNorm"
  bottom: "Concat9"
  top: "BatchNorm10"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale10"
  type: "Scale"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU10"
  type: "ReLU"
  bottom: "BatchNorm10"
  top: "BatchNorm10"
}
layer {
  name: "Convolution11"
  type: "Convolution"
  bottom: "BatchNorm10"
  top: "Convolution11"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout10"
  type: "Dropout"
  bottom: "Convolution11"
  top: "Dropout10"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat10"
  type: "Concat"
  bottom: "Concat9"
  bottom: "Dropout10"
  top: "Concat10"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm11"
  type: "BatchNorm"
  bottom: "Concat10"
  top: "BatchNorm11"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale11"
  type: "Scale"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU11"
  type: "ReLU"
  bottom: "BatchNorm11"
  top: "BatchNorm11"
}
layer {
  name: "Convolution12"
  type: "Convolution"
  bottom: "BatchNorm11"
  top: "Convolution12"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout11"
  type: "Dropout"
  bottom: "Convolution12"
  top: "Dropout11"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat11"
  type: "Concat"
  bottom: "Concat10"
  bottom: "Dropout11"
  top: "Concat11"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm12"
  type: "BatchNorm"
  bottom: "Concat11"
  top: "BatchNorm12"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale12"
  type: "Scale"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU12"
  type: "ReLU"
  bottom: "BatchNorm12"
  top: "BatchNorm12"
}
layer {
  name: "Convolution13"
  type: "Convolution"
  bottom: "BatchNorm12"
  top: "Convolution13"
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 3
    stride: 1
    weight_filler {
      type: "msra"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "Dropout12"
  type: "Dropout"
  bottom: "Convolution13"
  top: "Dropout12"
  dropout_param {
    dropout_ratio: 0.2
  }
}
layer {
  name: "Concat12"
  type: "Concat"
  bottom: "Concat11"
  bottom: "Dropout12"
  top: "Concat12"
  concat_param {
    axis: 1
  }
}
layer {
  name: "BatchNorm13"
  type: "BatchNorm"
  bottom: "Concat12"
  top: "BatchNorm13"
  batch_norm_param {
    use_global_stats: false
  }
}
layer {
  name: "Scale13"
  type: "Scale"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
  scale_param {
    filler {
      value: 1
    }
    bias_term: true
    bias_filler {
      value: 0
    }
  }
}
layer {
  name: "ReLU13"
  type: "ReLU"
  bottom: "BatchNorm13"
  top: "BatchNorm13"
}
layer {
  name: "Pooling3"
  type: "Pooling"
  bottom: "BatchNorm13"
  top: "Pooling3"
  pooling_param {
    pool: AVE
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "InnerProduct1"
  type: "InnerProduct"
  bottom: "Pooling3"
  top: "InnerProduct1"
  inner_product_param {
    num_output: 16
    bias_term: true
    weight_filler {
      type: "xavier"
    }
    bias_filler {
      type: "constant"
    }
  }
}
layer {
  name: "SoftmaxWithLoss1"
  type: "SoftmaxWithLoss"
  bottom: "InnerProduct1"
  bottom: "label"
  top: "SoftmaxWithLoss1"
}
layer {
  name: "Accuracy1"
  type: "Accuracy"
  bottom: "InnerProduct1"
  bottom: "label"
  top: "Accuracy1"
  include {
    phase: TEST
  }
}
I1123 11:43:16.550935 19447 layer_factory.hpp:77] Creating layer Data1
I1123 11:43:16.582311 19447 db_lmdb.cpp:35] Opened lmdb /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/img_val_lmdb
I1123 11:43:16.587040 19447 net.cpp:84] Creating Layer Data1
I1123 11:43:16.587075 19447 net.cpp:380] Data1 -> Data1
I1123 11:43:16.587095 19447 net.cpp:380] Data1 -> label
I1123 11:43:16.587107 19447 data_transformer.cpp:25] Loading mean file from: /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/mean.binaryproto
I1123 11:43:16.617609 19447 data_layer.cpp:45] output data size: 3,3,224,224
I1123 11:43:16.627887 19447 net.cpp:122] Setting up Data1
I1123 11:43:16.627915 19447 net.cpp:129] Top shape: 3 3 224 224 (451584)
I1123 11:43:16.627923 19447 net.cpp:129] Top shape: 3 (3)
I1123 11:43:16.627925 19447 net.cpp:137] Memory required for data: 1806348
I1123 11:43:16.627933 19447 layer_factory.hpp:77] Creating layer label_Data1_1_split
I1123 11:43:16.627949 19447 net.cpp:84] Creating Layer label_Data1_1_split
I1123 11:43:16.627956 19447 net.cpp:406] label_Data1_1_split <- label
I1123 11:43:16.627967 19447 net.cpp:380] label_Data1_1_split -> label_Data1_1_split_0
I1123 11:43:16.627985 19447 net.cpp:380] label_Data1_1_split -> label_Data1_1_split_1
I1123 11:43:16.628078 19447 net.cpp:122] Setting up label_Data1_1_split
I1123 11:43:16.628089 19447 net.cpp:129] Top shape: 3 (3)
I1123 11:43:16.628093 19447 net.cpp:129] Top shape: 3 (3)
I1123 11:43:16.628096 19447 net.cpp:137] Memory required for data: 1806372
I1123 11:43:16.628100 19447 layer_factory.hpp:77] Creating layer Convolution1_3x3
I1123 11:43:16.628119 19447 net.cpp:84] Creating Layer Convolution1_3x3
I1123 11:43:16.628129 19447 net.cpp:406] Convolution1_3x3 <- Data1
I1123 11:43:16.628139 19447 net.cpp:380] Convolution1_3x3 -> Convolution1_3x3
I1123 11:43:16.631326 19447 net.cpp:122] Setting up Convolution1_3x3
I1123 11:43:16.631362 19447 net.cpp:129] Top shape: 3 16 112 112 (602112)
I1123 11:43:16.631367 19447 net.cpp:137] Memory required for data: 4214820
I1123 11:43:16.631377 19447 layer_factory.hpp:77] Creating layer Convolution1
I1123 11:43:16.631387 19447 net.cpp:84] Creating Layer Convolution1
I1123 11:43:16.631392 19447 net.cpp:406] Convolution1 <- Convolution1_3x3
I1123 11:43:16.631398 19447 net.cpp:380] Convolution1 -> Convolution1
I1123 11:43:16.633098 19447 net.cpp:122] Setting up Convolution1
I1123 11:43:16.633118 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.633124 19447 net.cpp:137] Memory required for data: 4816932
I1123 11:43:16.633136 19447 layer_factory.hpp:77] Creating layer Convolution1_Convolution1_0_split
I1123 11:43:16.633146 19447 net.cpp:84] Creating Layer Convolution1_Convolution1_0_split
I1123 11:43:16.633152 19447 net.cpp:406] Convolution1_Convolution1_0_split <- Convolution1
I1123 11:43:16.633160 19447 net.cpp:380] Convolution1_Convolution1_0_split -> Convolution1_Convolution1_0_split_0
I1123 11:43:16.633170 19447 net.cpp:380] Convolution1_Convolution1_0_split -> Convolution1_Convolution1_0_split_1
I1123 11:43:16.633235 19447 net.cpp:122] Setting up Convolution1_Convolution1_0_split
I1123 11:43:16.633247 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.633255 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.633258 19447 net.cpp:137] Memory required for data: 6021156
I1123 11:43:16.633263 19447 layer_factory.hpp:77] Creating layer BatchNorm1
I1123 11:43:16.633272 19447 net.cpp:84] Creating Layer BatchNorm1
I1123 11:43:16.633294 19447 net.cpp:406] BatchNorm1 <- Convolution1_Convolution1_0_split_0
I1123 11:43:16.633303 19447 net.cpp:380] BatchNorm1 -> BatchNorm1
I1123 11:43:16.633646 19447 net.cpp:122] Setting up BatchNorm1
I1123 11:43:16.633661 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.633666 19447 net.cpp:137] Memory required for data: 6623268
I1123 11:43:16.633684 19447 layer_factory.hpp:77] Creating layer Scale1
I1123 11:43:16.633694 19447 net.cpp:84] Creating Layer Scale1
I1123 11:43:16.633699 19447 net.cpp:406] Scale1 <- BatchNorm1
I1123 11:43:16.633707 19447 net.cpp:367] Scale1 -> BatchNorm1 (in-place)
I1123 11:43:16.633780 19447 layer_factory.hpp:77] Creating layer Scale1
I1123 11:43:16.633975 19447 net.cpp:122] Setting up Scale1
I1123 11:43:16.633987 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.633992 19447 net.cpp:137] Memory required for data: 7225380
I1123 11:43:16.634001 19447 layer_factory.hpp:77] Creating layer ReLU1
I1123 11:43:16.634011 19447 net.cpp:84] Creating Layer ReLU1
I1123 11:43:16.634016 19447 net.cpp:406] ReLU1 <- BatchNorm1
I1123 11:43:16.634023 19447 net.cpp:367] ReLU1 -> BatchNorm1 (in-place)
I1123 11:43:16.634956 19447 net.cpp:122] Setting up ReLU1
I1123 11:43:16.634977 19447 net.cpp:129] Top shape: 3 16 56 56 (150528)
I1123 11:43:16.634984 19447 net.cpp:137] Memory required for data: 7827492
I1123 11:43:16.634989 19447 layer_factory.hpp:77] Creating layer Convolution2
I1123 11:43:16.635005 19447 net.cpp:84] Creating Layer Convolution2
I1123 11:43:16.635011 19447 net.cpp:406] Convolution2 <- BatchNorm1
I1123 11:43:16.635022 19447 net.cpp:380] Convolution2 -> Convolution2
I1123 11:43:16.638131 19447 net.cpp:122] Setting up Convolution2
I1123 11:43:16.638145 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.638149 19447 net.cpp:137] Memory required for data: 8279076
I1123 11:43:16.638154 19447 layer_factory.hpp:77] Creating layer Dropout1
I1123 11:43:16.638159 19447 net.cpp:84] Creating Layer Dropout1
I1123 11:43:16.638161 19447 net.cpp:406] Dropout1 <- Convolution2
I1123 11:43:16.638166 19447 net.cpp:380] Dropout1 -> Dropout1
I1123 11:43:16.638208 19447 net.cpp:122] Setting up Dropout1
I1123 11:43:16.638216 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.638218 19447 net.cpp:137] Memory required for data: 8730660
I1123 11:43:16.638221 19447 layer_factory.hpp:77] Creating layer Concat1
I1123 11:43:16.638226 19447 net.cpp:84] Creating Layer Concat1
I1123 11:43:16.638242 19447 net.cpp:406] Concat1 <- Convolution1_Convolution1_0_split_1
I1123 11:43:16.638247 19447 net.cpp:406] Concat1 <- Dropout1
I1123 11:43:16.638257 19447 net.cpp:380] Concat1 -> Concat1
I1123 11:43:16.638283 19447 net.cpp:122] Setting up Concat1
I1123 11:43:16.638288 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.638290 19447 net.cpp:137] Memory required for data: 9784356
I1123 11:43:16.638293 19447 layer_factory.hpp:77] Creating layer Concat1_Concat1_0_split
I1123 11:43:16.638298 19447 net.cpp:84] Creating Layer Concat1_Concat1_0_split
I1123 11:43:16.638300 19447 net.cpp:406] Concat1_Concat1_0_split <- Concat1
I1123 11:43:16.638304 19447 net.cpp:380] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_0
I1123 11:43:16.638309 19447 net.cpp:380] Concat1_Concat1_0_split -> Concat1_Concat1_0_split_1
I1123 11:43:16.638340 19447 net.cpp:122] Setting up Concat1_Concat1_0_split
I1123 11:43:16.638346 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.638350 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.638351 19447 net.cpp:137] Memory required for data: 11891748
I1123 11:43:16.638353 19447 layer_factory.hpp:77] Creating layer BatchNorm2
I1123 11:43:16.638358 19447 net.cpp:84] Creating Layer BatchNorm2
I1123 11:43:16.638361 19447 net.cpp:406] BatchNorm2 <- Concat1_Concat1_0_split_0
I1123 11:43:16.638365 19447 net.cpp:380] BatchNorm2 -> BatchNorm2
I1123 11:43:16.638562 19447 net.cpp:122] Setting up BatchNorm2
I1123 11:43:16.638569 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.638572 19447 net.cpp:137] Memory required for data: 12945444
I1123 11:43:16.638581 19447 layer_factory.hpp:77] Creating layer Scale2
I1123 11:43:16.638587 19447 net.cpp:84] Creating Layer Scale2
I1123 11:43:16.638589 19447 net.cpp:406] Scale2 <- BatchNorm2
I1123 11:43:16.638593 19447 net.cpp:367] Scale2 -> BatchNorm2 (in-place)
I1123 11:43:16.638633 19447 layer_factory.hpp:77] Creating layer Scale2
I1123 11:43:16.638743 19447 net.cpp:122] Setting up Scale2
I1123 11:43:16.638751 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.638753 19447 net.cpp:137] Memory required for data: 13999140
I1123 11:43:16.638757 19447 layer_factory.hpp:77] Creating layer ReLU2
I1123 11:43:16.638762 19447 net.cpp:84] Creating Layer ReLU2
I1123 11:43:16.638764 19447 net.cpp:406] ReLU2 <- BatchNorm2
I1123 11:43:16.638768 19447 net.cpp:367] ReLU2 -> BatchNorm2 (in-place)
I1123 11:43:16.638926 19447 net.cpp:122] Setting up ReLU2
I1123 11:43:16.638936 19447 net.cpp:129] Top shape: 3 28 56 56 (263424)
I1123 11:43:16.638937 19447 net.cpp:137] Memory required for data: 15052836
I1123 11:43:16.638941 19447 layer_factory.hpp:77] Creating layer Convolution3
I1123 11:43:16.638947 19447 net.cpp:84] Creating Layer Convolution3
I1123 11:43:16.638949 19447 net.cpp:406] Convolution3 <- BatchNorm2
I1123 11:43:16.638954 19447 net.cpp:380] Convolution3 -> Convolution3
I1123 11:43:16.640605 19447 net.cpp:122] Setting up Convolution3
I1123 11:43:16.640617 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.640620 19447 net.cpp:137] Memory required for data: 15504420
I1123 11:43:16.640625 19447 layer_factory.hpp:77] Creating layer Dropout2
I1123 11:43:16.640631 19447 net.cpp:84] Creating Layer Dropout2
I1123 11:43:16.640635 19447 net.cpp:406] Dropout2 <- Convolution3
I1123 11:43:16.640638 19447 net.cpp:380] Dropout2 -> Dropout2
I1123 11:43:16.640679 19447 net.cpp:122] Setting up Dropout2
I1123 11:43:16.640692 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.640697 19447 net.cpp:137] Memory required for data: 15956004
I1123 11:43:16.640702 19447 layer_factory.hpp:77] Creating layer Concat2
I1123 11:43:16.640705 19447 net.cpp:84] Creating Layer Concat2
I1123 11:43:16.640708 19447 net.cpp:406] Concat2 <- Concat1_Concat1_0_split_1
I1123 11:43:16.640712 19447 net.cpp:406] Concat2 <- Dropout2
I1123 11:43:16.640717 19447 net.cpp:380] Concat2 -> Concat2
I1123 11:43:16.640741 19447 net.cpp:122] Setting up Concat2
I1123 11:43:16.640748 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.640844 19447 net.cpp:137] Memory required for data: 17461284
I1123 11:43:16.640849 19447 layer_factory.hpp:77] Creating layer Concat2_Concat2_0_split
I1123 11:43:16.640854 19447 net.cpp:84] Creating Layer Concat2_Concat2_0_split
I1123 11:43:16.640856 19447 net.cpp:406] Concat2_Concat2_0_split <- Concat2
I1123 11:43:16.640861 19447 net.cpp:380] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_0
I1123 11:43:16.640866 19447 net.cpp:380] Concat2_Concat2_0_split -> Concat2_Concat2_0_split_1
I1123 11:43:16.640904 19447 net.cpp:122] Setting up Concat2_Concat2_0_split
I1123 11:43:16.640911 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.640914 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.640916 19447 net.cpp:137] Memory required for data: 20471844
I1123 11:43:16.640918 19447 layer_factory.hpp:77] Creating layer BatchNorm3
I1123 11:43:16.640923 19447 net.cpp:84] Creating Layer BatchNorm3
I1123 11:43:16.640925 19447 net.cpp:406] BatchNorm3 <- Concat2_Concat2_0_split_0
I1123 11:43:16.640929 19447 net.cpp:380] BatchNorm3 -> BatchNorm3
I1123 11:43:16.641137 19447 net.cpp:122] Setting up BatchNorm3
I1123 11:43:16.641147 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.641151 19447 net.cpp:137] Memory required for data: 21977124
I1123 11:43:16.641165 19447 layer_factory.hpp:77] Creating layer Scale3
I1123 11:43:16.641175 19447 net.cpp:84] Creating Layer Scale3
I1123 11:43:16.641180 19447 net.cpp:406] Scale3 <- BatchNorm3
I1123 11:43:16.641185 19447 net.cpp:367] Scale3 -> BatchNorm3 (in-place)
I1123 11:43:16.641233 19447 layer_factory.hpp:77] Creating layer Scale3
I1123 11:43:16.641384 19447 net.cpp:122] Setting up Scale3
I1123 11:43:16.641394 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.641396 19447 net.cpp:137] Memory required for data: 23482404
I1123 11:43:16.641402 19447 layer_factory.hpp:77] Creating layer ReLU3
I1123 11:43:16.641405 19447 net.cpp:84] Creating Layer ReLU3
I1123 11:43:16.641408 19447 net.cpp:406] ReLU3 <- BatchNorm3
I1123 11:43:16.641412 19447 net.cpp:367] ReLU3 -> BatchNorm3 (in-place)
I1123 11:43:16.642055 19447 net.cpp:122] Setting up ReLU3
I1123 11:43:16.642066 19447 net.cpp:129] Top shape: 3 40 56 56 (376320)
I1123 11:43:16.642069 19447 net.cpp:137] Memory required for data: 24987684
I1123 11:43:16.642071 19447 layer_factory.hpp:77] Creating layer Convolution4
I1123 11:43:16.642079 19447 net.cpp:84] Creating Layer Convolution4
I1123 11:43:16.642082 19447 net.cpp:406] Convolution4 <- BatchNorm3
I1123 11:43:16.642099 19447 net.cpp:380] Convolution4 -> Convolution4
I1123 11:43:16.643586 19447 net.cpp:122] Setting up Convolution4
I1123 11:43:16.643599 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.643601 19447 net.cpp:137] Memory required for data: 25439268
I1123 11:43:16.643606 19447 layer_factory.hpp:77] Creating layer Dropout3
I1123 11:43:16.643611 19447 net.cpp:84] Creating Layer Dropout3
I1123 11:43:16.643615 19447 net.cpp:406] Dropout3 <- Convolution4
I1123 11:43:16.643618 19447 net.cpp:380] Dropout3 -> Dropout3
I1123 11:43:16.643658 19447 net.cpp:122] Setting up Dropout3
I1123 11:43:16.643666 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.643667 19447 net.cpp:137] Memory required for data: 25890852
I1123 11:43:16.643669 19447 layer_factory.hpp:77] Creating layer Concat3
I1123 11:43:16.643674 19447 net.cpp:84] Creating Layer Concat3
I1123 11:43:16.643676 19447 net.cpp:406] Concat3 <- Concat2_Concat2_0_split_1
I1123 11:43:16.643681 19447 net.cpp:406] Concat3 <- Dropout3
I1123 11:43:16.643684 19447 net.cpp:380] Concat3 -> Concat3
I1123 11:43:16.643707 19447 net.cpp:122] Setting up Concat3
I1123 11:43:16.643715 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.643718 19447 net.cpp:137] Memory required for data: 27847716
I1123 11:43:16.643719 19447 layer_factory.hpp:77] Creating layer Concat3_Concat3_0_split
I1123 11:43:16.643724 19447 net.cpp:84] Creating Layer Concat3_Concat3_0_split
I1123 11:43:16.643726 19447 net.cpp:406] Concat3_Concat3_0_split <- Concat3
I1123 11:43:16.643731 19447 net.cpp:380] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_0
I1123 11:43:16.643746 19447 net.cpp:380] Concat3_Concat3_0_split -> Concat3_Concat3_0_split_1
I1123 11:43:16.643786 19447 net.cpp:122] Setting up Concat3_Concat3_0_split
I1123 11:43:16.643792 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.643795 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.643797 19447 net.cpp:137] Memory required for data: 31761444
I1123 11:43:16.643800 19447 layer_factory.hpp:77] Creating layer BatchNorm4
I1123 11:43:16.643805 19447 net.cpp:84] Creating Layer BatchNorm4
I1123 11:43:16.643806 19447 net.cpp:406] BatchNorm4 <- Concat3_Concat3_0_split_0
I1123 11:43:16.643810 19447 net.cpp:380] BatchNorm4 -> BatchNorm4
I1123 11:43:16.644201 19447 net.cpp:122] Setting up BatchNorm4
I1123 11:43:16.644210 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.644213 19447 net.cpp:137] Memory required for data: 33718308
I1123 11:43:16.644219 19447 layer_factory.hpp:77] Creating layer Scale4
I1123 11:43:16.644224 19447 net.cpp:84] Creating Layer Scale4
I1123 11:43:16.644227 19447 net.cpp:406] Scale4 <- BatchNorm4
I1123 11:43:16.644232 19447 net.cpp:367] Scale4 -> BatchNorm4 (in-place)
I1123 11:43:16.644270 19447 layer_factory.hpp:77] Creating layer Scale4
I1123 11:43:16.644382 19447 net.cpp:122] Setting up Scale4
I1123 11:43:16.644389 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.644392 19447 net.cpp:137] Memory required for data: 35675172
I1123 11:43:16.644397 19447 layer_factory.hpp:77] Creating layer ReLU4
I1123 11:43:16.644400 19447 net.cpp:84] Creating Layer ReLU4
I1123 11:43:16.644403 19447 net.cpp:406] ReLU4 <- BatchNorm4
I1123 11:43:16.644407 19447 net.cpp:367] ReLU4 -> BatchNorm4 (in-place)
I1123 11:43:16.645110 19447 net.cpp:122] Setting up ReLU4
I1123 11:43:16.645123 19447 net.cpp:129] Top shape: 3 52 56 56 (489216)
I1123 11:43:16.645125 19447 net.cpp:137] Memory required for data: 37632036
I1123 11:43:16.645128 19447 layer_factory.hpp:77] Creating layer Convolution5
I1123 11:43:16.645136 19447 net.cpp:84] Creating Layer Convolution5
I1123 11:43:16.645139 19447 net.cpp:406] Convolution5 <- BatchNorm4
I1123 11:43:16.645144 19447 net.cpp:380] Convolution5 -> Convolution5
I1123 11:43:16.646916 19447 net.cpp:122] Setting up Convolution5
I1123 11:43:16.646931 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.646934 19447 net.cpp:137] Memory required for data: 38083620
I1123 11:43:16.646939 19447 layer_factory.hpp:77] Creating layer Dropout4
I1123 11:43:16.646945 19447 net.cpp:84] Creating Layer Dropout4
I1123 11:43:16.646947 19447 net.cpp:406] Dropout4 <- Convolution5
I1123 11:43:16.646952 19447 net.cpp:380] Dropout4 -> Dropout4
I1123 11:43:16.646998 19447 net.cpp:122] Setting up Dropout4
I1123 11:43:16.647006 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.647007 19447 net.cpp:137] Memory required for data: 38535204
I1123 11:43:16.647009 19447 layer_factory.hpp:77] Creating layer Concat4
I1123 11:43:16.647014 19447 net.cpp:84] Creating Layer Concat4
I1123 11:43:16.647017 19447 net.cpp:406] Concat4 <- Concat3_Concat3_0_split_1
I1123 11:43:16.647020 19447 net.cpp:406] Concat4 <- Dropout4
I1123 11:43:16.647024 19447 net.cpp:380] Concat4 -> Concat4
I1123 11:43:16.647047 19447 net.cpp:122] Setting up Concat4
I1123 11:43:16.647053 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.647054 19447 net.cpp:137] Memory required for data: 40943652
I1123 11:43:16.647058 19447 layer_factory.hpp:77] Creating layer Concat4_Concat4_0_split
I1123 11:43:16.647064 19447 net.cpp:84] Creating Layer Concat4_Concat4_0_split
I1123 11:43:16.647068 19447 net.cpp:406] Concat4_Concat4_0_split <- Concat4
I1123 11:43:16.647071 19447 net.cpp:380] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_0
I1123 11:43:16.647078 19447 net.cpp:380] Concat4_Concat4_0_split -> Concat4_Concat4_0_split_1
I1123 11:43:16.647120 19447 net.cpp:122] Setting up Concat4_Concat4_0_split
I1123 11:43:16.647127 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.647130 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.647143 19447 net.cpp:137] Memory required for data: 45760548
I1123 11:43:16.647146 19447 layer_factory.hpp:77] Creating layer BatchNorm5
I1123 11:43:16.647150 19447 net.cpp:84] Creating Layer BatchNorm5
I1123 11:43:16.647153 19447 net.cpp:406] BatchNorm5 <- Concat4_Concat4_0_split_0
I1123 11:43:16.647157 19447 net.cpp:380] BatchNorm5 -> BatchNorm5
I1123 11:43:16.647390 19447 net.cpp:122] Setting up BatchNorm5
I1123 11:43:16.647402 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.647403 19447 net.cpp:137] Memory required for data: 48168996
I1123 11:43:16.647409 19447 layer_factory.hpp:77] Creating layer Scale5
I1123 11:43:16.647415 19447 net.cpp:84] Creating Layer Scale5
I1123 11:43:16.647418 19447 net.cpp:406] Scale5 <- BatchNorm5
I1123 11:43:16.647423 19447 net.cpp:367] Scale5 -> BatchNorm5 (in-place)
I1123 11:43:16.647462 19447 layer_factory.hpp:77] Creating layer Scale5
I1123 11:43:16.647603 19447 net.cpp:122] Setting up Scale5
I1123 11:43:16.647613 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.647614 19447 net.cpp:137] Memory required for data: 50577444
I1123 11:43:16.647619 19447 layer_factory.hpp:77] Creating layer ReLU5
I1123 11:43:16.647624 19447 net.cpp:84] Creating Layer ReLU5
I1123 11:43:16.647626 19447 net.cpp:406] ReLU5 <- BatchNorm5
I1123 11:43:16.647629 19447 net.cpp:367] ReLU5 -> BatchNorm5 (in-place)
I1123 11:43:16.647922 19447 net.cpp:122] Setting up ReLU5
I1123 11:43:16.647933 19447 net.cpp:129] Top shape: 3 64 56 56 (602112)
I1123 11:43:16.647935 19447 net.cpp:137] Memory required for data: 52985892
I1123 11:43:16.647938 19447 layer_factory.hpp:77] Creating layer Convolution6
I1123 11:43:16.647945 19447 net.cpp:84] Creating Layer Convolution6
I1123 11:43:16.647950 19447 net.cpp:406] Convolution6 <- BatchNorm5
I1123 11:43:16.647958 19447 net.cpp:380] Convolution6 -> Convolution6
I1123 11:43:16.649799 19447 net.cpp:122] Setting up Convolution6
I1123 11:43:16.649812 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.649816 19447 net.cpp:137] Memory required for data: 53437476
I1123 11:43:16.649821 19447 layer_factory.hpp:77] Creating layer Dropout5
I1123 11:43:16.649826 19447 net.cpp:84] Creating Layer Dropout5
I1123 11:43:16.649828 19447 net.cpp:406] Dropout5 <- Convolution6
I1123 11:43:16.649833 19447 net.cpp:380] Dropout5 -> Dropout5
I1123 11:43:16.649874 19447 net.cpp:122] Setting up Dropout5
I1123 11:43:16.649881 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.649884 19447 net.cpp:137] Memory required for data: 53889060
I1123 11:43:16.649886 19447 layer_factory.hpp:77] Creating layer Concat5
I1123 11:43:16.649900 19447 net.cpp:84] Creating Layer Concat5
I1123 11:43:16.649905 19447 net.cpp:406] Concat5 <- Concat4_Concat4_0_split_1
I1123 11:43:16.649909 19447 net.cpp:406] Concat5 <- Dropout5
I1123 11:43:16.649914 19447 net.cpp:380] Concat5 -> Concat5
I1123 11:43:16.649938 19447 net.cpp:122] Setting up Concat5
I1123 11:43:16.649945 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.649947 19447 net.cpp:137] Memory required for data: 56749092
I1123 11:43:16.649950 19447 layer_factory.hpp:77] Creating layer Concat5_Concat5_0_split
I1123 11:43:16.649955 19447 net.cpp:84] Creating Layer Concat5_Concat5_0_split
I1123 11:43:16.649957 19447 net.cpp:406] Concat5_Concat5_0_split <- Concat5
I1123 11:43:16.649961 19447 net.cpp:380] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_0
I1123 11:43:16.649966 19447 net.cpp:380] Concat5_Concat5_0_split -> Concat5_Concat5_0_split_1
I1123 11:43:16.650001 19447 net.cpp:122] Setting up Concat5_Concat5_0_split
I1123 11:43:16.650007 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.650009 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.650012 19447 net.cpp:137] Memory required for data: 62469156
I1123 11:43:16.650014 19447 layer_factory.hpp:77] Creating layer BatchNorm6
I1123 11:43:16.650019 19447 net.cpp:84] Creating Layer BatchNorm6
I1123 11:43:16.650022 19447 net.cpp:406] BatchNorm6 <- Concat5_Concat5_0_split_0
I1123 11:43:16.650038 19447 net.cpp:380] BatchNorm6 -> BatchNorm6
I1123 11:43:16.650270 19447 net.cpp:122] Setting up BatchNorm6
I1123 11:43:16.650281 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.650283 19447 net.cpp:137] Memory required for data: 65329188
I1123 11:43:16.650293 19447 layer_factory.hpp:77] Creating layer Scale6
I1123 11:43:16.650300 19447 net.cpp:84] Creating Layer Scale6
I1123 11:43:16.650302 19447 net.cpp:406] Scale6 <- BatchNorm6
I1123 11:43:16.650306 19447 net.cpp:367] Scale6 -> BatchNorm6 (in-place)
I1123 11:43:16.650348 19447 layer_factory.hpp:77] Creating layer Scale6
I1123 11:43:16.650466 19447 net.cpp:122] Setting up Scale6
I1123 11:43:16.650475 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.650476 19447 net.cpp:137] Memory required for data: 68189220
I1123 11:43:16.650481 19447 layer_factory.hpp:77] Creating layer ReLU6
I1123 11:43:16.650485 19447 net.cpp:84] Creating Layer ReLU6
I1123 11:43:16.650488 19447 net.cpp:406] ReLU6 <- BatchNorm6
I1123 11:43:16.650492 19447 net.cpp:367] ReLU6 -> BatchNorm6 (in-place)
I1123 11:43:16.651178 19447 net.cpp:122] Setting up ReLU6
I1123 11:43:16.651190 19447 net.cpp:129] Top shape: 3 76 56 56 (715008)
I1123 11:43:16.651193 19447 net.cpp:137] Memory required for data: 71049252
I1123 11:43:16.651196 19447 layer_factory.hpp:77] Creating layer Convolution7
I1123 11:43:16.651203 19447 net.cpp:84] Creating Layer Convolution7
I1123 11:43:16.651207 19447 net.cpp:406] Convolution7 <- BatchNorm6
I1123 11:43:16.651212 19447 net.cpp:380] Convolution7 -> Convolution7
I1123 11:43:16.653265 19447 net.cpp:122] Setting up Convolution7
I1123 11:43:16.653282 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.653285 19447 net.cpp:137] Memory required for data: 71500836
I1123 11:43:16.653290 19447 layer_factory.hpp:77] Creating layer Dropout6
I1123 11:43:16.653297 19447 net.cpp:84] Creating Layer Dropout6
I1123 11:43:16.653301 19447 net.cpp:406] Dropout6 <- Convolution7
I1123 11:43:16.653306 19447 net.cpp:380] Dropout6 -> Dropout6
I1123 11:43:16.653353 19447 net.cpp:122] Setting up Dropout6
I1123 11:43:16.653362 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.653363 19447 net.cpp:137] Memory required for data: 71952420
I1123 11:43:16.653367 19447 layer_factory.hpp:77] Creating layer Concat6
I1123 11:43:16.653372 19447 net.cpp:84] Creating Layer Concat6
I1123 11:43:16.653379 19447 net.cpp:406] Concat6 <- Concat5_Concat5_0_split_1
I1123 11:43:16.653384 19447 net.cpp:406] Concat6 <- Dropout6
I1123 11:43:16.653389 19447 net.cpp:380] Concat6 -> Concat6
I1123 11:43:16.653415 19447 net.cpp:122] Setting up Concat6
I1123 11:43:16.653422 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.653425 19447 net.cpp:137] Memory required for data: 75264036
I1123 11:43:16.653429 19447 layer_factory.hpp:77] Creating layer Concat6_Concat6_0_split
I1123 11:43:16.653439 19447 net.cpp:84] Creating Layer Concat6_Concat6_0_split
I1123 11:43:16.653445 19447 net.cpp:406] Concat6_Concat6_0_split <- Concat6
I1123 11:43:16.653453 19447 net.cpp:380] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_0
I1123 11:43:16.653462 19447 net.cpp:380] Concat6_Concat6_0_split -> Concat6_Concat6_0_split_1
I1123 11:43:16.653532 19447 net.cpp:122] Setting up Concat6_Concat6_0_split
I1123 11:43:16.653547 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.653555 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.653559 19447 net.cpp:137] Memory required for data: 81887268
I1123 11:43:16.653563 19447 layer_factory.hpp:77] Creating layer BatchNorm7
I1123 11:43:16.653570 19447 net.cpp:84] Creating Layer BatchNorm7
I1123 11:43:16.653573 19447 net.cpp:406] BatchNorm7 <- Concat6_Concat6_0_split_0
I1123 11:43:16.653579 19447 net.cpp:380] BatchNorm7 -> BatchNorm7
I1123 11:43:16.653802 19447 net.cpp:122] Setting up BatchNorm7
I1123 11:43:16.653811 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.653813 19447 net.cpp:137] Memory required for data: 85198884
I1123 11:43:16.653820 19447 layer_factory.hpp:77] Creating layer Scale7
I1123 11:43:16.653836 19447 net.cpp:84] Creating Layer Scale7
I1123 11:43:16.653838 19447 net.cpp:406] Scale7 <- BatchNorm7
I1123 11:43:16.653842 19447 net.cpp:367] Scale7 -> BatchNorm7 (in-place)
I1123 11:43:16.653887 19447 layer_factory.hpp:77] Creating layer Scale7
I1123 11:43:16.654011 19447 net.cpp:122] Setting up Scale7
I1123 11:43:16.654019 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.654021 19447 net.cpp:137] Memory required for data: 88510500
I1123 11:43:16.654036 19447 layer_factory.hpp:77] Creating layer ReLU7
I1123 11:43:16.654040 19447 net.cpp:84] Creating Layer ReLU7
I1123 11:43:16.654043 19447 net.cpp:406] ReLU7 <- BatchNorm7
I1123 11:43:16.654049 19447 net.cpp:367] ReLU7 -> BatchNorm7 (in-place)
I1123 11:43:16.654703 19447 net.cpp:122] Setting up ReLU7
I1123 11:43:16.654716 19447 net.cpp:129] Top shape: 3 88 56 56 (827904)
I1123 11:43:16.654718 19447 net.cpp:137] Memory required for data: 91822116
I1123 11:43:16.654721 19447 layer_factory.hpp:77] Creating layer Convolution8
I1123 11:43:16.654731 19447 net.cpp:84] Creating Layer Convolution8
I1123 11:43:16.654736 19447 net.cpp:406] Convolution8 <- BatchNorm7
I1123 11:43:16.654741 19447 net.cpp:380] Convolution8 -> Convolution8
I1123 11:43:16.657124 19447 net.cpp:122] Setting up Convolution8
I1123 11:43:16.657135 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.657138 19447 net.cpp:137] Memory required for data: 92273700
I1123 11:43:16.657143 19447 layer_factory.hpp:77] Creating layer Dropout7
I1123 11:43:16.657150 19447 net.cpp:84] Creating Layer Dropout7
I1123 11:43:16.657153 19447 net.cpp:406] Dropout7 <- Convolution8
I1123 11:43:16.657158 19447 net.cpp:380] Dropout7 -> Dropout7
I1123 11:43:16.657202 19447 net.cpp:122] Setting up Dropout7
I1123 11:43:16.657208 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.657210 19447 net.cpp:137] Memory required for data: 92725284
I1123 11:43:16.657213 19447 layer_factory.hpp:77] Creating layer Concat7
I1123 11:43:16.657217 19447 net.cpp:84] Creating Layer Concat7
I1123 11:43:16.657220 19447 net.cpp:406] Concat7 <- Concat6_Concat6_0_split_1
I1123 11:43:16.657224 19447 net.cpp:406] Concat7 <- Dropout7
I1123 11:43:16.657228 19447 net.cpp:380] Concat7 -> Concat7
I1123 11:43:16.657251 19447 net.cpp:122] Setting up Concat7
I1123 11:43:16.657258 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.657260 19447 net.cpp:137] Memory required for data: 96488484
I1123 11:43:16.657263 19447 layer_factory.hpp:77] Creating layer Concat7_Concat7_0_split
I1123 11:43:16.657269 19447 net.cpp:84] Creating Layer Concat7_Concat7_0_split
I1123 11:43:16.657272 19447 net.cpp:406] Concat7_Concat7_0_split <- Concat7
I1123 11:43:16.657277 19447 net.cpp:380] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_0
I1123 11:43:16.657282 19447 net.cpp:380] Concat7_Concat7_0_split -> Concat7_Concat7_0_split_1
I1123 11:43:16.657316 19447 net.cpp:122] Setting up Concat7_Concat7_0_split
I1123 11:43:16.657325 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.657327 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.657330 19447 net.cpp:137] Memory required for data: 104014884
I1123 11:43:16.657331 19447 layer_factory.hpp:77] Creating layer BatchNorm8
I1123 11:43:16.657336 19447 net.cpp:84] Creating Layer BatchNorm8
I1123 11:43:16.657340 19447 net.cpp:406] BatchNorm8 <- Concat7_Concat7_0_split_0
I1123 11:43:16.657343 19447 net.cpp:380] BatchNorm8 -> BatchNorm8
I1123 11:43:16.657637 19447 net.cpp:122] Setting up BatchNorm8
I1123 11:43:16.657650 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.657655 19447 net.cpp:137] Memory required for data: 107778084
I1123 11:43:16.657663 19447 layer_factory.hpp:77] Creating layer Scale8
I1123 11:43:16.657672 19447 net.cpp:84] Creating Layer Scale8
I1123 11:43:16.657677 19447 net.cpp:406] Scale8 <- BatchNorm8
I1123 11:43:16.657685 19447 net.cpp:367] Scale8 -> BatchNorm8 (in-place)
I1123 11:43:16.657742 19447 layer_factory.hpp:77] Creating layer Scale8
I1123 11:43:16.657927 19447 net.cpp:122] Setting up Scale8
I1123 11:43:16.657951 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.657956 19447 net.cpp:137] Memory required for data: 111541284
I1123 11:43:16.657963 19447 layer_factory.hpp:77] Creating layer ReLU8
I1123 11:43:16.657970 19447 net.cpp:84] Creating Layer ReLU8
I1123 11:43:16.657977 19447 net.cpp:406] ReLU8 <- BatchNorm8
I1123 11:43:16.657985 19447 net.cpp:367] ReLU8 -> BatchNorm8 (in-place)
I1123 11:43:16.658213 19447 net.cpp:122] Setting up ReLU8
I1123 11:43:16.658226 19447 net.cpp:129] Top shape: 3 100 56 56 (940800)
I1123 11:43:16.658231 19447 net.cpp:137] Memory required for data: 115304484
I1123 11:43:16.658234 19447 layer_factory.hpp:77] Creating layer Convolution9
I1123 11:43:16.658246 19447 net.cpp:84] Creating Layer Convolution9
I1123 11:43:16.658252 19447 net.cpp:406] Convolution9 <- BatchNorm8
I1123 11:43:16.658259 19447 net.cpp:380] Convolution9 -> Convolution9
I1123 11:43:16.661018 19447 net.cpp:122] Setting up Convolution9
I1123 11:43:16.661033 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.661037 19447 net.cpp:137] Memory required for data: 115756068
I1123 11:43:16.661044 19447 layer_factory.hpp:77] Creating layer Dropout8
I1123 11:43:16.661051 19447 net.cpp:84] Creating Layer Dropout8
I1123 11:43:16.661056 19447 net.cpp:406] Dropout8 <- Convolution9
I1123 11:43:16.661062 19447 net.cpp:380] Dropout8 -> Dropout8
I1123 11:43:16.661113 19447 net.cpp:122] Setting up Dropout8
I1123 11:43:16.661120 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.661123 19447 net.cpp:137] Memory required for data: 116207652
I1123 11:43:16.661125 19447 layer_factory.hpp:77] Creating layer Concat8
I1123 11:43:16.661129 19447 net.cpp:84] Creating Layer Concat8
I1123 11:43:16.661132 19447 net.cpp:406] Concat8 <- Concat7_Concat7_0_split_1
I1123 11:43:16.661136 19447 net.cpp:406] Concat8 <- Dropout8
I1123 11:43:16.661141 19447 net.cpp:380] Concat8 -> Concat8
I1123 11:43:16.661167 19447 net.cpp:122] Setting up Concat8
I1123 11:43:16.661173 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.661176 19447 net.cpp:137] Memory required for data: 120422436
I1123 11:43:16.661178 19447 layer_factory.hpp:77] Creating layer Concat8_Concat8_0_split
I1123 11:43:16.661182 19447 net.cpp:84] Creating Layer Concat8_Concat8_0_split
I1123 11:43:16.661185 19447 net.cpp:406] Concat8_Concat8_0_split <- Concat8
I1123 11:43:16.661190 19447 net.cpp:380] Concat8_Concat8_0_split -> Concat8_Concat8_0_split_0
I1123 11:43:16.661195 19447 net.cpp:380] Concat8_Concat8_0_split -> Concat8_Concat8_0_split_1
I1123 11:43:16.661236 19447 net.cpp:122] Setting up Concat8_Concat8_0_split
I1123 11:43:16.661242 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.661245 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.661248 19447 net.cpp:137] Memory required for data: 128852004
I1123 11:43:16.661250 19447 layer_factory.hpp:77] Creating layer BatchNorm9
I1123 11:43:16.661257 19447 net.cpp:84] Creating Layer BatchNorm9
I1123 11:43:16.661259 19447 net.cpp:406] BatchNorm9 <- Concat8_Concat8_0_split_0
I1123 11:43:16.661265 19447 net.cpp:380] BatchNorm9 -> BatchNorm9
I1123 11:43:16.661530 19447 net.cpp:122] Setting up BatchNorm9
I1123 11:43:16.661541 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.661543 19447 net.cpp:137] Memory required for data: 133066788
I1123 11:43:16.661550 19447 layer_factory.hpp:77] Creating layer Scale9
I1123 11:43:16.661554 19447 net.cpp:84] Creating Layer Scale9
I1123 11:43:16.661557 19447 net.cpp:406] Scale9 <- BatchNorm9
I1123 11:43:16.661563 19447 net.cpp:367] Scale9 -> BatchNorm9 (in-place)
I1123 11:43:16.661622 19447 layer_factory.hpp:77] Creating layer Scale9
I1123 11:43:16.661770 19447 net.cpp:122] Setting up Scale9
I1123 11:43:16.661779 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.661782 19447 net.cpp:137] Memory required for data: 137281572
I1123 11:43:16.661788 19447 layer_factory.hpp:77] Creating layer ReLU9
I1123 11:43:16.661797 19447 net.cpp:84] Creating Layer ReLU9
I1123 11:43:16.661816 19447 net.cpp:406] ReLU9 <- BatchNorm9
I1123 11:43:16.661826 19447 net.cpp:367] ReLU9 -> BatchNorm9 (in-place)
I1123 11:43:16.662518 19447 net.cpp:122] Setting up ReLU9
I1123 11:43:16.662529 19447 net.cpp:129] Top shape: 3 112 56 56 (1053696)
I1123 11:43:16.662531 19447 net.cpp:137] Memory required for data: 141496356
I1123 11:43:16.662535 19447 layer_factory.hpp:77] Creating layer Convolution10
I1123 11:43:16.662557 19447 net.cpp:84] Creating Layer Convolution10
I1123 11:43:16.662565 19447 net.cpp:406] Convolution10 <- BatchNorm9
I1123 11:43:16.662575 19447 net.cpp:380] Convolution10 -> Convolution10
I1123 11:43:16.664479 19447 net.cpp:122] Setting up Convolution10
I1123 11:43:16.664490 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.664494 19447 net.cpp:137] Memory required for data: 141947940
I1123 11:43:16.664499 19447 layer_factory.hpp:77] Creating layer Dropout9
I1123 11:43:16.664503 19447 net.cpp:84] Creating Layer Dropout9
I1123 11:43:16.664506 19447 net.cpp:406] Dropout9 <- Convolution10
I1123 11:43:16.664512 19447 net.cpp:380] Dropout9 -> Dropout9
I1123 11:43:16.664578 19447 net.cpp:122] Setting up Dropout9
I1123 11:43:16.664587 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.664589 19447 net.cpp:137] Memory required for data: 142399524
I1123 11:43:16.664593 19447 layer_factory.hpp:77] Creating layer Concat9
I1123 11:43:16.664602 19447 net.cpp:84] Creating Layer Concat9
I1123 11:43:16.664609 19447 net.cpp:406] Concat9 <- Concat8_Concat8_0_split_1
I1123 11:43:16.664615 19447 net.cpp:406] Concat9 <- Dropout9
I1123 11:43:16.664623 19447 net.cpp:380] Concat9 -> Concat9
I1123 11:43:16.664667 19447 net.cpp:122] Setting up Concat9
I1123 11:43:16.664675 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.664679 19447 net.cpp:137] Memory required for data: 147065892
I1123 11:43:16.664680 19447 layer_factory.hpp:77] Creating layer Concat9_Concat9_0_split
I1123 11:43:16.664697 19447 net.cpp:84] Creating Layer Concat9_Concat9_0_split
I1123 11:43:16.664703 19447 net.cpp:406] Concat9_Concat9_0_split <- Concat9
I1123 11:43:16.664713 19447 net.cpp:380] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_0
I1123 11:43:16.664726 19447 net.cpp:380] Concat9_Concat9_0_split -> Concat9_Concat9_0_split_1
I1123 11:43:16.664785 19447 net.cpp:122] Setting up Concat9_Concat9_0_split
I1123 11:43:16.664794 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.664798 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.664800 19447 net.cpp:137] Memory required for data: 156398628
I1123 11:43:16.664804 19447 layer_factory.hpp:77] Creating layer BatchNorm10
I1123 11:43:16.664813 19447 net.cpp:84] Creating Layer BatchNorm10
I1123 11:43:16.664819 19447 net.cpp:406] BatchNorm10 <- Concat9_Concat9_0_split_0
I1123 11:43:16.664829 19447 net.cpp:380] BatchNorm10 -> BatchNorm10
I1123 11:43:16.665067 19447 net.cpp:122] Setting up BatchNorm10
I1123 11:43:16.665076 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.665079 19447 net.cpp:137] Memory required for data: 161064996
I1123 11:43:16.665088 19447 layer_factory.hpp:77] Creating layer Scale10
I1123 11:43:16.665097 19447 net.cpp:84] Creating Layer Scale10
I1123 11:43:16.665104 19447 net.cpp:406] Scale10 <- BatchNorm10
I1123 11:43:16.665113 19447 net.cpp:367] Scale10 -> BatchNorm10 (in-place)
I1123 11:43:16.665172 19447 layer_factory.hpp:77] Creating layer Scale10
I1123 11:43:16.665370 19447 net.cpp:122] Setting up Scale10
I1123 11:43:16.665381 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.665383 19447 net.cpp:137] Memory required for data: 165731364
I1123 11:43:16.665390 19447 layer_factory.hpp:77] Creating layer ReLU10
I1123 11:43:16.665396 19447 net.cpp:84] Creating Layer ReLU10
I1123 11:43:16.665401 19447 net.cpp:406] ReLU10 <- BatchNorm10
I1123 11:43:16.665410 19447 net.cpp:367] ReLU10 -> BatchNorm10 (in-place)
I1123 11:43:16.666069 19447 net.cpp:122] Setting up ReLU10
I1123 11:43:16.666080 19447 net.cpp:129] Top shape: 3 124 56 56 (1166592)
I1123 11:43:16.666082 19447 net.cpp:137] Memory required for data: 170397732
I1123 11:43:16.666100 19447 layer_factory.hpp:77] Creating layer Convolution11
I1123 11:43:16.666116 19447 net.cpp:84] Creating Layer Convolution11
I1123 11:43:16.666124 19447 net.cpp:406] Convolution11 <- BatchNorm10
I1123 11:43:16.666136 19447 net.cpp:380] Convolution11 -> Convolution11
I1123 11:43:16.668438 19447 net.cpp:122] Setting up Convolution11
I1123 11:43:16.668452 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.668454 19447 net.cpp:137] Memory required for data: 170849316
I1123 11:43:16.668460 19447 layer_factory.hpp:77] Creating layer Dropout10
I1123 11:43:16.668468 19447 net.cpp:84] Creating Layer Dropout10
I1123 11:43:16.668475 19447 net.cpp:406] Dropout10 <- Convolution11
I1123 11:43:16.668485 19447 net.cpp:380] Dropout10 -> Dropout10
I1123 11:43:16.668553 19447 net.cpp:122] Setting up Dropout10
I1123 11:43:16.668563 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.668565 19447 net.cpp:137] Memory required for data: 171300900
I1123 11:43:16.668570 19447 layer_factory.hpp:77] Creating layer Concat10
I1123 11:43:16.668579 19447 net.cpp:84] Creating Layer Concat10
I1123 11:43:16.668586 19447 net.cpp:406] Concat10 <- Concat9_Concat9_0_split_1
I1123 11:43:16.668593 19447 net.cpp:406] Concat10 <- Dropout10
I1123 11:43:16.668601 19447 net.cpp:380] Concat10 -> Concat10
I1123 11:43:16.668640 19447 net.cpp:122] Setting up Concat10
I1123 11:43:16.668648 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.668651 19447 net.cpp:137] Memory required for data: 176418852
I1123 11:43:16.668653 19447 layer_factory.hpp:77] Creating layer Concat10_Concat10_0_split
I1123 11:43:16.668660 19447 net.cpp:84] Creating Layer Concat10_Concat10_0_split
I1123 11:43:16.668665 19447 net.cpp:406] Concat10_Concat10_0_split <- Concat10
I1123 11:43:16.668673 19447 net.cpp:380] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_0
I1123 11:43:16.668694 19447 net.cpp:380] Concat10_Concat10_0_split -> Concat10_Concat10_0_split_1
I1123 11:43:16.668750 19447 net.cpp:122] Setting up Concat10_Concat10_0_split
I1123 11:43:16.668757 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.668761 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.668762 19447 net.cpp:137] Memory required for data: 186654756
I1123 11:43:16.668766 19447 layer_factory.hpp:77] Creating layer BatchNorm11
I1123 11:43:16.668774 19447 net.cpp:84] Creating Layer BatchNorm11
I1123 11:43:16.668779 19447 net.cpp:406] BatchNorm11 <- Concat10_Concat10_0_split_0
I1123 11:43:16.668788 19447 net.cpp:380] BatchNorm11 -> BatchNorm11
I1123 11:43:16.669019 19447 net.cpp:122] Setting up BatchNorm11
I1123 11:43:16.669028 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.669030 19447 net.cpp:137] Memory required for data: 191772708
I1123 11:43:16.669049 19447 layer_factory.hpp:77] Creating layer Scale11
I1123 11:43:16.669061 19447 net.cpp:84] Creating Layer Scale11
I1123 11:43:16.669070 19447 net.cpp:406] Scale11 <- BatchNorm11
I1123 11:43:16.669076 19447 net.cpp:367] Scale11 -> BatchNorm11 (in-place)
I1123 11:43:16.669137 19447 layer_factory.hpp:77] Creating layer Scale11
I1123 11:43:16.669296 19447 net.cpp:122] Setting up Scale11
I1123 11:43:16.669304 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.669306 19447 net.cpp:137] Memory required for data: 196890660
I1123 11:43:16.669312 19447 layer_factory.hpp:77] Creating layer ReLU11
I1123 11:43:16.669322 19447 net.cpp:84] Creating Layer ReLU11
I1123 11:43:16.669328 19447 net.cpp:406] ReLU11 <- BatchNorm11
I1123 11:43:16.669335 19447 net.cpp:367] ReLU11 -> BatchNorm11 (in-place)
I1123 11:43:16.669522 19447 net.cpp:122] Setting up ReLU11
I1123 11:43:16.669531 19447 net.cpp:129] Top shape: 3 136 56 56 (1279488)
I1123 11:43:16.669534 19447 net.cpp:137] Memory required for data: 202008612
I1123 11:43:16.669538 19447 layer_factory.hpp:77] Creating layer Convolution12
I1123 11:43:16.669551 19447 net.cpp:84] Creating Layer Convolution12
I1123 11:43:16.669559 19447 net.cpp:406] Convolution12 <- BatchNorm11
I1123 11:43:16.669582 19447 net.cpp:380] Convolution12 -> Convolution12
I1123 11:43:16.671921 19447 net.cpp:122] Setting up Convolution12
I1123 11:43:16.671933 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.671936 19447 net.cpp:137] Memory required for data: 202460196
I1123 11:43:16.671941 19447 layer_factory.hpp:77] Creating layer Dropout11
I1123 11:43:16.671947 19447 net.cpp:84] Creating Layer Dropout11
I1123 11:43:16.671949 19447 net.cpp:406] Dropout11 <- Convolution12
I1123 11:43:16.671955 19447 net.cpp:380] Dropout11 -> Dropout11
I1123 11:43:16.672026 19447 net.cpp:122] Setting up Dropout11
I1123 11:43:16.672035 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.672039 19447 net.cpp:137] Memory required for data: 202911780
I1123 11:43:16.672041 19447 layer_factory.hpp:77] Creating layer Concat11
I1123 11:43:16.672049 19447 net.cpp:84] Creating Layer Concat11
I1123 11:43:16.672053 19447 net.cpp:406] Concat11 <- Concat10_Concat10_0_split_1
I1123 11:43:16.672060 19447 net.cpp:406] Concat11 <- Dropout11
I1123 11:43:16.672066 19447 net.cpp:380] Concat11 -> Concat11
I1123 11:43:16.672109 19447 net.cpp:122] Setting up Concat11
I1123 11:43:16.672117 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.672119 19447 net.cpp:137] Memory required for data: 208481316
I1123 11:43:16.672122 19447 layer_factory.hpp:77] Creating layer Concat11_Concat11_0_split
I1123 11:43:16.672128 19447 net.cpp:84] Creating Layer Concat11_Concat11_0_split
I1123 11:43:16.672132 19447 net.cpp:406] Concat11_Concat11_0_split <- Concat11
I1123 11:43:16.672142 19447 net.cpp:380] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_0
I1123 11:43:16.672165 19447 net.cpp:380] Concat11_Concat11_0_split -> Concat11_Concat11_0_split_1
I1123 11:43:16.672224 19447 net.cpp:122] Setting up Concat11_Concat11_0_split
I1123 11:43:16.672233 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.672236 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.672240 19447 net.cpp:137] Memory required for data: 219620388
I1123 11:43:16.672243 19447 layer_factory.hpp:77] Creating layer BatchNorm12
I1123 11:43:16.672250 19447 net.cpp:84] Creating Layer BatchNorm12
I1123 11:43:16.672257 19447 net.cpp:406] BatchNorm12 <- Concat11_Concat11_0_split_0
I1123 11:43:16.672266 19447 net.cpp:380] BatchNorm12 -> BatchNorm12
I1123 11:43:16.672500 19447 net.cpp:122] Setting up BatchNorm12
I1123 11:43:16.672510 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.672513 19447 net.cpp:137] Memory required for data: 225189924
I1123 11:43:16.672523 19447 layer_factory.hpp:77] Creating layer Scale12
I1123 11:43:16.672531 19447 net.cpp:84] Creating Layer Scale12
I1123 11:43:16.672538 19447 net.cpp:406] Scale12 <- BatchNorm12
I1123 11:43:16.672545 19447 net.cpp:367] Scale12 -> BatchNorm12 (in-place)
I1123 11:43:16.672605 19447 layer_factory.hpp:77] Creating layer Scale12
I1123 11:43:16.672775 19447 net.cpp:122] Setting up Scale12
I1123 11:43:16.672797 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.672801 19447 net.cpp:137] Memory required for data: 230759460
I1123 11:43:16.672807 19447 layer_factory.hpp:77] Creating layer ReLU12
I1123 11:43:16.672814 19447 net.cpp:84] Creating Layer ReLU12
I1123 11:43:16.672818 19447 net.cpp:406] ReLU12 <- BatchNorm12
I1123 11:43:16.672825 19447 net.cpp:367] ReLU12 -> BatchNorm12 (in-place)
I1123 11:43:16.673501 19447 net.cpp:122] Setting up ReLU12
I1123 11:43:16.673512 19447 net.cpp:129] Top shape: 3 148 56 56 (1392384)
I1123 11:43:16.673516 19447 net.cpp:137] Memory required for data: 236328996
I1123 11:43:16.673517 19447 layer_factory.hpp:77] Creating layer Convolution13
I1123 11:43:16.673527 19447 net.cpp:84] Creating Layer Convolution13
I1123 11:43:16.673529 19447 net.cpp:406] Convolution13 <- BatchNorm12
I1123 11:43:16.673535 19447 net.cpp:380] Convolution13 -> Convolution13
I1123 11:43:16.675385 19447 net.cpp:122] Setting up Convolution13
I1123 11:43:16.675397 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.675400 19447 net.cpp:137] Memory required for data: 236780580
I1123 11:43:16.675413 19447 layer_factory.hpp:77] Creating layer Dropout12
I1123 11:43:16.675420 19447 net.cpp:84] Creating Layer Dropout12
I1123 11:43:16.675422 19447 net.cpp:406] Dropout12 <- Convolution13
I1123 11:43:16.675429 19447 net.cpp:380] Dropout12 -> Dropout12
I1123 11:43:16.675475 19447 net.cpp:122] Setting up Dropout12
I1123 11:43:16.675483 19447 net.cpp:129] Top shape: 3 12 56 56 (112896)
I1123 11:43:16.675485 19447 net.cpp:137] Memory required for data: 237232164
I1123 11:43:16.675487 19447 layer_factory.hpp:77] Creating layer Concat12
I1123 11:43:16.675493 19447 net.cpp:84] Creating Layer Concat12
I1123 11:43:16.675496 19447 net.cpp:406] Concat12 <- Concat11_Concat11_0_split_1
I1123 11:43:16.675499 19447 net.cpp:406] Concat12 <- Dropout12
I1123 11:43:16.675503 19447 net.cpp:380] Concat12 -> Concat12
I1123 11:43:16.675529 19447 net.cpp:122] Setting up Concat12
I1123 11:43:16.675535 19447 net.cpp:129] Top shape: 3 160 56 56 (1505280)
I1123 11:43:16.675537 19447 net.cpp:137] Memory required for data: 243253284
I1123 11:43:16.675539 19447 layer_factory.hpp:77] Creating layer BatchNorm13
I1123 11:43:16.675544 19447 net.cpp:84] Creating Layer BatchNorm13
I1123 11:43:16.675546 19447 net.cpp:406] BatchNorm13 <- Concat12
I1123 11:43:16.675551 19447 net.cpp:380] BatchNorm13 -> BatchNorm13
I1123 11:43:16.675768 19447 net.cpp:122] Setting up BatchNorm13
I1123 11:43:16.675776 19447 net.cpp:129] Top shape: 3 160 56 56 (1505280)
I1123 11:43:16.675778 19447 net.cpp:137] Memory required for data: 249274404
I1123 11:43:16.675783 19447 layer_factory.hpp:77] Creating layer Scale13
I1123 11:43:16.675788 19447 net.cpp:84] Creating Layer Scale13
I1123 11:43:16.675791 19447 net.cpp:406] Scale13 <- BatchNorm13
I1123 11:43:16.675794 19447 net.cpp:367] Scale13 -> BatchNorm13 (in-place)
I1123 11:43:16.675837 19447 layer_factory.hpp:77] Creating layer Scale13
I1123 11:43:16.676409 19447 net.cpp:122] Setting up Scale13
I1123 11:43:16.676420 19447 net.cpp:129] Top shape: 3 160 56 56 (1505280)
I1123 11:43:16.676424 19447 net.cpp:137] Memory required for data: 255295524
I1123 11:43:16.676429 19447 layer_factory.hpp:77] Creating layer ReLU13
I1123 11:43:16.676434 19447 net.cpp:84] Creating Layer ReLU13
I1123 11:43:16.676436 19447 net.cpp:406] ReLU13 <- BatchNorm13
I1123 11:43:16.676440 19447 net.cpp:367] ReLU13 -> BatchNorm13 (in-place)
I1123 11:43:16.677091 19447 net.cpp:122] Setting up ReLU13
I1123 11:43:16.677103 19447 net.cpp:129] Top shape: 3 160 56 56 (1505280)
I1123 11:43:16.677105 19447 net.cpp:137] Memory required for data: 261316644
I1123 11:43:16.677109 19447 layer_factory.hpp:77] Creating layer Pooling3
I1123 11:43:16.677114 19447 net.cpp:84] Creating Layer Pooling3
I1123 11:43:16.677117 19447 net.cpp:406] Pooling3 <- BatchNorm13
I1123 11:43:16.677122 19447 net.cpp:380] Pooling3 -> Pooling3
I1123 11:43:16.677796 19447 net.cpp:122] Setting up Pooling3
I1123 11:43:16.677808 19447 net.cpp:129] Top shape: 3 160 54 54 (1399680)
I1123 11:43:16.677810 19447 net.cpp:137] Memory required for data: 266915364
I1123 11:43:16.677812 19447 layer_factory.hpp:77] Creating layer InnerProduct1
I1123 11:43:16.677819 19447 net.cpp:84] Creating Layer InnerProduct1
I1123 11:43:16.677822 19447 net.cpp:406] InnerProduct1 <- Pooling3
I1123 11:43:16.677826 19447 net.cpp:380] InnerProduct1 -> InnerProduct1
I1123 11:43:16.723093 19447 net.cpp:122] Setting up InnerProduct1
I1123 11:43:16.723121 19447 net.cpp:129] Top shape: 3 16 (48)
I1123 11:43:16.723124 19447 net.cpp:137] Memory required for data: 266915556
I1123 11:43:16.723132 19447 layer_factory.hpp:77] Creating layer InnerProduct1_InnerProduct1_0_split
I1123 11:43:16.723142 19447 net.cpp:84] Creating Layer InnerProduct1_InnerProduct1_0_split
I1123 11:43:16.723146 19447 net.cpp:406] InnerProduct1_InnerProduct1_0_split <- InnerProduct1
I1123 11:43:16.723155 19447 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_0
I1123 11:43:16.723165 19447 net.cpp:380] InnerProduct1_InnerProduct1_0_split -> InnerProduct1_InnerProduct1_0_split_1
I1123 11:43:16.723240 19447 net.cpp:122] Setting up InnerProduct1_InnerProduct1_0_split
I1123 11:43:16.723245 19447 net.cpp:129] Top shape: 3 16 (48)
I1123 11:43:16.723248 19447 net.cpp:129] Top shape: 3 16 (48)
I1123 11:43:16.723250 19447 net.cpp:137] Memory required for data: 266915940
I1123 11:43:16.723253 19447 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1123 11:43:16.723258 19447 net.cpp:84] Creating Layer SoftmaxWithLoss1
I1123 11:43:16.723260 19447 net.cpp:406] SoftmaxWithLoss1 <- InnerProduct1_InnerProduct1_0_split_0
I1123 11:43:16.723263 19447 net.cpp:406] SoftmaxWithLoss1 <- label_Data1_1_split_0
I1123 11:43:16.723268 19447 net.cpp:380] SoftmaxWithLoss1 -> SoftmaxWithLoss1
I1123 11:43:16.723276 19447 layer_factory.hpp:77] Creating layer SoftmaxWithLoss1
I1123 11:43:16.723603 19447 net.cpp:122] Setting up SoftmaxWithLoss1
I1123 11:43:16.723610 19447 net.cpp:129] Top shape: (1)
I1123 11:43:16.723613 19447 net.cpp:132]     with loss weight 1
I1123 11:43:16.723623 19447 net.cpp:137] Memory required for data: 266915944
I1123 11:43:16.723624 19447 layer_factory.hpp:77] Creating layer Accuracy1
I1123 11:43:16.723637 19447 net.cpp:84] Creating Layer Accuracy1
I1123 11:43:16.723640 19447 net.cpp:406] Accuracy1 <- InnerProduct1_InnerProduct1_0_split_1
I1123 11:43:16.723644 19447 net.cpp:406] Accuracy1 <- label_Data1_1_split_1
I1123 11:43:16.723649 19447 net.cpp:380] Accuracy1 -> Accuracy1
I1123 11:43:16.723655 19447 net.cpp:122] Setting up Accuracy1
I1123 11:43:16.723659 19447 net.cpp:129] Top shape: (1)
I1123 11:43:16.723660 19447 net.cpp:137] Memory required for data: 266915948
I1123 11:43:16.723664 19447 net.cpp:200] Accuracy1 does not need backward computation.
I1123 11:43:16.723666 19447 net.cpp:198] SoftmaxWithLoss1 needs backward computation.
I1123 11:43:16.723670 19447 net.cpp:198] InnerProduct1_InnerProduct1_0_split needs backward computation.
I1123 11:43:16.723671 19447 net.cpp:198] InnerProduct1 needs backward computation.
I1123 11:43:16.723675 19447 net.cpp:198] Pooling3 needs backward computation.
I1123 11:43:16.723676 19447 net.cpp:198] ReLU13 needs backward computation.
I1123 11:43:16.723678 19447 net.cpp:198] Scale13 needs backward computation.
I1123 11:43:16.723680 19447 net.cpp:198] BatchNorm13 needs backward computation.
I1123 11:43:16.723683 19447 net.cpp:198] Concat12 needs backward computation.
I1123 11:43:16.723687 19447 net.cpp:198] Dropout12 needs backward computation.
I1123 11:43:16.723690 19447 net.cpp:198] Convolution13 needs backward computation.
I1123 11:43:16.723692 19447 net.cpp:198] ReLU12 needs backward computation.
I1123 11:43:16.723695 19447 net.cpp:198] Scale12 needs backward computation.
I1123 11:43:16.723697 19447 net.cpp:198] BatchNorm12 needs backward computation.
I1123 11:43:16.723701 19447 net.cpp:198] Concat11_Concat11_0_split needs backward computation.
I1123 11:43:16.723703 19447 net.cpp:198] Concat11 needs backward computation.
I1123 11:43:16.723706 19447 net.cpp:198] Dropout11 needs backward computation.
I1123 11:43:16.723708 19447 net.cpp:198] Convolution12 needs backward computation.
I1123 11:43:16.723711 19447 net.cpp:198] ReLU11 needs backward computation.
I1123 11:43:16.723713 19447 net.cpp:198] Scale11 needs backward computation.
I1123 11:43:16.723716 19447 net.cpp:198] BatchNorm11 needs backward computation.
I1123 11:43:16.723718 19447 net.cpp:198] Concat10_Concat10_0_split needs backward computation.
I1123 11:43:16.723721 19447 net.cpp:198] Concat10 needs backward computation.
I1123 11:43:16.723724 19447 net.cpp:198] Dropout10 needs backward computation.
I1123 11:43:16.723729 19447 net.cpp:198] Convolution11 needs backward computation.
I1123 11:43:16.723731 19447 net.cpp:198] ReLU10 needs backward computation.
I1123 11:43:16.723734 19447 net.cpp:198] Scale10 needs backward computation.
I1123 11:43:16.723737 19447 net.cpp:198] BatchNorm10 needs backward computation.
I1123 11:43:16.723738 19447 net.cpp:198] Concat9_Concat9_0_split needs backward computation.
I1123 11:43:16.723742 19447 net.cpp:198] Concat9 needs backward computation.
I1123 11:43:16.723752 19447 net.cpp:198] Dropout9 needs backward computation.
I1123 11:43:16.723754 19447 net.cpp:198] Convolution10 needs backward computation.
I1123 11:43:16.723757 19447 net.cpp:198] ReLU9 needs backward computation.
I1123 11:43:16.723759 19447 net.cpp:198] Scale9 needs backward computation.
I1123 11:43:16.723762 19447 net.cpp:198] BatchNorm9 needs backward computation.
I1123 11:43:16.723764 19447 net.cpp:198] Concat8_Concat8_0_split needs backward computation.
I1123 11:43:16.723767 19447 net.cpp:198] Concat8 needs backward computation.
I1123 11:43:16.723772 19447 net.cpp:198] Dropout8 needs backward computation.
I1123 11:43:16.723774 19447 net.cpp:198] Convolution9 needs backward computation.
I1123 11:43:16.723776 19447 net.cpp:198] ReLU8 needs backward computation.
I1123 11:43:16.723778 19447 net.cpp:198] Scale8 needs backward computation.
I1123 11:43:16.723781 19447 net.cpp:198] BatchNorm8 needs backward computation.
I1123 11:43:16.723783 19447 net.cpp:198] Concat7_Concat7_0_split needs backward computation.
I1123 11:43:16.723786 19447 net.cpp:198] Concat7 needs backward computation.
I1123 11:43:16.723789 19447 net.cpp:198] Dropout7 needs backward computation.
I1123 11:43:16.723791 19447 net.cpp:198] Convolution8 needs backward computation.
I1123 11:43:16.723794 19447 net.cpp:198] ReLU7 needs backward computation.
I1123 11:43:16.723796 19447 net.cpp:198] Scale7 needs backward computation.
I1123 11:43:16.723798 19447 net.cpp:198] BatchNorm7 needs backward computation.
I1123 11:43:16.723801 19447 net.cpp:198] Concat6_Concat6_0_split needs backward computation.
I1123 11:43:16.723804 19447 net.cpp:198] Concat6 needs backward computation.
I1123 11:43:16.723806 19447 net.cpp:198] Dropout6 needs backward computation.
I1123 11:43:16.723809 19447 net.cpp:198] Convolution7 needs backward computation.
I1123 11:43:16.723812 19447 net.cpp:198] ReLU6 needs backward computation.
I1123 11:43:16.723814 19447 net.cpp:198] Scale6 needs backward computation.
I1123 11:43:16.723816 19447 net.cpp:198] BatchNorm6 needs backward computation.
I1123 11:43:16.723819 19447 net.cpp:198] Concat5_Concat5_0_split needs backward computation.
I1123 11:43:16.723821 19447 net.cpp:198] Concat5 needs backward computation.
I1123 11:43:16.723824 19447 net.cpp:198] Dropout5 needs backward computation.
I1123 11:43:16.723829 19447 net.cpp:198] Convolution6 needs backward computation.
I1123 11:43:16.723830 19447 net.cpp:198] ReLU5 needs backward computation.
I1123 11:43:16.723832 19447 net.cpp:198] Scale5 needs backward computation.
I1123 11:43:16.723834 19447 net.cpp:198] BatchNorm5 needs backward computation.
I1123 11:43:16.723837 19447 net.cpp:198] Concat4_Concat4_0_split needs backward computation.
I1123 11:43:16.723840 19447 net.cpp:198] Concat4 needs backward computation.
I1123 11:43:16.723842 19447 net.cpp:198] Dropout4 needs backward computation.
I1123 11:43:16.723845 19447 net.cpp:198] Convolution5 needs backward computation.
I1123 11:43:16.723847 19447 net.cpp:198] ReLU4 needs backward computation.
I1123 11:43:16.723850 19447 net.cpp:198] Scale4 needs backward computation.
I1123 11:43:16.723852 19447 net.cpp:198] BatchNorm4 needs backward computation.
I1123 11:43:16.723855 19447 net.cpp:198] Concat3_Concat3_0_split needs backward computation.
I1123 11:43:16.723857 19447 net.cpp:198] Concat3 needs backward computation.
I1123 11:43:16.723860 19447 net.cpp:198] Dropout3 needs backward computation.
I1123 11:43:16.723862 19447 net.cpp:198] Convolution4 needs backward computation.
I1123 11:43:16.723865 19447 net.cpp:198] ReLU3 needs backward computation.
I1123 11:43:16.723867 19447 net.cpp:198] Scale3 needs backward computation.
I1123 11:43:16.723870 19447 net.cpp:198] BatchNorm3 needs backward computation.
I1123 11:43:16.723872 19447 net.cpp:198] Concat2_Concat2_0_split needs backward computation.
I1123 11:43:16.723875 19447 net.cpp:198] Concat2 needs backward computation.
I1123 11:43:16.723878 19447 net.cpp:198] Dropout2 needs backward computation.
I1123 11:43:16.723881 19447 net.cpp:198] Convolution3 needs backward computation.
I1123 11:43:16.723886 19447 net.cpp:198] ReLU2 needs backward computation.
I1123 11:43:16.723889 19447 net.cpp:198] Scale2 needs backward computation.
I1123 11:43:16.723891 19447 net.cpp:198] BatchNorm2 needs backward computation.
I1123 11:43:16.723894 19447 net.cpp:198] Concat1_Concat1_0_split needs backward computation.
I1123 11:43:16.723896 19447 net.cpp:198] Concat1 needs backward computation.
I1123 11:43:16.723901 19447 net.cpp:198] Dropout1 needs backward computation.
I1123 11:43:16.723903 19447 net.cpp:198] Convolution2 needs backward computation.
I1123 11:43:16.723906 19447 net.cpp:198] ReLU1 needs backward computation.
I1123 11:43:16.723908 19447 net.cpp:198] Scale1 needs backward computation.
I1123 11:43:16.723911 19447 net.cpp:198] BatchNorm1 needs backward computation.
I1123 11:43:16.723913 19447 net.cpp:198] Convolution1_Convolution1_0_split needs backward computation.
I1123 11:43:16.723917 19447 net.cpp:198] Convolution1 needs backward computation.
I1123 11:43:16.723919 19447 net.cpp:198] Convolution1_3x3 needs backward computation.
I1123 11:43:16.723922 19447 net.cpp:200] label_Data1_1_split does not need backward computation.
I1123 11:43:16.723925 19447 net.cpp:200] Data1 does not need backward computation.
I1123 11:43:16.723927 19447 net.cpp:242] This network produces output Accuracy1
I1123 11:43:16.723929 19447 net.cpp:242] This network produces output SoftmaxWithLoss1
I1123 11:43:16.723979 19447 net.cpp:255] Network initialization done.
I1123 11:43:16.724156 19447 solver.cpp:56] Solver scaffolding done.
I1123 11:43:16.743661 19447 caffe.cpp:248] Starting Optimization
I1123 11:43:16.769739 19447 solver.cpp:272] Solving 
I1123 11:43:16.769744 19447 solver.cpp:273] Learning Rate Policy: multistep
I1123 11:43:16.773532 19447 solver.cpp:330] Iteration 0, Testing net (#0)
I1123 11:43:19.290951 19447 blocking_queue.cpp:49] Waiting for data
I1123 11:43:37.752995 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 11:43:37.862200 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.0659239
I1123 11:43:37.869539 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 2.8776 (* 1 = 2.8776 loss)
I1123 11:43:38.065387 19447 solver.cpp:218] Iteration 0 (1.42904e-07 iter/s, 21.2945s/3629 iters), loss = 2.69785
I1123 11:43:38.065434 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 2.69785 (* 1 = 2.69785 loss)
I1123 11:43:38.067451 19447 sgd_solver.cpp:105] Iteration 0, lr = 0.0005
I1123 11:48:26.710672 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 11:48:27.004740 19447 solver.cpp:330] Iteration 3629, Testing net (#0)
I1123 11:48:48.236150 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 11:48:48.349761 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.766017
I1123 11:48:48.349809 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.894788 (* 1 = 0.894788 loss)
I1123 11:48:48.442708 19447 solver.cpp:218] Iteration 3629 (11.6925 iter/s, 310.371s/3629 iters), loss = 0.429593
I1123 11:48:48.442759 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.429597 (* 1 = 0.429597 loss)
I1123 11:48:48.442786 19447 sgd_solver.cpp:105] Iteration 3629, lr = 0.0005
I1123 11:53:38.844120 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 11:53:39.160965 19447 solver.cpp:330] Iteration 7258, Testing net (#0)
I1123 11:54:00.415205 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 11:54:00.527066 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.805942
I1123 11:54:00.537792 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.788992 (* 1 = 0.788992 loss)
I1123 11:54:00.654438 19447 solver.cpp:218] Iteration 7258 (11.6237 iter/s, 312.206s/3629 iters), loss = 0.159186
I1123 11:54:00.654508 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.159191 (* 1 = 0.159191 loss)
I1123 11:54:00.669494 19447 sgd_solver.cpp:105] Iteration 7258, lr = 0.0005
I1123 11:57:39.696794 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_10000.caffemodel
I1123 11:57:40.367031 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_10000.solverstate
I1123 11:58:51.512435 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 11:58:51.828749 19447 solver.cpp:330] Iteration 10887, Testing net (#0)
I1123 11:59:13.232961 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 11:59:13.400521 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.822656
I1123 11:59:13.439010 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.762106 (* 1 = 0.762106 loss)
I1123 11:59:13.533450 19447 solver.cpp:218] Iteration 10887 (11.599 iter/s, 312.872s/3629 iters), loss = 0.392403
I1123 11:59:13.533493 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.392409 (* 1 = 0.392409 loss)
I1123 11:59:13.581804 19447 sgd_solver.cpp:105] Iteration 10887, lr = 0.0005
I1123 12:04:03.034984 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:04:03.297277 19447 solver.cpp:330] Iteration 14516, Testing net (#0)
I1123 12:04:24.870815 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:04:24.957672 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.818941
I1123 12:04:24.996361 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.93252 (* 1 = 0.93252 loss)
I1123 12:04:25.101982 19447 solver.cpp:218] Iteration 14516 (11.6478 iter/s, 311.561s/3629 iters), loss = 0.0226882
I1123 12:04:25.102032 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0226951 (* 1 = 0.0226951 loss)
I1123 12:04:25.116926 19447 sgd_solver.cpp:105] Iteration 14516, lr = 0.0005
I1123 12:09:14.747113 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:09:15.065274 19447 solver.cpp:330] Iteration 18145, Testing net (#0)
I1123 12:09:24.508800 19447 blocking_queue.cpp:49] Waiting for data
I1123 12:09:36.453383 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:09:36.569738 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.832869
I1123 12:09:36.653252 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.894266 (* 1 = 0.894266 loss)
I1123 12:09:36.758889 19447 solver.cpp:218] Iteration 18145 (11.6445 iter/s, 311.65s/3629 iters), loss = 0.0164329
I1123 12:09:36.758939 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0164409 (* 1 = 0.0164409 loss)
I1123 12:09:36.762719 19447 sgd_solver.cpp:105] Iteration 18145, lr = 0.0005
I1123 12:12:57.326643 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_20000.caffemodel
I1123 12:12:57.557497 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_20000.solverstate
I1123 12:15:17.913399 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:15:18.224903 19447 solver.cpp:330] Iteration 21774, Testing net (#0)
I1123 12:15:39.225411 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:15:39.335463 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.841226
I1123 12:15:39.335506 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.908937 (* 1 = 0.908937 loss)
I1123 12:15:39.411252 19447 solver.cpp:218] Iteration 21774 (10.007 iter/s, 362.645s/3629 iters), loss = 0.0342514
I1123 12:15:39.411283 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0342583 (* 1 = 0.0342583 loss)
I1123 12:15:39.411288 19447 sgd_solver.cpp:105] Iteration 21774, lr = 0.0005
I1123 12:20:25.745446 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:20:26.056402 19447 solver.cpp:330] Iteration 25403, Testing net (#0)
I1123 12:20:46.835341 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:20:47.061153 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.829155
I1123 12:20:47.061182 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.872096 (* 1 = 0.872096 loss)
I1123 12:20:47.137850 19447 solver.cpp:218] Iteration 25403 (11.7932 iter/s, 307.721s/3629 iters), loss = 0.0198145
I1123 12:20:47.137892 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.019821 (* 1 = 0.019821 loss)
I1123 12:20:47.137898 19447 sgd_solver.cpp:105] Iteration 25403, lr = 0.0005
I1123 12:25:33.454895 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:25:33.766013 19447 solver.cpp:330] Iteration 29032, Testing net (#0)
I1123 12:25:54.545480 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:25:54.771838 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.850511
I1123 12:25:54.771864 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.817862 (* 1 = 0.817862 loss)
I1123 12:25:54.849154 19447 solver.cpp:218] Iteration 29032 (11.7937 iter/s, 307.706s/3629 iters), loss = 0.00741049
I1123 12:25:54.849185 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00741685 (* 1 = 0.00741685 loss)
I1123 12:25:54.849191 19447 sgd_solver.cpp:105] Iteration 29032, lr = 0.0005
I1123 12:27:11.231971 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_30000.caffemodel
I1123 12:27:11.313266 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_30000.solverstate
I1123 12:30:41.267649 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:30:41.578502 19447 solver.cpp:330] Iteration 32661, Testing net (#0)
I1123 12:31:02.405339 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:31:02.576277 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.831941
I1123 12:31:02.576305 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.93159 (* 1 = 0.93159 loss)
I1123 12:31:02.653297 19447 solver.cpp:218] Iteration 32661 (11.7902 iter/s, 307.798s/3629 iters), loss = 0.0121702
I1123 12:31:02.653329 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0121767 (* 1 = 0.0121767 loss)
I1123 12:31:02.653336 19447 sgd_solver.cpp:105] Iteration 32661, lr = 0.0005
I1123 12:35:49.023779 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:35:49.336887 19447 solver.cpp:330] Iteration 36290, Testing net (#0)
I1123 12:36:10.216704 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:36:10.445359 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.827298
I1123 12:36:10.445394 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.935815 (* 1 = 0.935815 loss)
I1123 12:36:10.523314 19447 solver.cpp:218] Iteration 36290 (11.7877 iter/s, 307.864s/3629 iters), loss = 0.0121157
I1123 12:36:10.523353 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0121225 (* 1 = 0.0121225 loss)
I1123 12:36:10.523360 19447 sgd_solver.cpp:105] Iteration 36290, lr = 0.0005
I1123 12:41:00.018394 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:41:00.333947 19447 solver.cpp:330] Iteration 39919, Testing net (#0)
I1123 12:41:21.314836 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:41:21.487090 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.864438
I1123 12:41:21.487125 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.826229 (* 1 = 0.826229 loss)
I1123 12:41:21.564695 19447 solver.cpp:218] Iteration 39919 (11.6675 iter/s, 311.035s/3629 iters), loss = 0.0056956
I1123 12:41:21.564734 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00570292 (* 1 = 0.00570292 loss)
I1123 12:41:21.564740 19447 sgd_solver.cpp:105] Iteration 39919, lr = 0.0005
I1123 12:41:27.958657 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_40000.caffemodel
I1123 12:41:28.071668 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_40000.solverstate
I1123 12:46:11.601711 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:46:11.916651 19447 solver.cpp:330] Iteration 43548, Testing net (#0)
I1123 12:46:32.842448 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:46:33.075239 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.856082
I1123 12:46:33.075273 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.956979 (* 1 = 0.956979 loss)
I1123 12:46:33.152755 19447 solver.cpp:218] Iteration 43548 (11.647 iter/s, 311.582s/3629 iters), loss = 0.000841542
I1123 12:46:33.152796 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000848692 (* 1 = 0.000848692 loss)
I1123 12:46:33.152804 19447 sgd_solver.cpp:105] Iteration 43548, lr = 0.0005
I1123 12:51:23.070487 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:51:23.386512 19447 solver.cpp:330] Iteration 47177, Testing net (#0)
I1123 12:51:44.306262 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:51:44.537760 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.851439
I1123 12:51:44.537792 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.896228 (* 1 = 0.896228 loss)
I1123 12:51:44.615547 19447 solver.cpp:218] Iteration 47177 (11.6517 iter/s, 311.457s/3629 iters), loss = 0.0317229
I1123 12:51:44.615592 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0317305 (* 1 = 0.0317305 loss)
I1123 12:51:44.615600 19447 sgd_solver.cpp:105] Iteration 47177, lr = 0.0005
I1123 12:55:30.345199 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_50000.caffemodel
I1123 12:55:30.425408 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_50000.solverstate
I1123 12:56:34.601683 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:56:34.916252 19447 solver.cpp:330] Iteration 50806, Testing net (#0)
I1123 12:56:55.826958 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 12:56:56.055901 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.856081
I1123 12:56:56.055933 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.919002 (* 1 = 0.919002 loss)
I1123 12:56:56.134068 19447 solver.cpp:218] Iteration 50806 (11.6496 iter/s, 311.513s/3629 iters), loss = 0.00918151
I1123 12:56:56.134109 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00918978 (* 1 = 0.00918978 loss)
I1123 12:56:56.134114 19447 sgd_solver.cpp:105] Iteration 50806, lr = 0.0005
I1123 13:01:46.043478 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:01:46.358732 19447 solver.cpp:330] Iteration 54435, Testing net (#0)
I1123 13:02:07.237231 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:02:07.468261 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.852368
I1123 13:02:07.468295 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.872874 (* 1 = 0.872874 loss)
I1123 13:02:07.546180 19447 solver.cpp:218] Iteration 54435 (11.6536 iter/s, 311.407s/3629 iters), loss = 0.00288831
I1123 13:02:07.546221 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00289708 (* 1 = 0.00289708 loss)
I1123 13:02:07.546228 19447 sgd_solver.cpp:105] Iteration 54435, lr = 0.0005
I1123 13:06:57.422523 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:06:57.738617 19447 solver.cpp:330] Iteration 58064, Testing net (#0)
I1123 13:07:18.704108 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:07:18.874431 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.861653
I1123 13:07:18.874465 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.870632 (* 1 = 0.870632 loss)
I1123 13:07:18.952471 19447 solver.cpp:218] Iteration 58064 (11.6538 iter/s, 311.401s/3629 iters), loss = 0.00150946
I1123 13:07:18.952512 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00151838 (* 1 = 0.00151838 loss)
I1123 13:07:18.952517 19447 sgd_solver.cpp:105] Iteration 58064, lr = 0.0005
I1123 13:09:53.708186 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_60000.caffemodel
I1123 13:09:53.788897 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_60000.solverstate
I1123 13:12:08.836781 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:12:09.151476 19447 solver.cpp:330] Iteration 61693, Testing net (#0)
I1123 13:12:30.214712 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:12:30.323891 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.870938
I1123 13:12:30.323923 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.733911 (* 1 = 0.733911 loss)
I1123 13:12:30.402192 19447 solver.cpp:218] Iteration 61693 (11.6522 iter/s, 311.444s/3629 iters), loss = 0.0114503
I1123 13:12:30.402230 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0114596 (* 1 = 0.0114596 loss)
I1123 13:12:30.402236 19447 sgd_solver.cpp:105] Iteration 61693, lr = 0.0005
I1123 13:17:20.258137 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:17:20.571104 19447 solver.cpp:330] Iteration 65322, Testing net (#0)
I1123 13:17:41.486670 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:17:41.718178 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.86351
I1123 13:17:41.718209 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.917709 (* 1 = 0.917709 loss)
I1123 13:17:41.795863 19447 solver.cpp:218] Iteration 65322 (11.6543 iter/s, 311.388s/3629 iters), loss = 0.0130319
I1123 13:17:41.795903 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0130408 (* 1 = 0.0130408 loss)
I1123 13:17:41.795909 19447 sgd_solver.cpp:105] Iteration 65322, lr = 0.0005
I1123 13:22:31.622597 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:22:31.936911 19447 solver.cpp:330] Iteration 68951, Testing net (#0)
I1123 13:22:52.975137 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:22:53.086583 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.877437
I1123 13:22:53.086616 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.792165 (* 1 = 0.792165 loss)
I1123 13:22:53.164960 19447 solver.cpp:218] Iteration 68951 (11.6552 iter/s, 311.364s/3629 iters), loss = 0.00225538
I1123 13:22:53.164997 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00226457 (* 1 = 0.00226457 loss)
I1123 13:22:53.165004 19447 sgd_solver.cpp:105] Iteration 68951, lr = 0.0005
I1123 13:24:17.022369 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_70000.caffemodel
I1123 13:24:17.102802 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_70000.solverstate
I1123 13:27:43.221468 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:27:43.536790 19447 solver.cpp:330] Iteration 72580, Testing net (#0)
I1123 13:28:04.506762 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:28:04.740375 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.874652
I1123 13:28:04.740412 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.894678 (* 1 = 0.894678 loss)
I1123 13:28:04.818055 19447 solver.cpp:218] Iteration 72580 (11.6446 iter/s, 311.648s/3629 iters), loss = 0.0336349
I1123 13:28:04.818094 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0336436 (* 1 = 0.0336436 loss)
I1123 13:28:04.818100 19447 sgd_solver.cpp:105] Iteration 72580, lr = 0.0005
I1123 13:32:54.613438 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:32:54.928324 19447 solver.cpp:330] Iteration 76209, Testing net (#0)
I1123 13:33:07.296409 19447 blocking_queue.cpp:49] Waiting for data
I1123 13:33:15.966392 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:33:16.077924 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.870009
I1123 13:33:16.077962 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.898733 (* 1 = 0.898733 loss)
I1123 13:33:16.154970 19447 solver.cpp:218] Iteration 76209 (11.6564 iter/s, 311.331s/3629 iters), loss = 0.00440065
I1123 13:33:16.155009 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00440928 (* 1 = 0.00440928 loss)
I1123 13:33:16.155016 19447 sgd_solver.cpp:105] Iteration 76209, lr = 0.0005
I1123 13:38:06.031750 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:38:06.346503 19447 solver.cpp:330] Iteration 79838, Testing net (#0)
I1123 13:38:27.285893 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:38:27.514710 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.858867
I1123 13:38:27.514746 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.970596 (* 1 = 0.970596 loss)
I1123 13:38:27.592581 19447 solver.cpp:218] Iteration 79838 (11.6526 iter/s, 311.432s/3629 iters), loss = 0.00027089
I1123 13:38:27.592619 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000279612 (* 1 = 0.000279612 loss)
I1123 13:38:27.592625 19447 sgd_solver.cpp:105] Iteration 79838, lr = 0.0005
I1123 13:38:40.467301 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_80000.caffemodel
I1123 13:38:40.696831 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_80000.solverstate
I1123 13:43:17.599871 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:43:17.914796 19447 solver.cpp:330] Iteration 83467, Testing net (#0)
I1123 13:43:38.978405 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:43:39.148460 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.880223
I1123 13:43:39.148519 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.760178 (* 1 = 0.760178 loss)
I1123 13:43:39.226003 19447 solver.cpp:218] Iteration 83467 (11.645 iter/s, 311.636s/3629 iters), loss = 0.0584279
I1123 13:43:39.226048 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0584367 (* 1 = 0.0584367 loss)
I1123 13:43:39.226058 19447 sgd_solver.cpp:105] Iteration 83467, lr = 0.0005
I1123 13:48:28.816802 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:48:29.133131 19447 solver.cpp:330] Iteration 87096, Testing net (#0)
I1123 13:48:50.415333 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:48:50.517416 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.880223
I1123 13:48:50.518551 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.81851 (* 1 = 0.81851 loss)
I1123 13:48:50.596835 19447 solver.cpp:218] Iteration 87096 (11.6549 iter/s, 311.372s/3629 iters), loss = 0.000970676
I1123 13:48:50.596992 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000979542 (* 1 = 0.000979542 loss)
I1123 13:48:50.597007 19447 sgd_solver.cpp:105] Iteration 87096, lr = 0.0005
I1123 13:52:44.791046 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_90000.caffemodel
I1123 13:52:45.541460 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_90000.solverstate
I1123 13:53:43.457602 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:53:43.784950 19447 solver.cpp:330] Iteration 90725, Testing net (#0)
I1123 13:54:05.203478 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:54:05.312289 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.886722
I1123 13:54:05.328199 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.677073 (* 1 = 0.677073 loss)
I1123 13:54:05.406880 19447 solver.cpp:218] Iteration 90725 (11.5277 iter/s, 314.808s/3629 iters), loss = 0.00478626
I1123 13:54:05.406922 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00479561 (* 1 = 0.00479561 loss)
I1123 13:54:05.415433 19447 sgd_solver.cpp:105] Iteration 90725, lr = 0.0005
I1123 13:58:54.962368 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:58:55.271445 19447 solver.cpp:330] Iteration 94354, Testing net (#0)
I1123 13:59:16.168649 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 13:59:16.399194 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.886722
I1123 13:59:16.399245 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.729116 (* 1 = 0.729116 loss)
I1123 13:59:16.476227 19447 solver.cpp:218] Iteration 94354 (11.6663 iter/s, 311.066s/3629 iters), loss = 0.00434312
I1123 13:59:16.476305 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00435277 (* 1 = 0.00435277 loss)
I1123 13:59:16.491070 19447 sgd_solver.cpp:105] Iteration 94354, lr = 0.0005
I1123 14:04:04.147184 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:04:04.458104 19447 solver.cpp:330] Iteration 97983, Testing net (#0)
I1123 14:04:25.381532 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:04:25.552002 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.884865
I1123 14:04:25.552033 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.763399 (* 1 = 0.763399 loss)
I1123 14:04:25.629401 19447 solver.cpp:218] Iteration 97983 (11.7386 iter/s, 309.15s/3629 iters), loss = 0.00309291
I1123 14:04:25.629436 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00310256 (* 1 = 0.00310256 loss)
I1123 14:04:25.629442 19447 sgd_solver.cpp:105] Iteration 97983, lr = 0.0005
I1123 14:07:05.011607 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_100000.caffemodel
I1123 14:07:05.091594 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_100000.solverstate
I1123 14:09:12.252921 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:09:12.564214 19447 solver.cpp:330] Iteration 101612, Testing net (#0)
I1123 14:09:33.571722 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:09:33.685854 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.877437
I1123 14:09:33.685889 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.764127 (* 1 = 0.764127 loss)
I1123 14:09:33.763685 19447 solver.cpp:218] Iteration 101612 (11.7775 iter/s, 308.13s/3629 iters), loss = 0.00227386
I1123 14:09:33.763726 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00228339 (* 1 = 0.00228339 loss)
I1123 14:09:33.763734 19447 sgd_solver.cpp:105] Iteration 101612, lr = 0.0005
I1123 14:14:25.676836 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:14:25.994441 19447 solver.cpp:330] Iteration 105241, Testing net (#0)
I1123 14:14:47.012672 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:14:47.243628 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.889508
I1123 14:14:47.243664 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.599852 (* 1 = 0.599852 loss)
I1123 14:14:47.322146 19447 solver.cpp:218] Iteration 105241 (11.5739 iter/s, 313.55s/3629 iters), loss = 0.000484609
I1123 14:14:47.322185 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000493786 (* 1 = 0.000493786 loss)
I1123 14:14:47.322191 19447 sgd_solver.cpp:105] Iteration 105241, lr = 0.0005
I1123 14:19:39.206032 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:19:39.523056 19447 solver.cpp:330] Iteration 108870, Testing net (#0)
I1123 14:20:00.450008 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:20:00.678617 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.895079
I1123 14:20:00.678648 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.665242 (* 1 = 0.665242 loss)
I1123 14:20:00.757194 19447 solver.cpp:218] Iteration 108870 (11.5789 iter/s, 313.415s/3629 iters), loss = 0.00326214
I1123 14:20:00.757232 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00327155 (* 1 = 0.00327155 loss)
I1123 14:20:00.757237 19447 sgd_solver.cpp:46] MultiStep Status: Iteration 108870, step = 1
I1123 14:20:00.757241 19447 sgd_solver.cpp:105] Iteration 108870, lr = 5e-05
I1123 14:21:31.680402 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_110000.caffemodel
I1123 14:21:31.760632 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_110000.solverstate
I1123 14:24:52.851701 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:24:53.168596 19447 solver.cpp:330] Iteration 112499, Testing net (#0)
I1123 14:24:56.775992 19447 blocking_queue.cpp:49] Waiting for data
I1123 14:25:14.140609 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:25:14.370870 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.887651
I1123 14:25:14.370899 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.660508 (* 1 = 0.660508 loss)
I1123 14:25:14.449697 19447 solver.cpp:218] Iteration 112499 (11.5692 iter/s, 313.678s/3629 iters), loss = 0.00233889
I1123 14:25:14.449736 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00234819 (* 1 = 0.00234819 loss)
I1123 14:25:14.449743 19447 sgd_solver.cpp:105] Iteration 112499, lr = 5e-05
I1123 14:30:06.198529 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:30:06.515137 19447 solver.cpp:330] Iteration 116128, Testing net (#0)
I1123 14:30:27.463795 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:30:27.693264 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.893222
I1123 14:30:27.693295 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.684582 (* 1 = 0.684582 loss)
I1123 14:30:27.771982 19447 solver.cpp:218] Iteration 116128 (11.5827 iter/s, 313.311s/3629 iters), loss = 0.000364077
I1123 14:30:27.772022 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000373375 (* 1 = 0.000373375 loss)
I1123 14:30:27.772029 19447 sgd_solver.cpp:105] Iteration 116128, lr = 5e-05
I1123 14:35:19.615690 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:35:19.931473 19447 solver.cpp:330] Iteration 119757, Testing net (#0)
I1123 14:35:40.929458 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:35:41.160836 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.893222
I1123 14:35:41.160866 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.678472 (* 1 = 0.678472 loss)
I1123 14:35:41.239008 19447 solver.cpp:218] Iteration 119757 (11.5773 iter/s, 313.457s/3629 iters), loss = 0.00539489
I1123 14:35:41.239045 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00540411 (* 1 = 0.00540411 loss)
I1123 14:35:41.239051 19447 sgd_solver.cpp:105] Iteration 119757, lr = 5e-05
I1123 14:36:00.725181 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_120000.caffemodel
I1123 14:36:00.814968 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_120000.solverstate
I1123 14:40:33.286666 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:40:33.604316 19447 solver.cpp:330] Iteration 123386, Testing net (#0)
I1123 14:40:54.682370 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:40:54.788589 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 14:40:54.788622 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.670486 (* 1 = 0.670486 loss)
I1123 14:40:54.865896 19447 solver.cpp:218] Iteration 123386 (11.5714 iter/s, 313.618s/3629 iters), loss = 0.00583405
I1123 14:40:54.865932 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00584335 (* 1 = 0.00584335 loss)
I1123 14:40:54.865938 19447 sgd_solver.cpp:105] Iteration 123386, lr = 5e-05
I1123 14:45:46.559234 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:45:46.875906 19447 solver.cpp:330] Iteration 127015, Testing net (#0)
I1123 14:46:07.941221 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:46:08.058504 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.893222
I1123 14:46:08.058538 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.668014 (* 1 = 0.668014 loss)
I1123 14:46:08.137356 19447 solver.cpp:218] Iteration 127015 (11.5845 iter/s, 313.263s/3629 iters), loss = 0.000714051
I1123 14:46:08.137393 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00072341 (* 1 = 0.00072341 loss)
I1123 14:46:08.137400 19447 sgd_solver.cpp:105] Iteration 127015, lr = 5e-05
I1123 14:50:07.763815 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_130000.caffemodel
I1123 14:50:07.843962 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_130000.solverstate
I1123 14:51:11.242995 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:51:11.758919 19447 solver.cpp:330] Iteration 130644, Testing net (#0)
I1123 14:51:37.316201 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:51:37.607657 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.890436
I1123 14:51:37.607687 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.696755 (* 1 = 0.696755 loss)
I1123 14:51:37.698108 19447 solver.cpp:218] Iteration 130644 (11.0117 iter/s, 329.558s/3629 iters), loss = 0.000611069
I1123 14:51:37.698143 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000620704 (* 1 = 0.000620704 loss)
I1123 14:51:37.698149 19447 sgd_solver.cpp:105] Iteration 130644, lr = 5e-05
I1123 14:56:29.862881 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:56:30.179327 19447 solver.cpp:330] Iteration 134273, Testing net (#0)
I1123 14:56:51.146149 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 14:56:51.377111 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.892293
I1123 14:56:51.377141 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.681535 (* 1 = 0.681535 loss)
I1123 14:56:51.455879 19447 solver.cpp:218] Iteration 134273 (11.5664 iter/s, 313.754s/3629 iters), loss = 0.000550814
I1123 14:56:51.455917 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000560302 (* 1 = 0.000560302 loss)
I1123 14:56:51.455924 19447 sgd_solver.cpp:105] Iteration 134273, lr = 5e-05
I1123 15:01:43.258137 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:01:43.575006 19447 solver.cpp:330] Iteration 137902, Testing net (#0)
I1123 15:02:04.569968 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:02:04.802331 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.889508
I1123 15:02:04.802361 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.706674 (* 1 = 0.706674 loss)
I1123 15:02:04.880921 19447 solver.cpp:218] Iteration 137902 (11.5787 iter/s, 313.42s/3629 iters), loss = 0.00333285
I1123 15:02:04.880961 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00334236 (* 1 = 0.00334236 loss)
I1123 15:02:04.880967 19447 sgd_solver.cpp:105] Iteration 137902, lr = 5e-05
I1123 15:04:53.618065 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_140000.caffemodel
I1123 15:04:53.769140 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_140000.solverstate
I1123 15:06:56.719537 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:06:57.036432 19447 solver.cpp:330] Iteration 141531, Testing net (#0)
I1123 15:07:18.228655 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:07:18.340209 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.889508
I1123 15:07:18.340242 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.682389 (* 1 = 0.682389 loss)
I1123 15:07:18.417065 19447 solver.cpp:218] Iteration 141531 (11.5746 iter/s, 313.53s/3629 iters), loss = 0.000844757
I1123 15:07:18.417112 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000854323 (* 1 = 0.000854323 loss)
I1123 15:07:18.417121 19447 sgd_solver.cpp:105] Iteration 141531, lr = 5e-05
I1123 15:12:10.581957 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:12:10.895160 19447 solver.cpp:330] Iteration 145160, Testing net (#0)
I1123 15:12:30.227084 19447 blocking_queue.cpp:49] Waiting for data
I1123 15:12:32.064374 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:12:32.238031 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.893222
I1123 15:12:32.238071 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.680916 (* 1 = 0.680916 loss)
I1123 15:12:32.315778 19447 solver.cpp:218] Iteration 145160 (11.5613 iter/s, 313.893s/3629 iters), loss = 0.000382139
I1123 15:12:32.315837 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000391639 (* 1 = 0.000391639 loss)
I1123 15:12:32.315851 19447 sgd_solver.cpp:105] Iteration 145160, lr = 5e-05
I1123 15:17:24.551808 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:17:24.868291 19447 solver.cpp:330] Iteration 148789, Testing net (#0)
I1123 15:17:45.968135 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:17:46.083600 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.893222
I1123 15:17:46.083634 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.68298 (* 1 = 0.68298 loss)
I1123 15:17:46.160276 19447 solver.cpp:218] Iteration 148789 (11.5633 iter/s, 313.838s/3629 iters), loss = 0.00498385
I1123 15:17:46.160436 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00499339 (* 1 = 0.00499339 loss)
I1123 15:17:46.160446 19447 sgd_solver.cpp:105] Iteration 148789, lr = 5e-05
I1123 15:19:23.477478 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_150000.caffemodel
I1123 15:19:23.864862 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_150000.solverstate
I1123 15:22:38.315479 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:22:38.633378 19447 solver.cpp:330] Iteration 152418, Testing net (#0)
I1123 15:22:59.791687 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:23:00.021334 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.891365
I1123 15:23:00.021370 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.680052 (* 1 = 0.680052 loss)
I1123 15:23:00.100428 19447 solver.cpp:218] Iteration 152418 (11.5597 iter/s, 313.934s/3629 iters), loss = 0.00200362
I1123 15:23:00.100481 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0020131 (* 1 = 0.0020131 loss)
I1123 15:23:00.100492 19447 sgd_solver.cpp:105] Iteration 152418, lr = 5e-05
I1123 15:27:51.863390 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:27:52.168557 19447 solver.cpp:330] Iteration 156047, Testing net (#0)
I1123 15:28:13.497551 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:28:13.665089 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.892293
I1123 15:28:13.723055 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.68599 (* 1 = 0.68599 loss)
I1123 15:28:13.803071 19447 solver.cpp:218] Iteration 156047 (11.5684 iter/s, 313.699s/3629 iters), loss = 0.00846893
I1123 15:28:13.803351 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0084784 (* 1 = 0.0084784 loss)
I1123 15:28:13.810289 19447 sgd_solver.cpp:105] Iteration 156047, lr = 5e-05
I1123 15:33:05.484871 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:33:05.812458 19447 solver.cpp:330] Iteration 159676, Testing net (#0)
I1123 15:33:27.107209 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:33:27.213390 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.89415
I1123 15:33:27.233690 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.691293 (* 1 = 0.691293 loss)
I1123 15:33:27.313031 19447 solver.cpp:218] Iteration 159676 (11.5756 iter/s, 313.505s/3629 iters), loss = 0.00052091
I1123 15:33:27.313076 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000530312 (* 1 = 0.000530312 loss)
I1123 15:33:27.332053 19447 sgd_solver.cpp:105] Iteration 159676, lr = 5e-05
I1123 15:33:53.304492 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_160000.caffemodel
I1123 15:33:54.035447 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_160000.solverstate
I1123 15:38:20.192263 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:38:20.501334 19447 solver.cpp:330] Iteration 163305, Testing net (#0)
I1123 15:38:41.717494 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:38:41.825520 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.891365
I1123 15:38:41.878296 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.678261 (* 1 = 0.678261 loss)
I1123 15:38:41.957924 19447 solver.cpp:218] Iteration 163305 (11.5338 iter/s, 314.639s/3629 iters), loss = 0.00156131
I1123 15:38:41.957974 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00157073 (* 1 = 0.00157073 loss)
I1123 15:38:41.987745 19447 sgd_solver.cpp:105] Iteration 163305, lr = 5e-05
I1123 15:43:33.818912 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:43:34.124222 19447 solver.cpp:330] Iteration 166934, Testing net (#0)
I1123 15:43:40.774610 19447 blocking_queue.cpp:49] Waiting for data
I1123 15:43:55.366588 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:43:55.475023 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.890436
I1123 15:43:55.489960 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.696514 (* 1 = 0.696514 loss)
I1123 15:43:55.569064 19447 solver.cpp:218] Iteration 166934 (11.5719 iter/s, 313.605s/3629 iters), loss = 0.00166193
I1123 15:43:55.569138 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00167128 (* 1 = 0.00167128 loss)
I1123 15:43:55.577204 19447 sgd_solver.cpp:105] Iteration 166934, lr = 5e-05
I1123 15:48:02.441102 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_170000.caffemodel
I1123 15:48:03.272753 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_170000.solverstate
I1123 15:48:48.463551 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:48:48.769352 19447 solver.cpp:330] Iteration 170563, Testing net (#0)
I1123 15:49:10.029533 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:49:10.138059 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 15:49:10.146219 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.693927 (* 1 = 0.693927 loss)
I1123 15:49:10.225453 19447 solver.cpp:218] Iteration 170563 (11.5334 iter/s, 314.65s/3629 iters), loss = 0.00281555
I1123 15:49:10.225502 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00282493 (* 1 = 0.00282493 loss)
I1123 15:49:10.244572 19447 sgd_solver.cpp:105] Iteration 170563, lr = 5e-05
I1123 15:54:02.407564 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:54:02.725739 19447 solver.cpp:330] Iteration 174192, Testing net (#0)
I1123 15:54:23.980365 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:54:24.097991 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 15:54:24.180387 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.691526 (* 1 = 0.691526 loss)
I1123 15:54:24.259565 19447 solver.cpp:218] Iteration 174192 (11.5563 iter/s, 314.028s/3629 iters), loss = 0.000223808
I1123 15:54:24.259613 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000233106 (* 1 = 0.000233106 loss)
I1123 15:54:24.267626 19447 sgd_solver.cpp:105] Iteration 174192, lr = 5e-05
I1123 15:59:16.042642 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:59:16.303797 19447 solver.cpp:330] Iteration 177821, Testing net (#0)
I1123 15:59:37.643000 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 15:59:37.809063 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896007
I1123 15:59:37.825034 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.69851 (* 1 = 0.69851 loss)
I1123 15:59:37.904027 19447 solver.cpp:218] Iteration 177821 (11.5706 iter/s, 313.639s/3629 iters), loss = 0.00521707
I1123 15:59:37.904083 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00522638 (* 1 = 0.00522638 loss)
I1123 15:59:37.912255 19447 sgd_solver.cpp:105] Iteration 177821, lr = 5e-05
I1123 16:02:33.557569 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_180000.caffemodel
I1123 16:02:34.250661 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_180000.solverstate
I1123 16:04:30.761410 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:04:31.070510 19447 solver.cpp:330] Iteration 181450, Testing net (#0)
I1123 16:04:50.104202 19447 blocking_queue.cpp:49] Waiting for data
I1123 16:04:52.304123 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:04:52.417601 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 16:04:52.425230 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.693764 (* 1 = 0.693764 loss)
I1123 16:04:52.505340 19447 solver.cpp:218] Iteration 181450 (11.5354 iter/s, 314.596s/3629 iters), loss = 0.000204224
I1123 16:04:52.505431 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000213502 (* 1 = 0.000213502 loss)
I1123 16:04:52.523571 19447 sgd_solver.cpp:105] Iteration 181450, lr = 5e-05
I1123 16:09:44.543690 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:09:44.904443 19447 solver.cpp:330] Iteration 185079, Testing net (#0)
I1123 16:10:06.135898 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:10:06.245530 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.89415
I1123 16:10:06.259057 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.671121 (* 1 = 0.671121 loss)
I1123 16:10:06.337523 19447 solver.cpp:218] Iteration 185079 (11.5637 iter/s, 313.826s/3629 iters), loss = 0.00260053
I1123 16:10:06.337574 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00260977 (* 1 = 0.00260977 loss)
I1123 16:10:06.357394 19447 sgd_solver.cpp:105] Iteration 185079, lr = 5e-05
I1123 16:14:58.133157 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:14:58.438415 19447 solver.cpp:330] Iteration 188708, Testing net (#0)
I1123 16:15:19.737668 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:15:19.843999 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 16:15:19.859709 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.650844 (* 1 = 0.650844 loss)
I1123 16:15:19.939007 19447 solver.cpp:218] Iteration 188708 (11.5722 iter/s, 313.595s/3629 iters), loss = 0.00450566
I1123 16:15:19.939052 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0045149 (* 1 = 0.0045149 loss)
I1123 16:15:19.969167 19447 sgd_solver.cpp:105] Iteration 188708, lr = 5e-05
I1123 16:17:03.866199 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_190000.caffemodel
I1123 16:17:04.862836 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_190000.solverstate
I1123 16:20:40.697213 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:20:41.015846 19447 solver.cpp:330] Iteration 192337, Testing net (#0)
I1123 16:21:02.268335 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:21:02.384896 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.895079
I1123 16:21:02.384946 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.655874 (* 1 = 0.655874 loss)
I1123 16:21:02.461509 19447 solver.cpp:218] Iteration 192337 (10.5951 iter/s, 342.516s/3629 iters), loss = 0.00180092
I1123 16:21:02.461841 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00181019 (* 1 = 0.00181019 loss)
I1123 16:21:02.461855 19447 sgd_solver.cpp:105] Iteration 192337, lr = 5e-05
I1123 16:25:54.062575 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:25:54.379411 19447 solver.cpp:330] Iteration 195966, Testing net (#0)
I1123 16:26:15.507853 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:26:15.620321 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.90065
I1123 16:26:15.620358 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.680404 (* 1 = 0.680404 loss)
I1123 16:26:15.697585 19447 solver.cpp:218] Iteration 195966 (11.5857 iter/s, 313.23s/3629 iters), loss = 0.000213284
I1123 16:26:15.697618 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000222444 (* 1 = 0.000222444 loss)
I1123 16:26:15.697624 19447 sgd_solver.cpp:105] Iteration 195966, lr = 5e-05
I1123 16:31:07.530181 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:31:07.835295 19447 solver.cpp:330] Iteration 199595, Testing net (#0)
I1123 16:31:28.919845 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:31:29.090407 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.90065
I1123 16:31:29.090445 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.679173 (* 1 = 0.679173 loss)
I1123 16:31:29.168850 19447 solver.cpp:218] Iteration 199595 (11.577 iter/s, 313.467s/3629 iters), loss = 0.000431546
I1123 16:31:29.168889 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000440695 (* 1 = 0.000440695 loss)
I1123 16:31:29.168895 19447 sgd_solver.cpp:105] Iteration 199595, lr = 5e-05
I1123 16:32:01.685662 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_200000.caffemodel
I1123 16:32:01.956609 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_200000.solverstate
I1123 16:36:21.760186 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:36:22.077100 19447 solver.cpp:330] Iteration 203224, Testing net (#0)
I1123 16:36:41.945102 19447 blocking_queue.cpp:49] Waiting for data
I1123 16:36:43.326134 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:36:43.437774 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.902507
I1123 16:36:43.437809 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.644618 (* 1 = 0.644618 loss)
I1123 16:36:43.515748 19447 solver.cpp:218] Iteration 203224 (11.5445 iter/s, 314.348s/3629 iters), loss = 0.00855615
I1123 16:36:43.515911 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0085654 (* 1 = 0.0085654 loss)
I1123 16:36:43.515923 19447 sgd_solver.cpp:105] Iteration 203224, lr = 5e-05
I1123 16:41:35.405405 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:41:35.721593 19447 solver.cpp:330] Iteration 206853, Testing net (#0)
I1123 16:41:56.714499 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:41:56.948313 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 16:41:56.948346 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.675087 (* 1 = 0.675087 loss)
I1123 16:41:57.028126 19447 solver.cpp:218] Iteration 206853 (11.5754 iter/s, 313.511s/3629 iters), loss = 0.00388801
I1123 16:41:57.028170 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00389725 (* 1 = 0.00389725 loss)
I1123 16:41:57.028179 19447 sgd_solver.cpp:105] Iteration 206853, lr = 5e-05
I1123 16:46:10.721496 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_210000.caffemodel
I1123 16:46:11.170927 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_210000.solverstate
I1123 16:46:50.129976 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:46:50.390225 19447 solver.cpp:330] Iteration 210482, Testing net (#0)
I1123 16:47:11.610359 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:47:11.726932 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896007
I1123 16:47:11.726968 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.689747 (* 1 = 0.689747 loss)
I1123 16:47:11.804682 19447 solver.cpp:218] Iteration 210482 (11.5289 iter/s, 314.774s/3629 iters), loss = 0.00233586
I1123 16:47:11.804844 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00234521 (* 1 = 0.00234521 loss)
I1123 16:47:11.804857 19447 sgd_solver.cpp:105] Iteration 210482, lr = 5e-05
I1123 16:52:03.954610 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:52:04.271190 19447 solver.cpp:330] Iteration 214111, Testing net (#0)
I1123 16:52:25.482372 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:52:25.591006 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 16:52:25.625690 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.681451 (* 1 = 0.681451 loss)
I1123 16:52:25.705104 19447 solver.cpp:218] Iteration 214111 (11.5611 iter/s, 313.896s/3629 iters), loss = 0.00129645
I1123 16:52:25.705157 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0013058 (* 1 = 0.0013058 loss)
I1123 16:52:25.724037 19447 sgd_solver.cpp:105] Iteration 214111, lr = 5e-05
I1123 16:57:17.786691 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:57:18.081310 19447 solver.cpp:330] Iteration 217740, Testing net (#0)
I1123 16:57:39.441514 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 16:57:39.564183 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.891365
I1123 16:57:39.580271 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.659023 (* 1 = 0.659023 loss)
I1123 16:57:39.658390 19447 solver.cpp:218] Iteration 217740 (11.5592 iter/s, 313.949s/3629 iters), loss = 0.00238763
I1123 16:57:39.658450 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00239687 (* 1 = 0.00239687 loss)
I1123 16:57:39.658460 19447 sgd_solver.cpp:46] MultiStep Status: Iteration 217740, step = 2
I1123 16:57:39.734181 19447 sgd_solver.cpp:105] Iteration 217740, lr = 5e-06
I1123 17:00:42.033150 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_220000.caffemodel
I1123 17:00:42.559852 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_220000.solverstate
I1123 17:02:32.652910 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:02:32.958266 19447 solver.cpp:330] Iteration 221369, Testing net (#0)
I1123 17:02:54.272131 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:02:54.381376 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.892293
I1123 17:02:54.401695 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.66579 (* 1 = 0.66579 loss)
I1123 17:02:54.480876 19447 solver.cpp:218] Iteration 221369 (11.5273 iter/s, 314.818s/3629 iters), loss = 0.000199913
I1123 17:02:54.480933 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000209094 (* 1 = 0.000209094 loss)
I1123 17:02:54.488914 19447 sgd_solver.cpp:105] Iteration 221369, lr = 5e-06
I1123 17:07:46.332258 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:07:46.648986 19447 solver.cpp:330] Iteration 224998, Testing net (#0)
I1123 17:08:07.903843 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:08:08.011041 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.893222
I1123 17:08:08.026088 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.669544 (* 1 = 0.669544 loss)
I1123 17:08:08.111935 19447 solver.cpp:218] Iteration 224998 (11.5714 iter/s, 313.618s/3629 iters), loss = 0.00156557
I1123 17:08:08.111981 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00157472 (* 1 = 0.00157472 loss)
I1123 17:08:08.124434 19447 sgd_solver.cpp:105] Iteration 224998, lr = 5e-06
I1123 17:13:00.173648 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:13:00.496935 19447 solver.cpp:330] Iteration 228627, Testing net (#0)
I1123 17:13:10.802455 19447 blocking_queue.cpp:49] Waiting for data
I1123 17:13:21.799955 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:13:21.907794 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.89415
I1123 17:13:21.929455 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.672555 (* 1 = 0.672555 loss)
I1123 17:13:22.008972 19447 solver.cpp:218] Iteration 228627 (11.5614 iter/s, 313.888s/3629 iters), loss = 0.000978775
I1123 17:13:22.009032 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000987901 (* 1 = 0.000987901 loss)
I1123 17:13:22.061147 19447 sgd_solver.cpp:105] Iteration 228627, lr = 5e-06
I1123 17:15:12.598541 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_230000.caffemodel
I1123 17:15:13.136659 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_230000.solverstate
I1123 17:18:14.967573 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:18:15.276813 19447 solver.cpp:330] Iteration 232256, Testing net (#0)
I1123 17:18:36.562381 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:18:36.659837 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.89415
I1123 17:18:36.675930 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.674108 (* 1 = 0.674108 loss)
I1123 17:18:36.771683 19447 solver.cpp:218] Iteration 232256 (11.5302 iter/s, 314.739s/3629 iters), loss = 0.00685007
I1123 17:18:36.771744 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0068592 (* 1 = 0.0068592 loss)
I1123 17:18:36.785395 19447 sgd_solver.cpp:105] Iteration 232256, lr = 5e-06
I1123 17:23:28.705879 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:23:29.011487 19447 solver.cpp:330] Iteration 235885, Testing net (#0)
I1123 17:23:50.238759 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:23:50.345360 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896007
I1123 17:23:50.366152 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.673797 (* 1 = 0.673797 loss)
I1123 17:23:50.445742 19447 solver.cpp:218] Iteration 235885 (11.5696 iter/s, 313.667s/3629 iters), loss = 0.0032812
I1123 17:23:50.445791 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00329031 (* 1 = 0.00329031 loss)
I1123 17:23:50.486740 19447 sgd_solver.cpp:105] Iteration 235885, lr = 5e-06
I1123 17:32:43.139791 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:32:43.450628 19447 solver.cpp:330] Iteration 239514, Testing net (#0)
I1123 17:33:04.638782 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:33:04.743104 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896007
I1123 17:33:04.743155 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.674311 (* 1 = 0.674311 loss)
I1123 17:33:04.820850 19447 solver.cpp:218] Iteration 239514 (6.54624 iter/s, 554.364s/3629 iters), loss = 0.00172506
I1123 17:33:04.820925 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00173409 (* 1 = 0.00173409 loss)
I1123 17:33:04.820945 19447 sgd_solver.cpp:105] Iteration 239514, lr = 5e-06
I1123 17:33:44.030684 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_240000.caffemodel
I1123 17:33:44.189046 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_240000.solverstate
I1123 17:37:56.456025 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:37:56.772286 19447 solver.cpp:330] Iteration 243143, Testing net (#0)
I1123 17:38:17.645503 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:38:17.873948 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896007
I1123 17:38:17.873977 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.673092 (* 1 = 0.673092 loss)
I1123 17:38:17.952174 19447 solver.cpp:218] Iteration 243143 (11.5896 iter/s, 313.126s/3629 iters), loss = 0.000298006
I1123 17:38:17.952208 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00030699 (* 1 = 0.00030699 loss)
I1123 17:38:17.952226 19447 sgd_solver.cpp:105] Iteration 243143, lr = 5e-06
I1123 17:43:08.992988 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:43:09.309470 19447 solver.cpp:330] Iteration 246772, Testing net (#0)
I1123 17:43:30.300997 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:43:30.412500 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896007
I1123 17:43:30.412528 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.671444 (* 1 = 0.671444 loss)
I1123 17:43:30.489308 19447 solver.cpp:218] Iteration 246772 (11.6118 iter/s, 312.526s/3629 iters), loss = 0.000529071
I1123 17:43:30.489351 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000537866 (* 1 = 0.000537866 loss)
I1123 17:43:30.489357 19447 sgd_solver.cpp:105] Iteration 246772, lr = 5e-06
I1123 17:47:49.398398 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_250000.caffemodel
I1123 17:47:49.478207 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_250000.solverstate
I1123 17:48:21.450646 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:48:21.768672 19447 solver.cpp:330] Iteration 250401, Testing net (#0)
I1123 17:48:42.650635 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:48:42.878255 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 17:48:42.878283 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.674022 (* 1 = 0.674022 loss)
I1123 17:48:42.956410 19447 solver.cpp:218] Iteration 250401 (11.6144 iter/s, 312.457s/3629 iters), loss = 0.000910938
I1123 17:48:42.956442 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000919853 (* 1 = 0.000919853 loss)
I1123 17:48:42.956447 19447 sgd_solver.cpp:105] Iteration 250401, lr = 5e-06
I1123 17:53:33.665393 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:53:33.981091 19447 solver.cpp:330] Iteration 254030, Testing net (#0)
I1123 17:53:54.848958 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:53:55.075456 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 17:53:55.075486 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.675569 (* 1 = 0.675569 loss)
I1123 17:53:55.152986 19447 solver.cpp:218] Iteration 254030 (11.6244 iter/s, 312.188s/3629 iters), loss = 0.000627623
I1123 17:53:55.153024 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000636526 (* 1 = 0.000636526 loss)
I1123 17:53:55.153030 19447 sgd_solver.cpp:105] Iteration 254030, lr = 5e-06
I1123 17:58:45.976833 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:58:46.292670 19447 solver.cpp:330] Iteration 257659, Testing net (#0)
I1123 17:59:07.154191 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 17:59:07.383010 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 17:59:07.383040 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.675687 (* 1 = 0.675687 loss)
I1123 17:59:07.460577 19447 solver.cpp:218] Iteration 257659 (11.6202 iter/s, 312.3s/3629 iters), loss = 0.00137906
I1123 17:59:07.460609 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0013879 (* 1 = 0.0013879 loss)
I1123 17:59:07.460616 19447 sgd_solver.cpp:105] Iteration 257659, lr = 5e-06
I1123 18:02:15.274300 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_260000.caffemodel
I1123 18:02:15.573437 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_260000.solverstate
I1123 18:03:58.790383 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:03:59.106734 19447 solver.cpp:330] Iteration 261288, Testing net (#0)
I1123 18:04:19.980197 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:04:20.208783 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 18:04:20.208813 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.674971 (* 1 = 0.674971 loss)
I1123 18:04:20.286885 19447 solver.cpp:218] Iteration 261288 (11.601 iter/s, 312.819s/3629 iters), loss = 0.00078865
I1123 18:04:20.286919 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000797465 (* 1 = 0.000797465 loss)
I1123 18:04:20.286937 19447 sgd_solver.cpp:105] Iteration 261288, lr = 5e-06
I1123 18:09:11.079864 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:09:11.396531 19447 solver.cpp:330] Iteration 264917, Testing net (#0)
I1123 18:09:32.258170 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:09:32.486771 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 18:09:32.486801 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.675725 (* 1 = 0.675725 loss)
I1123 18:09:32.565066 19447 solver.cpp:218] Iteration 264917 (11.6213 iter/s, 312.271s/3629 iters), loss = 0.0037517
I1123 18:09:32.565105 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00376069 (* 1 = 0.00376069 loss)
I1123 18:09:32.565125 19447 sgd_solver.cpp:105] Iteration 264917, lr = 5e-06
I1123 18:14:23.443806 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:14:23.759727 19447 solver.cpp:330] Iteration 268546, Testing net (#0)
I1123 18:14:44.780133 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:14:44.891520 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 18:14:44.891547 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.670896 (* 1 = 0.670896 loss)
I1123 18:14:44.968577 19447 solver.cpp:218] Iteration 268546 (11.6166 iter/s, 312.397s/3629 iters), loss = 0.000144784
I1123 18:14:44.968616 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000153878 (* 1 = 0.000153878 loss)
I1123 18:14:44.968622 19447 sgd_solver.cpp:105] Iteration 268546, lr = 5e-06
I1123 18:16:41.591850 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_270000.caffemodel
I1123 18:16:41.802732 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_270000.solverstate
I1123 18:19:36.443248 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:19:36.758944 19447 solver.cpp:330] Iteration 272175, Testing net (#0)
I1123 18:19:57.708577 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:19:57.878875 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 18:19:57.878916 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.676889 (* 1 = 0.676889 loss)
I1123 18:19:57.956193 19447 solver.cpp:218] Iteration 272175 (11.5948 iter/s, 312.985s/3629 iters), loss = 0.00231135
I1123 18:19:57.956239 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00232023 (* 1 = 0.00232023 loss)
I1123 18:19:57.956246 19447 sgd_solver.cpp:105] Iteration 272175, lr = 5e-06
I1123 18:24:48.807436 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:24:49.120141 19447 solver.cpp:330] Iteration 275804, Testing net (#0)
I1123 18:24:55.297930 19447 blocking_queue.cpp:49] Waiting for data
I1123 18:25:10.191294 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:25:10.302987 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 18:25:10.303020 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.678786 (* 1 = 0.678786 loss)
I1123 18:25:10.379824 19447 solver.cpp:218] Iteration 275804 (11.6158 iter/s, 312.419s/3629 iters), loss = 0.00211231
I1123 18:25:10.379978 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00212121 (* 1 = 0.00212121 loss)
I1123 18:25:10.379989 19447 sgd_solver.cpp:105] Iteration 275804, lr = 5e-06
I1123 18:30:01.063223 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:30:01.372148 19447 solver.cpp:330] Iteration 279433, Testing net (#0)
I1123 18:30:22.419317 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:30:22.530133 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 18:30:22.530167 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.679557 (* 1 = 0.679557 loss)
I1123 18:30:22.607004 19447 solver.cpp:218] Iteration 279433 (11.6231 iter/s, 312.222s/3629 iters), loss = 3.63601e-05
I1123 18:30:22.607043 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 4.51819e-05 (* 1 = 4.51819e-05 loss)
I1123 18:30:22.607051 19447 sgd_solver.cpp:105] Iteration 279433, lr = 5e-06
I1123 18:31:08.048579 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_280000.caffemodel
I1123 18:31:08.266752 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_280000.solverstate
I1123 18:35:13.741960 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:35:14.047888 19447 solver.cpp:330] Iteration 283062, Testing net (#0)
I1123 18:35:35.158232 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:35:35.269876 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 18:35:35.269906 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.682115 (* 1 = 0.682115 loss)
I1123 18:35:35.347308 19447 solver.cpp:218] Iteration 283062 (11.6041 iter/s, 312.735s/3629 iters), loss = 9.37856e-05
I1123 18:35:35.347342 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000102668 (* 1 = 0.000102668 loss)
I1123 18:35:35.347348 19447 sgd_solver.cpp:105] Iteration 283062, lr = 5e-06
I1123 18:40:26.324291 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:40:26.640930 19447 solver.cpp:330] Iteration 286691, Testing net (#0)
I1123 18:40:36.340018 19447 blocking_queue.cpp:49] Waiting for data
I1123 18:40:47.813978 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:40:47.923391 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 18:40:47.924618 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.678323 (* 1 = 0.678323 loss)
I1123 18:40:48.002653 19447 solver.cpp:218] Iteration 286691 (11.6073 iter/s, 312.649s/3629 iters), loss = 0.00025388
I1123 18:40:48.002727 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000262809 (* 1 = 0.000262809 loss)
I1123 18:40:48.003042 19447 sgd_solver.cpp:105] Iteration 286691, lr = 5e-06
I1123 18:45:13.325101 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_290000.caffemodel
I1123 18:45:14.608640 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_290000.solverstate
I1123 18:45:40.165915 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:45:40.467452 19447 solver.cpp:330] Iteration 290320, Testing net (#0)
I1123 18:46:01.574483 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:46:01.680338 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 18:46:01.696480 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.675723 (* 1 = 0.675723 loss)
I1123 18:46:01.775283 19447 solver.cpp:218] Iteration 290320 (11.5659 iter/s, 313.766s/3629 iters), loss = 0.00028693
I1123 18:46:01.775334 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000295847 (* 1 = 0.000295847 loss)
I1123 18:46:01.794813 19447 sgd_solver.cpp:105] Iteration 290320, lr = 5e-06
I1123 18:50:52.559762 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:50:52.831720 19447 solver.cpp:330] Iteration 293949, Testing net (#0)
I1123 18:51:13.973573 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:51:14.082897 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.896936
I1123 18:51:14.174139 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.677821 (* 1 = 0.677821 loss)
I1123 18:51:14.252840 19447 solver.cpp:218] Iteration 293949 (11.6134 iter/s, 312.483s/3629 iters), loss = 0.000446923
I1123 18:51:14.252889 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000455678 (* 1 = 0.000455678 loss)
I1123 18:51:14.261349 19447 sgd_solver.cpp:105] Iteration 293949, lr = 5e-06
I1123 18:56:05.057329 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:56:05.375015 19447 solver.cpp:330] Iteration 297578, Testing net (#0)
I1123 18:56:24.881088 19447 blocking_queue.cpp:49] Waiting for data
I1123 18:56:26.535750 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 18:56:26.612612 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 18:56:26.614006 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.681572 (* 1 = 0.681572 loss)
I1123 18:56:26.691483 19447 solver.cpp:218] Iteration 297578 (11.6148 iter/s, 312.445s/3629 iters), loss = 0.00106415
I1123 18:56:26.708150 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00107289 (* 1 = 0.00107289 loss)
I1123 18:56:26.716176 19447 sgd_solver.cpp:105] Iteration 297578, lr = 5e-06
I1123 18:59:40.879994 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_300000.caffemodel
I1123 18:59:42.140274 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_300000.solverstate
I1123 19:01:18.951114 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:01:19.256474 19447 solver.cpp:330] Iteration 301207, Testing net (#0)
I1123 19:01:40.377568 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:01:40.484112 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 19:01:40.499552 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.682225 (* 1 = 0.682225 loss)
I1123 19:01:40.577431 19447 solver.cpp:218] Iteration 301207 (11.5621 iter/s, 313.871s/3629 iters), loss = 0.00164931
I1123 19:01:40.577896 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00165799 (* 1 = 0.00165799 loss)
I1123 19:01:40.586776 19447 sgd_solver.cpp:105] Iteration 301207, lr = 5e-06
I1123 19:06:31.223846 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:06:31.529610 19447 solver.cpp:330] Iteration 304836, Testing net (#0)
I1123 19:06:52.745281 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:06:52.812604 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 19:06:52.814035 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.67854 (* 1 = 0.67854 loss)
I1123 19:06:52.892052 19447 solver.cpp:218] Iteration 304836 (11.6197 iter/s, 312.313s/3629 iters), loss = 0.00195729
I1123 19:06:52.892096 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.0019659 (* 1 = 0.0019659 loss)
I1123 19:06:52.892361 19447 sgd_solver.cpp:105] Iteration 304836, lr = 5e-06
I1123 19:11:43.964439 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:11:44.281955 19447 solver.cpp:330] Iteration 308465, Testing net (#0)
I1123 19:12:05.450778 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:12:05.557817 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 19:12:05.569689 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.675994 (* 1 = 0.675994 loss)
I1123 19:12:05.647850 19447 solver.cpp:218] Iteration 308465 (11.6034 iter/s, 312.754s/3629 iters), loss = 0.000286196
I1123 19:12:05.647899 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00029471 (* 1 = 0.00029471 loss)
I1123 19:12:05.656916 19447 sgd_solver.cpp:105] Iteration 308465, lr = 5e-06
I1123 19:14:09.807710 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_310000.caffemodel
I1123 19:14:10.981205 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_310000.solverstate
I1123 19:16:59.351459 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:16:59.668243 19447 solver.cpp:330] Iteration 312094, Testing net (#0)
I1123 19:17:13.934890 19447 blocking_queue.cpp:49] Waiting for data
I1123 19:17:20.869801 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:17:20.975518 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 19:17:20.989295 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.679484 (* 1 = 0.679484 loss)
I1123 19:17:21.067298 19447 solver.cpp:218] Iteration 312094 (11.5054 iter/s, 315.416s/3629 iters), loss = 0.00712558
I1123 19:17:21.067356 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00713405 (* 1 = 0.00713405 loss)
I1123 19:17:21.076520 19447 sgd_solver.cpp:105] Iteration 312094, lr = 5e-06
I1123 19:22:12.970548 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:22:13.288462 19447 solver.cpp:330] Iteration 315723, Testing net (#0)
I1123 19:22:34.579273 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:22:34.661599 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 19:22:34.710489 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.678595 (* 1 = 0.678595 loss)
I1123 19:22:34.789613 19447 solver.cpp:218] Iteration 315723 (11.5677 iter/s, 313.718s/3629 iters), loss = 0.00148749
I1123 19:22:34.789659 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00149595 (* 1 = 0.00149595 loss)
I1123 19:22:34.819936 19447 sgd_solver.cpp:105] Iteration 315723, lr = 5e-06
I1123 19:27:26.425030 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:27:26.730398 19447 solver.cpp:330] Iteration 319352, Testing net (#0)
I1123 19:27:47.984005 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:27:48.092537 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 19:27:48.092597 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.676816 (* 1 = 0.676816 loss)
I1123 19:27:48.169162 19447 solver.cpp:218] Iteration 319352 (11.5807 iter/s, 313.366s/3629 iters), loss = 0.000216911
I1123 19:27:48.169198 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000225376 (* 1 = 0.000225376 loss)
I1123 19:27:48.169209 19447 sgd_solver.cpp:105] Iteration 319352, lr = 5e-06
I1123 19:28:40.277842 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_320000.caffemodel
I1123 19:28:40.884140 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_320000.solverstate
I1123 19:32:40.484352 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:32:40.801821 19447 solver.cpp:330] Iteration 322981, Testing net (#0)
I1123 19:33:02.066303 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:33:02.151546 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 19:33:02.167755 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.681681 (* 1 = 0.681681 loss)
I1123 19:33:02.246841 19447 solver.cpp:218] Iteration 322981 (11.5548 iter/s, 314.068s/3629 iters), loss = 0.000339148
I1123 19:33:02.246896 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000347571 (* 1 = 0.000347571 loss)
I1123 19:33:02.266098 19447 sgd_solver.cpp:105] Iteration 322981, lr = 5e-06
I1123 19:37:54.021448 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:37:54.326759 19447 solver.cpp:330] Iteration 326610, Testing net (#0)
I1123 19:38:43.558967 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:38:43.670693 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 19:38:43.671690 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.682207 (* 1 = 0.682207 loss)
I1123 19:38:43.748003 19447 solver.cpp:218] Iteration 326610 (10.6269 iter/s, 341.493s/3629 iters), loss = 0.000255046
I1123 19:38:43.748157 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000263467 (* 1 = 0.000263467 loss)
I1123 19:38:43.748205 19447 sgd_solver.cpp:105] Iteration 326610, lr = 5e-06
I1123 19:43:11.357620 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_330000.caffemodel
I1123 19:43:11.581606 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_330000.solverstate
I1123 19:43:30.209642 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:43:30.519832 19447 solver.cpp:330] Iteration 330239, Testing net (#0)
I1123 19:43:51.463518 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:43:51.574847 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 19:43:51.574877 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.681666 (* 1 = 0.681666 loss)
I1123 19:43:51.651465 19447 solver.cpp:218] Iteration 330239 (11.7864 iter/s, 307.897s/3629 iters), loss = 0.000154106
I1123 19:43:51.651499 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000162444 (* 1 = 0.000162444 loss)
I1123 19:43:51.651506 19447 sgd_solver.cpp:105] Iteration 330239, lr = 5e-06
I1123 19:48:37.989064 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:48:38.299855 19447 solver.cpp:330] Iteration 333868, Testing net (#0)
I1123 19:48:59.060540 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:48:59.287824 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 19:48:59.287852 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.685773 (* 1 = 0.685773 loss)
I1123 19:48:59.364637 19447 solver.cpp:218] Iteration 333868 (11.7937 iter/s, 307.707s/3629 iters), loss = 0.00018832
I1123 19:48:59.364671 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000196752 (* 1 = 0.000196752 loss)
I1123 19:48:59.364678 19447 sgd_solver.cpp:105] Iteration 333868, lr = 5e-06
I1123 19:53:45.662663 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:53:45.973752 19447 solver.cpp:330] Iteration 337497, Testing net (#0)
I1123 19:54:06.740317 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:54:06.968017 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 19:54:06.968047 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.684165 (* 1 = 0.684165 loss)
I1123 19:54:07.044692 19447 solver.cpp:218] Iteration 337497 (11.7949 iter/s, 307.674s/3629 iters), loss = 0.00111597
I1123 19:54:07.044726 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00112442 (* 1 = 0.00112442 loss)
I1123 19:54:07.044734 19447 sgd_solver.cpp:105] Iteration 337497, lr = 5e-06
I1123 19:57:24.671957 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_340000.caffemodel
I1123 19:57:24.751539 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_340000.solverstate
I1123 19:58:54.973160 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:58:55.289830 19447 solver.cpp:330] Iteration 341126, Testing net (#0)
I1123 19:59:13.619019 19447 blocking_queue.cpp:49] Waiting for data
I1123 19:59:16.341537 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 19:59:16.456763 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 19:59:16.456795 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.684701 (* 1 = 0.684701 loss)
I1123 19:59:16.535097 19447 solver.cpp:218] Iteration 341126 (11.7263 iter/s, 309.474s/3629 iters), loss = 0.000138825
I1123 19:59:16.535135 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000147442 (* 1 = 0.000147442 loss)
I1123 19:59:16.535141 19447 sgd_solver.cpp:105] Iteration 341126, lr = 5e-06
I1123 20:04:08.217414 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:04:08.534610 19447 solver.cpp:330] Iteration 344755, Testing net (#0)
I1123 20:04:29.594892 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:04:29.707041 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 20:04:29.707072 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.683367 (* 1 = 0.683367 loss)
I1123 20:04:29.785171 19447 solver.cpp:218] Iteration 344755 (11.5856 iter/s, 313.233s/3629 iters), loss = 0.000186323
I1123 20:04:29.785207 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.000194975 (* 1 = 0.000194975 loss)
I1123 20:04:29.785214 19447 sgd_solver.cpp:105] Iteration 344755, lr = 5e-06
I1123 20:09:21.440729 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:09:21.738464 19447 solver.cpp:330] Iteration 348384, Testing net (#0)
I1123 20:09:42.883697 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:09:42.995021 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 20:09:42.995056 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.682893 (* 1 = 0.682893 loss)
I1123 20:09:43.071921 19447 solver.cpp:218] Iteration 348384 (11.5841 iter/s, 313.274s/3629 iters), loss = 0.00135155
I1123 20:09:43.071959 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00136022 (* 1 = 0.00136022 loss)
I1123 20:09:43.071966 19447 sgd_solver.cpp:105] Iteration 348384, lr = 5e-06
I1123 20:11:53.034958 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_350000.caffemodel
I1123 20:11:53.322329 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_350000.solverstate
I1123 20:14:35.224114 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:14:35.535490 19447 solver.cpp:330] Iteration 352013, Testing net (#0)
I1123 20:14:56.664427 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:14:56.776937 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 20:14:56.776974 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.683647 (* 1 = 0.683647 loss)
I1123 20:14:56.855180 19447 solver.cpp:218] Iteration 352013 (11.5657 iter/s, 313.773s/3629 iters), loss = 0.00220027
I1123 20:14:56.855332 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00220887 (* 1 = 0.00220887 loss)
I1123 20:14:56.855341 19447 sgd_solver.cpp:105] Iteration 352013, lr = 5e-06
I1123 20:19:48.540979 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:19:48.855979 19447 solver.cpp:330] Iteration 355642, Testing net (#0)
I1123 20:20:09.968596 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:20:10.080551 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 20:20:10.080590 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.684871 (* 1 = 0.684871 loss)
I1123 20:20:10.157876 19447 solver.cpp:218] Iteration 355642 (11.5834 iter/s, 313.293s/3629 iters), loss = 0.00320011
I1123 20:20:10.158061 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00320872 (* 1 = 0.00320872 loss)
I1123 20:20:10.158073 19447 sgd_solver.cpp:105] Iteration 355642, lr = 5e-06
I1123 20:25:01.820736 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:25:02.132496 19447 solver.cpp:330] Iteration 359271, Testing net (#0)
I1123 20:25:23.213812 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:25:23.326915 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.898793
I1123 20:25:23.326947 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.686336 (* 1 = 0.686336 loss)
I1123 20:25:23.403579 19447 solver.cpp:218] Iteration 359271 (11.5855 iter/s, 313.237s/3629 iters), loss = 0.000676225
I1123 20:25:23.403616 19447 solver.cpp:237]     Train net output #0: SoftmaxWithLoss1 = 0.00068494 (* 1 = 0.00068494 loss)
I1123 20:25:23.403622 19447 sgd_solver.cpp:105] Iteration 359271, lr = 5e-06
I1123 20:26:21.967025 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_360000.caffemodel
I1123 20:26:22.810819 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_360000.solverstate
I1123 20:30:16.428902 19474 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:30:16.733062 19447 solver.cpp:447] Snapshotting to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_362900.caffemodel
I1123 20:30:17.286123 19447 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/nssg/unclezhannssg/Downloads/anzhuangcaffe/caffe-master/examples/myfilen/DenseNet13/LOG_iter_362900.solverstate
I1123 20:30:17.399437 19447 solver.cpp:310] Iteration 362900, loss = 0.00272236
I1123 20:30:17.400514 19447 solver.cpp:330] Iteration 362900, Testing net (#0)
I1123 20:30:20.617384 19447 blocking_queue.cpp:49] Waiting for data
I1123 20:30:38.842411 19481 data_layer.cpp:73] Restarting data prefetching from start.
I1123 20:30:38.950772 19447 solver.cpp:397]     Test net output #0: Accuracy1 = 0.897864
I1123 20:30:38.951967 19447 solver.cpp:397]     Test net output #1: SoftmaxWithLoss1 = 0.686193 (* 1 = 0.686193 loss)
I1123 20:30:38.952096 19447 solver.cpp:315] Optimization Done.
I1123 20:30:38.971170 19447 caffe.cpp:259] Optimization Done.
